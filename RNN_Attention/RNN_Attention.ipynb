{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "deeply refered on https://ratsgo.github.io/from%20frequency%20to%20semantics/2017/10/06/attention/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchtext.datasets import TranslationDataset, Multi30k\n",
    "from torchtext.data import Field, BucketIterator\n",
    "\n",
    "import spacy\n",
    "\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "\n",
    "SEED = 1\n",
    "\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "spacy_de = spacy.load('de')\n",
    "spacy_en = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_de(text):\n",
    "    \"\"\"\n",
    "    Tokenizes German text from a string into a list of strings (tokens) and reverses it\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)][::-1]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    \"\"\"\n",
    "    Tokenizes English text from a string into a list of strings (tokens)\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "SRC = Field(tokenize=tokenize_de, init_token='<sos>', eos_token='<eos>', lower=True,include_lengths=True)\n",
    "# SRC 데이터에는 include_lengths 인자를 넣어주게 되면서, Encoder 의 pad_pack_sequence를 원활하게끔 지원해줍니다.\n",
    "TRG = Field(tokenize=tokenize_en, init_token='<sos>', eos_token='<eos>', lower=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data, test_data = Multi30k.splits(exts=('.de', '.en'), fields=(SRC, TRG))\n",
    "# 영어와, 독일 데이터를 다운받으면서, train, validation, test 데이터로 나눠서 가져오게 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC.build_vocab(train_data, min_freq=2)\n",
    "TRG.build_vocab(train_data, min_freq=2)\n",
    "# 최소한 2번 이상 나오는 vocab에 대해서만, numericalize 시키게 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "BATCH_SIZE = 1000\n",
    "\n",
    "train_iter, test_iter = BucketIterator.splits(datasets=(train_data,test_data),\n",
    "                                            batch_sizes=(BATCH_SIZE,BATCH_SIZE),  \n",
    "                                            sort_key=lambda x: len(x.src), \n",
    "                                            device=device,\n",
    "                                            sort_within_batch=True, \n",
    "                                            repeat=False)\n",
    "\n",
    "# device 는 cpu 또는 gpu 를 적용하게 되고, (pytorch에서는 Variable(device=device) 와 같은 형태로, \n",
    "#인풋 데이터에 대해서 device 를 할당해줍니다.\n",
    "# batch_size 를 할당해주면, 반환값에 randomly batch가 적용됩니다.(1000개씩 묶인 상태에서, 인덱스가 랜덤으로 섞인 iterator 가 됩니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n",
      "tensor([[   2,    2,    2,  ...,    2,    2,    2],\n",
      "        [   4,    4,    4,  ...,    4,    4,    4],\n",
      "        [1553,  617,   23,  ...,  142, 1259,   72],\n",
      "        ...,\n",
      "        [ 423, 1545,  116,  ...,   13,  537,  135],\n",
      "        [   8,    5,    5,  ...,    5,    5,    5],\n",
      "        [   3,    3,    3,  ...,    3,    3,    3]], device='cuda:0')\n",
      "tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(type(batch.src)) # torchtext 의  Field 클래스에서 include_lengths 를 True로 할당해주었기 때문에, turple 을 반환합니다.\n",
    "print(batch.src[0]) # 첫 번째 인자는 input_data가 batch_size에 맞춰 들어간 상태입니다.\n",
    "print(batch.src[1][:10]) # 두 번째 인자는 각 문장 별, 길이를 의미합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder 에 대한 논의\n",
    "- 네트워크는 기존 RNN encoder-decoder 모델에 사용된 Encoder와 같은 형태입니다.\n",
    "- Attention mechanism 을 사용하기 위해서는, 이번엔 hidden layer만 반환하는 것이 아닌, sequential output인 outputs 또한 반환합니다.\n",
    "- hidden 은 RNN encoder-decoder와 같이 context vector와 같은 역할을 합니다.\n",
    "- outputs 은 아래에 나올, Attention class의 인자로 들어가게 됩니다.\n",
    "\n",
    "### pack_padded_sequence, pad_packed_sequence 에 대한 논의\n",
    "- 이번에는 RNN 네트워크의 pytorch 의 utility function인 packing 과 padding 에 대한 함수를 사용하였습니다.\n",
    "- 사용하는 이유는, RNN 네트워크의 특징 상, 배치(batch)를 돌면서 다른 길이의 데이터를 받게 되는데, (CNN의 경우 데이터의 길이는 padding으로 일치시킵니다.) 그에 따라서, 0(n^2) 이라는 연산에 대한 비효율성이 발생하게 되는데, 이러한 문제를 줄여서 연산 효율성을 추구하기 위해 진행되는 프로세스입니다.\n",
    "\n",
    "```python\n",
    "nn.utils.rnn.pack_padded_sequence(embedded, seq_length)\n",
    "```\n",
    "위의 코드에서 보게 되면, embedding vector와 seq_length라는 인자가 함께 들어가게 되는데, sequence length 란, 각 문장의 길이를 의미하게 되는 것으로, 위의 torchtext 에서 Field 의 인자 ` include_length = True` 로 선언하여 length 또한 함께 이터레이터를 돌면서 반환하게끔 해놓은 상태이기 때문에, 인덱싱을 통해 쉽게 넣어줄 수 있게 됩니다.\n",
    "\n",
    "자세한 설명은 해당 페이지를 참고하시면 좋습니다. : https://gist.github.com/HarshTrivedi/f4e7293e941b17d19058f6fb90ab0fec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, num_layers, dropout):\n",
    "        \n",
    "        super().__init__()     \n",
    "        self.input_dim = input_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim,padding_idx=1)\n",
    "        self.num_layers = num_layers\n",
    "        self.LSTM = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, \n",
    "                            dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.hidden_dim = hidden_dim    \n",
    "        \n",
    "    def forward(self, text):\n",
    "        # text 는 tuple 입니다.\n",
    "        input_ = text[0] # tuple 의 첫 번째 엘리먼트는 input_data가 배치로 들어온 형태입니다.\n",
    "        seq_length = text[1] # tuple 의 두 번째 엘리먼트는 input_data 각각의 문장의 길이입니다.\n",
    "        embedded = self.dropout(self.embedding(input_)) #[max_length, batch_size, embedding_dim]\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, seq_length) \n",
    "        # packed_embedded 또한 tuple형태입니다. 하지만 기존의 RNN based 모델에 인자로 넣어주면 됩니다!\n",
    "        outputs, (hidden,cell) = self.LSTM(packed_embedded)\n",
    "        outputs,_ = nn.utils.rnn.pad_packed_sequence(outputs)\n",
    "        # hidden 은 가변 길이의 문장을 하나의 정보로 압축시키는 context vector 라고 생각하면 된다. \n",
    "        # 이러한 convext vector는 num_layer 의 갯수만큼 있고, decoder part에서 풀게 된다.\n",
    "        return outputs, hidden \n",
    "    # [max_length , batch_size , hidden_layer_dim], [1 , batch_size , hidden_layer_dim] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'attention_pic.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention 에 대한 논의\n",
    "- 자세한 논의는 차후에 링크되는 블로그의 논문 요약에서 보다 자세히 다루도록 하겠습니다.\n",
    "- RNN Encoder-decoder의 문제는 바로, Encoder에서 context vector 로 input_data(source data라고 주로 칭합니다.)의 information을 하나의 벡터로 압축(compress)시킨다는 것에 있습니다. 이에 따라, 한정된 정보만을 담을 수 있다는 것이 주요 단점으로 지적됩니다.\n",
    "- 해당 Attention mechanism은 이러한 문제를 해결하기 위해 제시된 것으로, source data 와 target data의 각각의 sequence 에서 각각의 sentence 간의 유사성이 존재할 것이라는 아이디어에 기반합니다.\n",
    "- 예로 들어서, \"나는 딥러닝이 좋아\" 와 \"I love deep-learning\" 두 문장이 존재할 경우, \"좋아\" 라는 단어와 \"love\"라는 단어 간의 유사성이 문맥적(contextual) 의미적(semantic) 유사성이 높을 것입니다. 이에 따라, `Encoder 의 t스텝의 hidden과 Decoder의 t-1 hidden 간의 유사성을 계산`하고, (위의 등식에서 score() 라는 함수로 계산됩니다.) 이를 `확률 형태로 반환`하여(Attention weights) source data 중에 어떤 단어가 target data의 특정 스텝에서 특정 단어와 `유사성이 가장 높은지를 확률 형태`로 알려주게 됩니다. \n",
    "- 기존에는, context vector를 Decoder의 initial hidden layer로 사용하면서, target data를 그대로 RNN 네트워크를 통해 연산을 진행해주었는데, Attention weights 를 Context vector에 곱해준 즉, 가중합(weighted sum)을 진행해주면서, 학습에 유의한(significant) 데이터에 \"집중\" 하게 만드는 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module) : \n",
    "    def __init__(self, outputs, hidden, hidden_dim):\n",
    "        \n",
    "        super().__init__()        \n",
    "        self.outputs = outputs # Encoder가 출력한 outputs 입니다.\n",
    "        self.hidden = hidden # Encoder가 출력한 hidden 입니다.\n",
    "        self.hidden_dim = hidden_dim # hidden_dim의 dimension 입니다.\n",
    "        self.score_fc = nn.Linear(hidden_dim,hidden_dim)\n",
    "        self.softmax_fc = nn.Linear(hidden_dim,1)\n",
    "        \n",
    "    def forward(self) : \n",
    "        outputs = self.outputs.permute(1,0,2) # [batch_size, max_length, hidden_dim]\n",
    "        hidden = self.hidden.permute(1,0,2) # [batch_size,1, hidden_dim]\n",
    "        attention_score = torch.tanh(self.score_fc(outputs) + self.score_fc(hidden)) # [batch_size, max_length, hidden_dim]\n",
    "        attention_weights = torch.softmax(self.softmax_fc(attention_score),dim=1) # [batch_size, max_length, 1]\n",
    "        context_vector = attention_weights * outputs#[batch_size, max_length, hidden_dim]\n",
    "        new_context_vector = torch.sum(context_vector,dim=1)#[batch_size, hidden_dim]\n",
    "        return new_context_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([15, 1000, 128]), torch.Size([1, 1000, 128]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim = len(SRC.vocab.itos)\n",
    "output_dim = len(TRG.vocab.itos)\n",
    "embedding_dim = 256\n",
    "hidden_dim = 128\n",
    "num_layer = 1\n",
    "dropout = 0.5\n",
    "attention_obj = Attention\n",
    "batch_size = 1000\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "enc = Encoder(input_dim,embedding_dim,hidden_dim,num_layer,dropout).to(device)\n",
    "batch = next(iter(train_iter))\n",
    "\n",
    "enc(batch.src)[0].size(),enc(batch.src)[1].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 128])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = enc(batch.src)[0]\n",
    "hidden = enc(batch.src)[1]\n",
    "attention_obj = Attention(outputs, hidden, 128).to(device)\n",
    "attention_obj().size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, embedding_dim, hidden_dim, num_layers,\\\n",
    "                 dropout, attention_obj,concat=False):\n",
    "        \n",
    "        super().__init__()        \n",
    "        self.embedding = nn.Embedding(output_dim, embedding_dim,padding_idx=1)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.concat = concat\n",
    "        # concat 을 넣은 이유 :\n",
    "        # Attention 을 공부하면서, 소스 코드의 종류를 두 가지를 발견했는데, 첫 번째는 attetion_vector를 hidden_layer로 \n",
    "        # 사용하는 경우였고, 두 번째는 attetion_vector를 embedding_vector와 concatenate시켜서 RNN 네트워크에 넣은 경우였습니다.\n",
    "        # 논문을 따로 읽으면서 구현을 한 것이 아니였기 때문에, 클래스의 파라미터로 두 가지 모두를 가능하게 만들어 놓았고,\n",
    "        # 논문을 추후에 읽으면서 확실한 것으로 대체하도록 하겠습니다.\n",
    "        \n",
    "        if self.concat : \n",
    "            self.rnn = nn.GRU(embedding_dim+hidden_dim,hidden_dim, num_layers=num_layers, \\\n",
    "                              dropout=dropout,batch_first=True)\n",
    "        else : \n",
    "            self.rnn = nn.GRU(embedding_dim,hidden_dim, num_layers=num_layers, \\\n",
    "                                  dropout=dropout,batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fully_connect = nn.Linear(hidden_dim,output_dim)\n",
    "        self.attention_obj = attention_obj\n",
    "        \n",
    "    def forward(self, inputs, hidden, enc_outputs):\n",
    "        \n",
    "        atten = self.attention_obj(enc_outputs,hidden,self.hidden_dim)\n",
    "        context_vector = atten.forward()\n",
    "        # 함수 내에서 Attention class을 호출해, context_vector를 계산해줍니다.\n",
    "        # 이 때, attention_obj 에서 변하는 argument 는 hidden layer의 값입니다.\n",
    "        embedding_vector = self.dropout(self.embedding(inputs)).permute(1,0,2)\n",
    "        \n",
    "        if not self.concat : \n",
    "            dec_hidden = context_vector.unsqueeze(1).permute(1,0,2)\n",
    "            outputs,hidden = self.rnn(embedding_vector,dec_hidden)            \n",
    "        else : \n",
    "            concated_vector = torch.cat((context_vector.unsqueeze(1), embedding_vector), -1)\n",
    "            outputs,hidden = self.rnn(concated_vector)\n",
    "            \n",
    "        outputs,hidden = self.rnn(embedding_vector,dec_hidden)\n",
    "        final_outputs = outputs.squeeze(1)\n",
    "        fc_layer = self.fully_connect(final_outputs)\n",
    "        if len(fc_layer.size()) < 3 :\n",
    "            fc_layer = fc_layer.unsqueeze(1)\n",
    "            fc_layer = fc_layer.permute(1,0,2)\n",
    "        else :\n",
    "            fc_layer = fc_layer.permute(1,0,2)\n",
    "        \n",
    "        return fc_layer, hidden\n",
    "#     [max_length, batch_size, output_dim] , [1, batch_size, output_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([23, 1000, 5893]), torch.Size([1, 1000, 128]))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim = len(SRC.vocab.itos)\n",
    "output_dim = len(TRG.vocab.itos)\n",
    "embedding_dim = 256\n",
    "hidden_dim = 128\n",
    "num_layer = 1\n",
    "dropout = 0.5\n",
    "attention_obj = Attention\n",
    "batch_size = 1000\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "batch = next(iter(train_iter))\n",
    "dec = Decoder(output_dim,embedding_dim,hidden_dim,num_layer,dropout,attention_obj).to(device)\n",
    "\n",
    "dec(batch.trg,enc(batch.src)[1],enc(batch.src)[0])[0].size(),\\\n",
    "dec(batch.trg,enc(batch.src)[1],enc(batch.src)[0])[1].size()\n",
    "#[max_length, batch_size, output_dim] , [1, batch_size, output_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "                                \n",
    "        batch_size = trg.shape[1]\n",
    "        max_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        # decoder 의 맨처음에는 encoder 에서 나온 hidden,cell을 넣어주어야 합니다! 이때, num_layer와 hidden_dim은 같아야 합니다!\n",
    "        enc_output, enc_hidden = self.encoder(src)\n",
    "        \n",
    "        # decoder 를 돌면서, 각 단어에 대한 outputs값(벡터 형태)이 나오게 되는데, 이러한 값들을 아래의 outputs 변수에 저장해줍니다\n",
    "        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        # 맨 처음에는 문장의 시작을 알리는 sos(start of sentence) 토큰을 넣어주어야 합니다.\n",
    "        dec_inputs = trg[0,:].unsqueeze(0)\n",
    "        dec_hidden = enc_hidden\n",
    "        for t in range(1, max_len):\n",
    "\n",
    "            fc_layer, dec_hidden = self.decoder(dec_inputs,dec_hidden,enc_output)\n",
    "#            output'dimension : [batch_size , output_dim], 여기서 output_dim 은 출현 가능한 모든 target lang 의 수 입니다.\n",
    "            outputs[t] = fc_layer\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = fc_layer.max(1)[1] # 해당 글자의 numericalized index 를 넣어주어야 합니다.\n",
    "            input_ = (trg[t] if teacher_force else top1)\n",
    "            # teacher_forcing 을 할 경우, 실제 trg데이터를 다음 input으로 사용, 그렇지 않을 경우, 이전 state에서 가장 높은 \n",
    "            # 값을 가진[나올 수 있는 모든 target vocab 리스트 중에서를 의미합니다. 확률값의 형태는 아니지만, 가장 개연성이 높은 단어를 의미합니다.]\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameter settings\n",
    "input_dim = len(SRC.vocab.itos)\n",
    "output_dim = len(TRG.vocab.itos)\n",
    "embedding_dim = 256\n",
    "hidden_dim = 128\n",
    "num_layer = 1\n",
    "dropout = 0.5\n",
    "attention_obj = Attention\n",
    "batch_size = 1000\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "enc = Encoder(input_dim,embedding_dim,hidden_dim,num_layer,dropout)\n",
    "dec = Decoder(output_dim,embedding_dim,hidden_dim,num_layer,dropout,attention_obj)\n",
    "enc.to(device);dec.to(device)\n",
    "batch = next(iter(train_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([27, 1000, 5893])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq = Seq2Seq(enc,dec,device)\n",
    "seq(batch.src,batch.trg).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gradient clipping 에 대한 논의\n",
    "- https://hskimim.github.io/Avoid_Exploding_gradient_in_Neural_Net_with_gradient_clipping/ 에 보다 자세히 기록하였습니다.\n",
    "- RNN 베이스의 네트워크의 특징인 gradient exploding 을 방지해주는 방법론입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class fit() : \n",
    "    \n",
    "    def __init__(self, model, train_iter, test_iter, epoch = 5) : \n",
    "        self.optimizer = optim.Adam(model.parameters())\n",
    "        # <pad> 토큰은 임베딩 벡터와, loss_function에 argument 로 들어가서, training 과정에서 제외됩니다.\n",
    "        self.pad_idx = TRG.vocab.stoi['<pad>'] \n",
    "        self.criterion = nn.CrossEntropyLoss(ignore_index=self.pad_idx)\n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model = model.to(device)\n",
    "        self.train_iter = train_iter\n",
    "        self.test_iter = test_iter\n",
    "        self.epoch = epoch\n",
    "            \n",
    "    def train(self,clip):\n",
    "    \n",
    "        epoch_loss = 0 # loss per epoch\n",
    "        self.model.train()\n",
    "        \n",
    "        for i, batch in enumerate(self.train_iter):\n",
    "            print('batch : ',i,end='\\r')\n",
    "            src = batch.src\n",
    "            trg = batch.trg\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            output = model(src, trg)        \n",
    "\n",
    "            loss_output = output[1:].view(-1, output.shape[-1])\n",
    "            loss_trg = trg[1:].view(-1)\n",
    "            # sos 토큰을 제외하고, 차원을 맞춘 후에, output을 변수에 저장해줍니다.\n",
    "            \n",
    "            loss = self.criterion(loss_output, loss_trg)\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "            # gradient clipping\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        return epoch_loss / len(self.train_iter)\n",
    "    \n",
    "\n",
    "    def fit_by_iterate(self,clip) : \n",
    "        \n",
    "        for epoch in range(self.epoch):\n",
    "            print('epoch : ',epoch + 1)\n",
    "            train_loss= self.train(clip)\n",
    "            print(\"epoch's loss : {}\".format(train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitting_process = fit(seq,train_iter,test_iter,epoch=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch :  1\n",
      "epoch's loss : 6.89602072485562\n"
     ]
    }
   ],
   "source": [
    "fitting_process.fit_by_iterate(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
