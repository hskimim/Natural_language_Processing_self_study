{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, Yoshua Bengio (2014), \"Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation\" 논문을 기반으로 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchtext.datasets import TranslationDataset, Multi30k\n",
    "from torchtext.data import Field, BucketIterator\n",
    "\n",
    "import spacy\n",
    "\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "\n",
    "SEED = 1\n",
    "\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "spacy_de = spacy.load('de')\n",
    "spacy_en = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 해당 아키텍처에 대한 논의\n",
    "- 저번에 다뤘던 \"Sequence to Sequence Learning with Neural Networks\" 논문의 아키텍처에서는 Encoder와 Decoder의 개념에 대해서 이야기하고, Machine traslation 을 맥락을 잡아보았습니다.\n",
    "- 학습을 통해, 번역의 성능이 향상된다는 점에서 의미가 있었지만, 해당 논문의 아키텍처에는 한계점이 존재하였는데, 바로 `information compression` 이라는 부분입니다. \n",
    "- Encoder 가 반환하는 hidden_state 또는 context vector ($h_{t}$) 는 Decoder 의 initial hidden state($s_{0}$)가 되었는데 여기서 context vector는 source sequence data에 대한 정보를 하나의 state로 압축시킨 형태가 됩니다.\n",
    "- 이에 따라서, 모든 sequence data에 대한 정보가 흐려지게 되고, 이에 대한 한계가 대두되어, 이어서 Seq2Seq 기반의 논문들이 나오게 되는데, 그 중 첫 번째 논문이 바로 해당 논문  \"Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation\" 이 되겠습니다.\n",
    "- 수식적으로 표현하면 아래와 같습니다.\n",
    "    - Sequence to Sequence Learning with Neural Networks : \n",
    "        - $h_{t} = EncoderRNN(x_{t},h_{t-1})$ : encoder에는 source data 와 hidden state가 필요, hidden_state를 반환\n",
    "        - $s_{0} = z = h_{T}$ : decoder의 initial hidden state는 encoder의 마지막 hidden_state\n",
    "        - $s_{t} = DecoderRNN(y_{t},s_{t-1})$ : encoder와 같은 decoder의 network architecture\n",
    "    - Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation : \n",
    "        - $s_{t} = DecoderRNN(y_{t},s_{t-1},z)$ : decoder에서 context vector input 이 추가됩니다.\n",
    "        - $y_{t+1}(pred) = f(y_{t},s_{t},z) $ : $y_{t}$ 를 가지고 fully_connected layer를 해줌으로써, $y_{t+1}$를 예측하는 데, 이 때 linear layer에는 $t$ 시점의 predicted_output(trg data), hidden_state, context_vector 가 변수로 들어갑니다. \n",
    "\n",
    "<img src = 'RNN_seq2seq_2.png'>\n",
    "\n",
    "- 이제 본격적으로 아키텍처에 대해서 알아볼까요?\n",
    "    - 위의 식을 계속 보시면서 따라오면 편하신데, 새롭게 짜여진 아키텍처에서는 y_t가 update되면서, context vector인 z가 계속 함수 안에 들어가게 됩니다. 즉, 기존에는 decoder의 initial hidden state로 사용되던 context vector가 이제 전체 시퀀스에 동일하게 들어가게 되는 것이죠.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_de(text):\n",
    "    \"\"\"\n",
    "    Tokenizes German text from a string into a list of strings (tokens) and reverses it\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)][::-1]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    \"\"\"\n",
    "    Tokenizes English text from a string into a list of strings (tokens)\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "SRC = Field(tokenize=tokenize_de, init_token='<sos>', eos_token='<eos>', lower=True)\n",
    "TRG = Field(tokenize=tokenize_en, init_token='<sos>', eos_token='<eos>', lower=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `<sos>` 와 `<eos>` 에 대한 논의\n",
    "- 아시는 분도 계시겠지만, 각각 Start Of Sentence, End of Sentence 를 의미합니다.\n",
    "- Encoder와 Decoder 에 들어가는 source data, target data에 대해 앞뒤로 해당 두 토큰이 들어가게 됩니다.\n",
    "- 문장의 시작과 끝을 일괄적으로 적용시켜줌으로써, 문장의 시작점과 끝점을 알려주는 역할을 합니다.\n",
    "- CNN에서 이미지 처리를 할 때, 모서리 부분에 padding을 적용해줌으로써, 이미지의 경계를 학습시키는 효과와 유사합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data, test_data = Multi30k.splits(exts=('.de', '.en'), fields=(SRC, TRG))\n",
    "# 영어와, 독일 데이터를 다운받으면서, train, validation, test 데이터로 나눠서 가져오게 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC.build_vocab(train_data, min_freq=2)\n",
    "TRG.build_vocab(train_data, min_freq=2)\n",
    "# 최소한 2번 이상 나오는 vocab에 대해서만, numericalize 시키게 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "BATCH_SIZE = 1000\n",
    "\n",
    "train_iter, test_iter = BucketIterator.splits\\\n",
    "    (\n",
    "    (train_data, test_data), batch_size=BATCH_SIZE, device=device\n",
    "    )\n",
    "\n",
    "# device 는 cpu 또는 gpu 를 적용하게 되고, (pytorch에서는 Variable(device=device) 와 같은 형태로, \n",
    "#인풋 데이터에 대해서 device 를 할당해줍니다.\n",
    "# batch_size 를 할당해주면, 반환값에 randomly batch가 적용됩니다.(1000개씩 묶인 상태에서, 인덱스가 랜덤으로 섞인 iterator 가 됩니다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder\n",
    "- 분류 모델과 내부는 크게 다르지 않습니다. (many-to-one)\n",
    "- 마지막 결과값은 outputs 값이 아닌, hidden_layer 값을 받는 다는 것에 특징이 있습니다.\n",
    "    - 마지막 outputs 값은 반환하는 hidden 값과 같습니다.\n",
    "- encder 가 주는 hidden 값을 decoder 가 받아야 하기 때문에 약속되어야(assert?) 되어야 하는 것이 있습니다.\n",
    "    - 인코더와 디코더의 layer 층의 갯수는 같아야 합니다.(모델에서는 num_layers로 선언되어 있습니다.)\n",
    "    - 인코더와 디코더의 hidden_layer의 차원(dimension)이 같아야 합니다. (모델에서는 hidden_dim으로 선언되어 있습니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, num_layers, dropout, batch_size):\n",
    "        \n",
    "        super().__init__()     \n",
    "        self.input_dim = input_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim,padding_idx=1)\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_size = batch_size\n",
    "        self.rnn = nn.GRU(embedding_dim, hidden_dim, num_layers=num_layers, \n",
    "                            dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.hidden_dim = hidden_dim    \n",
    "        \n",
    "    def forward(self, text):\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        outputs, hidden = self.rnn(embedded)\n",
    "        # rnn 의 argument 에서 embedding_vector 만 넣고, initial_hidden이나 initial_cell을 따로 넣어주지 않게 되면, \n",
    "        # dimension 이 자동으로, 맞춰져서 0벡터로 들어가게 됩니다.\n",
    "        # hidden 은 가변 길이의 문장을 하나의 정보로 압축시키는 context vector 라고 생각하면 됩니다.\n",
    "        # 최적의 context vector는 training 과정에서 최적화되는 파라미터입니다.\n",
    "        # 이러한 convext vector는 num_layer 의 갯수만큼 있고, decoder part에서 풀게 됩니다.\n",
    "        return hidden\n",
    "    # hidden 변수 안에는 GRU의 경우 cell state가 없습니다. [1,batch_size, hidden_dim]\n",
    "    # LSTM 의 경우에는 tuple 형태로 ([1,batch_size, hidden_dim],[1,batch_size, hidden_dim])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TRG.vocab)\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "HID_DIM = 512\n",
    "N_LAYERS = 1\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "BATCH_SIZE = 1000\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT,BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1000, 512])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(train_iter))\n",
    "testing_hidden = enc(batch.src)\n",
    "testing_hidden.size()\n",
    "# [1,batch_size, hidden_dim]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder\n",
    "- 해당 논문의 Decoder 의 input 값과 output 값을 이해함을 통해 네트워크를 살펴보도록 하겠습니다.\n",
    "- Decoder 는 `세 가지 인풋값`과, `한 가지 반환값`을 가지는데, 각자의 특성에 대해 이야기해보겠습니다.\n",
    "- INput data : \n",
    "    - inputs : Encoder 의 경우에서는 한번에 sequential data를 넣어줍니다. ['I','am','a','boy']를 한번에 넣어준다는 것이죠. 정보를 압축시키는 인코더의 역할적 맥락상 당연한 듯 합니다. 하지만, Decoder의 경우에는, training 과정에서는 target data(True value)를 넣어주지 않기 때문에, 출력해야 하는 문장의 길이가 얼마가 되어야 하고, (['나','는','소년','입니다'] 와 같은..) 얼마나 잘 맞추고 있는지에 대한, 기준점이 될만한 것이 필요합니다. 따라서, 단어를 하나씩 읽어드린다는 느낌으로, 데이터를 하나씩 넣고 예측 단어가 가질 수 있는 모든 경우의 수를 늘여놓은 벡터를 반환하게 되는 것이죠. 말이 좀 길어졌지만, 아래 그림의 오른쪽의 파랑색 부분의 네트워크를 보시면 좀 더 이해가 쉬우실 것 같습니다.\n",
    "    - hidden : 처음엔 Encoder의 마지막 hidden state $h_t$ 가 같고, step을 진행함에 따라, update되는 변수입니다.        \n",
    "    - context :  Encoder의 마지막 hidden state $h_t$ 를 의미합니다.\n",
    "- OUTput data :\n",
    "    - outputs : rnn 베이스 네트워크는 input 데이터가 hidden layer를 거쳐서 output 예측 데이터를 반환하게 되는데, many-to-many 네트워크는 문장을 반환해야 하고, 하나의 output이 반환할 수 있는 경우의 수는 모든 target data가 가지고 있는 vocab의 수가 됩니다. 따라서, `[hidden_layer's dimension, num of target_data's vocab]`에 대한 linear 함수를 씌워주어, 하나의 input 당  num of target_data's vocab만큼의 길이를 가지는 벡터를 반환하게 됩니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 이번 아키텍처에 대한 보다 깊은 논의\n",
    "\n",
    "<img src ='rnn_seq2seq.png'>\n",
    "\n",
    "- 이번에는 outputs의 형태보다는, 이러한 outputs이 어떻게 형성되었는지에 대해서 이야기하는 것이 더욱 중요할 것 같습니다.\n",
    "- context vector를 모든 sequence step에 어떻게 일괄적으로 넣어줄 수 있을까요?\n",
    "\n",
    "\n",
    "1. 처음에는 Embedding vector에 context vetor를 concatenate 시켜줍니다.\n",
    "    - Embedding vector's dim : [1,batch,embedding_dim] \n",
    "    - Context vector's dim : [1,batch,hidden_dim]\n",
    "    - concated_embedding vector's dim : [1,batch, embedding_dim + hidden_dim]\n",
    "\n",
    "2. 이러한 임베딩 벡터를 RNN 네트워크에 넣고 나온 output , hidden\n",
    "\n",
    "3.  $y_{t+1}(pred) = f(y_{t},s_{t},z) $ 에 따라, outputs, hidden, context vector를 넣고 fully_connect하는 것이 최종 outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, embedding_dim, hidden_dim, num_layers, dropout):\n",
    "        \n",
    "        super().__init__()        \n",
    "        self.embedding = nn.Embedding(output_dim, embedding_dim,padding_idx=1)\n",
    "        self.hidden_dim = hidden_dim # decoder 의 hidden_dim와 같아야 합니다.\n",
    "        self.output_dim = output_dim # 나올 수 있는 target data vocab의 모든 경우의 수 길이가 됩니다.\n",
    "        self.num_layers = num_layers # decoder 의 num_layers와 같아야 합니다.\n",
    "        self.linear = nn.Linear(embedding_dim + hidden_dim*2, output_dim)\n",
    "        self.rnn = nn.GRU(embedding_dim+hidden_dim, hidden_dim, num_layers=num_layers, dropout=dropout)\n",
    "        # hidden_dim *2 인 이유는, hidden, cell 두 개가 concat 되었는데, 이러한 hidden을 fully_connected 시켜서 입니다.\n",
    "        self.dropout = nn.Dropout(dropout)        \n",
    "        \n",
    "    def forward(self, inputs, hidden, context):\n",
    "        \n",
    "        # hidden 은 초기에는 context 와 같지만, 나중에는 update되는 변수입니다.\n",
    "        # context는 계속해서 고정적으로 embedding vector에 들어가는 변수입니다.\n",
    "        \n",
    "        inputs = inputs.unsqueeze(0) \n",
    "        # input은 train_iter 또는 test_iter의 이터레이터에 있는, trg 가 됩니다. \n",
    "       \n",
    "        embedded = self.dropout(self.embedding(inputs))\n",
    "        concated_embedded = torch.cat((embedded,context),dim=2)        \n",
    "        output, hidden = self.rnn(concated_embedded,hidden)\n",
    "        \n",
    "        #output : \n",
    "        # 원래대로면 dimension 이 [문장의 길이, batch_size , hidden_dim * n_direction] 인데,\n",
    "        # 여기서 sent_length 가 무조건 1이 됩니다. 왜냐면 context vector에서 압축된 정보를 input 으로 받아오기 때문입니다.\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #cell = [n layers * n directions, batch size, hid dim]\n",
    "        # hidden , cell의 dimension은 같다. \n",
    "        output = torch.cat((embedded.squeeze(0), hidden.squeeze(0), context.squeeze(0)), \n",
    "                           dim = 1)\n",
    "        \n",
    "        outputs = self.linear(output.squeeze(0))\n",
    "        #prediction : output.squeeze_(0)'s dimension // [batch_size , hidden_dim]\n",
    "        # prediction's dimension : [batch_size , output_dim]\n",
    "        \n",
    "        return outputs , hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TRG.vocab)\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "HID_DIM = 512\n",
    "N_LAYERS = 1\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "BATCH_SIZE = 1000\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT,BATCH_SIZE)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1000, 512])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_hidden.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0775, -0.0566,  0.5983,  ..., -0.2491,  0.5507, -0.3622],\n",
       "         [ 0.2004, -0.1633,  0.4167,  ...,  0.8854,  0.2569, -1.1642],\n",
       "         [ 0.1979, -0.2888,  0.0685,  ...,  0.4897,  0.2731,  0.0900],\n",
       "         ...,\n",
       "         [ 0.2770, -0.3386, -0.0887,  ...,  0.4116,  0.5397, -1.0487],\n",
       "         [-0.3438, -0.1755,  0.1574,  ...,  0.6431,  0.6543, -0.6588],\n",
       "         [-0.4971,  0.4489,  0.5461,  ...,  0.4226,  0.2081, -0.1657]],\n",
       "        grad_fn=<AddmmBackward>),\n",
       " tensor([[[ 0.0132, -0.2921,  0.0892,  ..., -0.2381, -0.0025, -0.3103],\n",
       "          [-0.0766, -0.3606, -0.3623,  ..., -0.3297,  0.1986, -0.0025],\n",
       "          [-0.3166, -0.3405,  0.1986,  ..., -0.3240, -0.0667, -0.3141],\n",
       "          ...,\n",
       "          [-0.0831, -0.3876, -0.3821,  ...,  0.0165, -0.1277, -0.1536],\n",
       "          [-0.1792, -0.2685,  0.2522,  ..., -0.2239, -0.0652, -0.1337],\n",
       "          [ 0.2795, -0.3687, -0.0797,  ..., -0.3359,  0.3422, -0.2655]]],\n",
       "        grad_fn=<StackBackward>))"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec(batch.trg[0],testing_hidden,testing_hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2Seq 클래스에 있는 teacher_forcing_ratio에 대한 논의\n",
    "- 보다 자세한 설명은 https://machinelearningmastery.com/teacher-forcing-for-recurrent-neural-networks/ 을 참고하세요\n",
    "- RNN 베이스 뉴럴 네트워크의 대표적 단점 중 하나인 느린 수렴 속도를 극복하기 위해서, 디코더의 인풋 데이터 일부에 실제 target data를 넣어주는 것입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        batch_size = trg.shape[1]\n",
    "        max_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        # decoder 의 맨처음에는 encoder 에서 나온 hidden,cell을 넣어주어야 합니다! 이때, num_layer와 hidden_dim은 같아야 합니다!\n",
    "        hidden = self.encoder(src)\n",
    "        context = hidden.clone()\n",
    "        # decoder 를 돌면서, 각 단어에 대한 outputs값(벡터 형태)이 나오게 되는데, 이러한 값들을 아래의 outputs 변수에 저장해줍니다\n",
    "        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        # 맨 처음에는 문장의 시작을 알리는 sos(start of sentence) 토큰을 넣어주어야 합니다.\n",
    "        input_ = trg[0,:]\n",
    "        for t in range(1, max_len):\n",
    "            # for 문이 돈다는 것은, many-to-many의 네트워크가 한 칸씩 옆으로 이동한다는 뜻과 같습니다.\n",
    "            output, hidden = self.decoder(input_, hidden, context)\n",
    "#            output'dimension : [batch_size , output_dim], 여기서 output_dim 은 출현 가능한 모든 target lang 의 수 입니다.\n",
    "            outputs[t] = output\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.max(1)[1] # 해당 글자의 numericalized index 를 넣어주어야 합니다.\n",
    "            input_ = (trg[t] if teacher_force else top1)\n",
    "            # teacher_forcing 을 할 경우, 실제 trg데이터를 다음 input으로 사용, 그렇지 않을 경우, 이전 state에서 가장 높은 \n",
    "            # 값을 가진[나올 수 있는 모든 target vocab 리스트 중에서를 의미합니다. 확률값의 형태는 아니지만, 가장 개연성이 높은 단어를 의미합니다.]\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TRG.vocab)\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "HID_DIM = 512\n",
    "N_LAYERS = 1\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "BATCH_SIZE = 1000\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT,BATCH_SIZE)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
    "\n",
    "model = Seq2Seq(enc, dec, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([33, 1000, 5893])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(batch.src,batch.trg).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gradient clipping 에 대한 논의\n",
    "- https://hskimim.github.io/Avoid_Exploding_gradient_in_Neural_Net_with_gradient_clipping/ 에 보다 자세히 기록하였습니다.\n",
    "- RNN 베이스의 네트워크의 특징인 gradient exploding 을 방지해주는 방법론입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "class fit() : \n",
    "    \n",
    "    def __init__(self, model, train_iter, test_iter, epoch = 5) : \n",
    "        self.optimizer = optim.Adam(model.parameters())\n",
    "        # <pad> 토큰은 임베딩 벡터와, loss_function에 argument 로 들어가서, training 과정에서 제외됩니다.\n",
    "        self.pad_idx = TRG.vocab.stoi['<pad>'] \n",
    "        self.criterion = nn.CrossEntropyLoss(ignore_index=self.pad_idx)\n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model = model.to(device)\n",
    "        self.train_iter = train_iter\n",
    "        self.test_iter = test_iter\n",
    "        self.epoch = epoch\n",
    "            \n",
    "    def train(self,clip):\n",
    "    \n",
    "        epoch_loss = 0 # loss per epoch\n",
    "        self.model.train()\n",
    "        \n",
    "        for i, batch in enumerate(self.train_iter):\n",
    "            print('batch : ',i,end='\\r')\n",
    "            src = batch.src\n",
    "            trg = batch.trg\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            output = model(src, trg)        \n",
    "\n",
    "            loss_output = output[1:].view(-1, output.shape[-1])\n",
    "            loss_trg = trg[1:].view(-1)\n",
    "            # sos 토큰을 제외하고, 차원을 맞춘 후에, output을 변수에 저장해줍니다.\n",
    "            \n",
    "            loss = self.criterion(loss_output, loss_trg)\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "            # gradient clipping\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        return epoch_loss / len(self.train_iter)\n",
    "    \n",
    "\n",
    "    def fit_by_iterate(self,clip) : \n",
    "        \n",
    "        for epoch in range(self.epoch):\n",
    "            print('epoch : ',epoch + 1)\n",
    "            train_loss= self.train(clip)\n",
    "            print(\"epoch's loss : {}\".format(train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitting_process = fit(model,train_iter,test_iter,epoch=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch :  1\n",
      "batch :  2\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-120-31be1b4d703b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfitting_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_by_iterate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-118-a30528c37071>\u001b[0m in \u001b[0;36mfit_by_iterate\u001b[0;34m(self, clip)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'epoch : '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0mtrain_loss\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"epoch's loss : {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-118-a30528c37071>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, clip)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_trg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/engine_3.6/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/engine_3.6/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fitting_process.fit_by_iterate(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
