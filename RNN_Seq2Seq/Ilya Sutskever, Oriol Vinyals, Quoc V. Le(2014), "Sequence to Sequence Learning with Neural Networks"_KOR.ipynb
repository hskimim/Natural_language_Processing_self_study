{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 해당 노트북 파일은 Ilya Sutskever, Oriol Vinyals, Quoc V. Le(2014), \"Sequence to Sequence Learning with Neural Networks\" 논문을 기반으로 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchtext.datasets import TranslationDataset, Multi30k\n",
    "from torchtext.data import Field, BucketIterator\n",
    "\n",
    "import spacy\n",
    "\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "\n",
    "SEED = 1\n",
    "\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "spacy_de = spacy.load('de')\n",
    "spacy_en = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq2Seq 에 대한 논의\n",
    "- DNN(Deep Neural Network)은 모델에 들어가는 input, output data 의 형태(format)에 따라 나뉘어 질 수 있습니다.\n",
    "- 다른 폴더 `(CNN/, RNN_LSTM_GRU/)` 에 있는 네트워크의 경우에는, many-to-one의 형태를 가지고 있고, 대표적으로 감성 분석과 같은 분류 모델이 되겠습니다. `(문장 -> [긍정, 부정])`\n",
    "\n",
    "<img src = 'neural_network_structure.png'>\n",
    "\n",
    "\n",
    "- 이번에 다루려고 하는 Seq2Seq 모델은 말그대로 Sequence 가 들어가서 Sequence 가 나오는 모델로 many-to-many 형태의 모델입니다.\n",
    "- 이전에 분류 모델 아키텍처의 특징은 input data(문장 etc) 가 들어가서 neural nertwork 의 (deep) layer를 통과해서, sigmoid나 softmax 과 같은 값을 반환하는 형태였습니다. 즉, 문장 하나가 들어가서 sigmoid 또는 softmax 값이 딱 떨어지는 형태였던 것이죠.(many-to-one)\n",
    "\n",
    "\n",
    "- 이와는 다르게, 이번에 구현하는 Seq2seq 모델은 결과값이 딱 떨어지는 값이 아닌, input 값과 유사한 형태인 sequence 가 나와야 하기 때문에, neural network 를 `두 개를 쌓는다는 것에 특징이 있습니다.`\n",
    "- 두 개의 네트워크는 용도와 input,output에 따라 `인코더(encoder)`, `디코더(decoder)` 로 나뉘어져 있습니다.\n",
    "- Encoder 와 Decoder 의 각자의 역할에 대해서 살펴보도록 할까요?(RNN encoder-decoder 모델에 경우입니다!)\n",
    "    - Encoder : \n",
    "        - input 시퀀스 데이터(ex.문장)을 받아, 해당 문장에 대한 정보를 many-to-one 모델의 형태로 압축시키는 단계입니다.\n",
    "    - Decoder : \n",
    "        - Encoder 가 반환한 값`(context vector)`을 인풋으로 받아서, many-to-many 구조로 반환하는 단계입니다.\n",
    "        - **해당 논문에서는 context vector 가 decoder 단계에서의 initial hidden layer가 됩니다.(해당 논문의 특징이자 한계)**\n",
    "        - Encoder 가 반환한 context vector를 input으로 받는 것이 RNN-encoder-decoder 모델에서 encoder, decoder의 연결고리 역할을 합니다. \n",
    "- 보다 자세한 형태와 특징은 진행함에 따라, 부연 설명을 진행하도록 하고, 본격적으로 시작해보도록 하겠습니다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_de(text):\n",
    "    \"\"\"\n",
    "    Tokenizes German text from a string into a list of strings (tokens) and reverses it\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)][::-1]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    \"\"\"\n",
    "    Tokenizes English text from a string into a list of strings (tokens)\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "SRC = Field(tokenize=tokenize_de, init_token='<sos>', eos_token='<eos>', lower=True)\n",
    "TRG = Field(tokenize=tokenize_en, init_token='<sos>', eos_token='<eos>', lower=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `<sos>` 와 `<eos>` 에 대한 논의\n",
    "- 아시는 분도 계시겠지만, 각각 Start Of Sentence, End of Sentence 를 의미합니다.\n",
    "- Encoder와 Decoder 에 들어가는 source data, target data에 대해 앞뒤로 해당 두 토큰이 들어가게 됩니다.\n",
    "- 문장의 시작과 끝을 일괄적으로 적용시켜줌으로써, 문장의 시작점과 끝점을 알려주는 역할을 합니다.\n",
    "- CNN에서 이미지 처리를 할 때, 모서리 부분에 padding을 적용해줌으로써, 이미지의 경계를 학습시키는 효과와 유사합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data, test_data = Multi30k.splits(exts=('.de', '.en'), fields=(SRC, TRG))\n",
    "# 영어와, 독일 데이터를 다운받으면서, train, validation, test 데이터로 나눠서 가져오게 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC.build_vocab(train_data, min_freq=2)\n",
    "TRG.build_vocab(train_data, min_freq=2)\n",
    "# 최소한 2번 이상 나오는 vocab에 대해서만, numericalize 시키게 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device = 'cpu'\n",
    "BATCH_SIZE = 1000\n",
    "\n",
    "train_iter, test_iter = BucketIterator.splits\\\n",
    "    (\n",
    "    (train_data, test_data), batch_size=BATCH_SIZE, device=device\n",
    "    )\n",
    "\n",
    "# device 는 cpu 또는 gpu 를 적용하게 되고, (pytorch에서는 Variable(device=device) 와 같은 형태로, \n",
    "#인풋 데이터에 대해서 device 를 할당해줍니다.\n",
    "# batch_size 를 할당해주면, 반환값에 randomly batch가 적용됩니다.(1000개씩 묶인 상태에서, 인덱스가 랜덤으로 섞인 iterator 가 됩니다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder\n",
    "- 분류 모델과 내부는 크게 다르지 않습니다. (many-to-one)\n",
    "- 마지막 결과값은 outputs 값이 아닌, hidden_layer 값을 받는 다는 것에 특징이 있습니다.\n",
    "    - 마지막 outputs 값은 반환하는 hidden 값과 같습니다.\n",
    "- encder 가 주는 hidden 값을 decoder 가 받아야 하기 때문에 약속되어야(assert?) 되어야 하는 것이 있습니다.\n",
    "    - 인코더와 디코더의 layer 층의 갯수는 같아야 합니다.(모델에서는 num_layers로 선언되어 있습니다.)\n",
    "    - 인코더와 디코더의 hidden_layer의 차원(dimension)이 같아야 합니다. (모델에서는 hidden_dim으로 선언되어 있습니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, num_layers, dropout, batch_size):\n",
    "        \n",
    "        super().__init__()     \n",
    "        self.input_dim = input_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim,padding_idx=1)\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_size = batch_size\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, \n",
    "                            dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.hidden_dim = hidden_dim    \n",
    "        \n",
    "    def forward(self, text):\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        outputs, (hidden, cell) = self.rnn(embedded)\n",
    "        # rnn 의 argument 에서 embedding_vector 만 넣고, initial_hidden이나 initial_cell을 따로 넣어주지 않게 되면, \n",
    "        # dimension 이 자동으로, 맞춰져서 0벡터로 들어가게 됩니다.\n",
    "        # hidden 은 가변 길이의 문장을 하나의 정보로 압축시키는 context vector 라고 생각하면 됩니다.\n",
    "        # 최적의 context vector는 training 과정에서 최적화되는 파라미터입니다.\n",
    "        # 이러한 convext vector는 num_layer 의 갯수만큼 있고, decoder part에서 풀게 됩니다.\n",
    "        return hidden , cell\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder\n",
    "- 위에서 간단하게 말씀드렸던 것과 같이, Encoder 가 반환한 hidden 값을(여기서는 cell도 포함) input으로 넣어줍니다.\n",
    "- Decoder 는 `세 가지 인풋값`과, `세 가지 반환값`을 가지는데, 각자의 특성에 대해 이야기해보겠습니다.\n",
    "    - 설명의 원활함을 위해 반환값부터 이야기해보겠습니다.\n",
    "- OUTput data :\n",
    "    - outputs : rnn 베이스 네트워크는 input 데이터가 hidden layer를 거쳐서 output 예측 데이터를 반환하게 되는데, many-to-many 네트워크는 문장을 반환해야 하고, 하나의 output이 반환할 수 있는 경우의 수는 모든 target data가 가지고 있는 vocab의 수가 됩니다. 따라서, `[hidden_layer's dimension, num of target_data's vocab]`에 대한 linear 함수를 씌워주어, 하나의 input 당  num of target_data's vocab만큼의 길이를 가지는 벡터를 반환하게 됩니다. \n",
    "    - hidden,cell : rnn 네트워크는 step이 진행되면서 이전 state(t-1)의 hidden state를 누진적으로 사용하기 때문에, 함께 반환해주면서 training 과정에서, 계속해서 사용하게 됩니다.\n",
    "    \n",
    "- INput data :\n",
    "    - inputs : Encoder 의 경우에서는 한번에 sequential data를 넣어줍니다. ['I','am','a','boy']를 한번에 넣어준다는 것이죠. 정보를 압축시키는 인코더의 역할적 맥락상 당연한 듯 합니다. 하지만, Decoder의 경우에는, training 과정에서는 target data(True value)를 넣어주지 않기 때문에, 출력해야 하는 문장의 길이가 얼마가 되어야 하고, (['나','는','소년','입니다'] 와 같은..) 얼마나 잘 맞추고 있는지에 대한, 기준점이 될만한 것이 필요합니다. 따라서, 단어를 하나씩 읽어드린다는 느낌으로, 데이터를 하나씩 넣고 예측 단어가 가질 수 있는 모든 경우의 수를 늘여놓은 벡터를 반환하게 되는 것이죠. 말이 좀 길어졌지만, 아래 그림의 오른쪽의 파랑색 부분의 네트워크를 보시면 좀 더 이해가 쉬우실 것 같습니다.\n",
    "\n",
    "<img src ='rnn_seq2seq.png'>\n",
    "\n",
    "- 단어 하나를 넣고 outputs(벡터형태)와 그에 따른 hidden_state를 반환한 후, 출력으로 나온 outputs과 hidden을 쓰게 되면 하나씩 그 옆으로 이동하게 되는 맥락이 되는 것입니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, embedding_dim, hidden_dim, num_layers, dropout):\n",
    "        \n",
    "        super().__init__()        \n",
    "        self.embedding = nn.Embedding(output_dim, embedding_dim,padding_idx=1)\n",
    "        self.hidden_dim = hidden_dim # decoder 의 hidden_dim와 같아야 합니다.\n",
    "        self.output_dim = output_dim # 나올 수 있는 target data vocab의 모든 경우의 수 길이가 됩니다.\n",
    "        self.num_layers = num_layers # decoder 의 num_layers와 같아야 합니다.\n",
    "        self.linear = nn.Linear(hidden_dim, output_dim)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, inputs, hidden , cell):\n",
    "        \n",
    "        inputs = inputs.unsqueeze(0) \n",
    "        # input은 train_iter 또는 test_iter의 이터레이터에 있는, trg 가 됩니다. \n",
    "        \n",
    "        embedded = self.dropout(self.embedding(inputs))\n",
    "        output, (hidden,cell) = self.rnn(embedded,(hidden,cell))\n",
    "        \n",
    "        #output : \n",
    "        # 원래대로면 dimension 이 [문장의 길이, batch_size , hidden_dim * n_direction] 인데,\n",
    "        # 여기서 sent_length 가 무조건 1이 됩니다. 왜냐면 context vector에서 압축된 정보를 input 으로 받아오기 때문입니다.\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #cell = [n layers * n directions, batch size, hid dim]\n",
    "        # hidden , cell의 dimension은 같다. \n",
    "        \n",
    "        outputs = self.linear(output.squeeze(0))\n",
    "        #prediction : output.squeeze_(0)'s dimension // [batch_size , hidden_dim]\n",
    "        # prediction's dimension : [batch_size , output_dim]\n",
    "        \n",
    "        return outputs , hidden , cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2Seq 클래스에 있는 teacher_forcing_ratio에 대한 논의\n",
    "- 보다 자세한 설명은 https://machinelearningmastery.com/teacher-forcing-for-recurrent-neural-networks/ 을 참고하세요\n",
    "- RNN 베이스 뉴럴 네트워크의 대표적 단점 중 하나인 느린 수렴 속도를 극복하기 위해서, 디코더의 인풋 데이터 일부에 실제 target data를 넣어주는 것입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        batch_size = trg.shape[1]\n",
    "        max_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        # decoder 의 맨처음에는 encoder 에서 나온 hidden,cell을 넣어주어야 합니다! 이때, num_layer와 hidden_dim은 같아야 합니다!\n",
    "        hidden, cell = self.encoder(src)\n",
    "        \n",
    "        # decoder 를 돌면서, 각 단어에 대한 outputs값(벡터 형태)이 나오게 되는데, 이러한 값들을 아래의 outputs 변수에 저장해줍니다\n",
    "        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        # 맨 처음에는 문장의 시작을 알리는 sos(start of sentence) 토큰을 넣어주어야 합니다.\n",
    "        input_ = trg[0,:]\n",
    "        \n",
    "        for t in range(1, max_len):\n",
    "            # for 문이 돈다는 것은, many-to-many의 네트워크가 한 칸씩 옆으로 이동한다는 뜻과 같습니다.\n",
    "            output, hidden, cell = self.decoder(input_, hidden, cell)\n",
    "#            output'dimension : [batch_size , output_dim], 여기서 output_dim 은 출현 가능한 모든 target lang 의 수 입니다.\n",
    "            outputs[t] = output\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.max(1)[1] # 해당 글자의 numericalized index 를 넣어주어야 합니다.\n",
    "            input_ = (trg[t] if teacher_force else top1)\n",
    "            # teacher_forcing 을 할 경우, 실제 trg데이터를 다음 input으로 사용, 그렇지 않을 경우, 이전 state에서 가장 높은 \n",
    "            # 값을 가진[나올 수 있는 모든 target vocab 리스트 중에서를 의미합니다. 확률값의 형태는 아니지만, 가장 개연성이 높은 단어를 의미합니다.]\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TRG.vocab)\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "HID_DIM = 512\n",
    "N_LAYERS = 2\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "BATCH_SIZE = 1000\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT,BATCH_SIZE)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
    "\n",
    "model = Seq2Seq(enc, dec, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([33, 1000, 5893])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(batch.src,batch.trg).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gradient clipping 에 대한 논의\n",
    "- https://hskimim.github.io/Avoid_Exploding_gradient_in_Neural_Net_with_gradient_clipping/ 에 보다 자세히 기록하였습니다.\n",
    "- RNN 베이스의 네트워크의 특징인 gradient exploding 을 방지해주는 방법론입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class fit() : \n",
    "    \n",
    "    def __init__(self, model, train_iter, test_iter, epoch = 5) : \n",
    "        self.optimizer = optim.Adam(model.parameters())\n",
    "        # <pad> 토큰은 임베딩 벡터와, loss_function에 argument 로 들어가서, training 과정에서 제외됩니다.\n",
    "        self.pad_idx = TRG.vocab.stoi['<pad>'] \n",
    "        self.criterion = nn.CrossEntropyLoss(ignore_index=self.pad_idx)\n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model = model.to(device)\n",
    "        self.train_iter = train_iter\n",
    "        self.test_iter = test_iter\n",
    "        self.epoch = epoch\n",
    "            \n",
    "    def train(self,clip):\n",
    "    \n",
    "        epoch_loss = 0 # loss per epoch\n",
    "        self.model.train()\n",
    "        \n",
    "        for i, batch in enumerate(self.train_iter):\n",
    "            print('batch : ',i,end='\\r')\n",
    "            src = batch.src\n",
    "            trg = batch.trg\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            output = model(src, trg)        \n",
    "\n",
    "            loss_output = output[1:].view(-1, output.shape[-1])\n",
    "            loss_trg = trg[1:].view(-1)\n",
    "            # sos 토큰을 제외하고, 차원을 맞춘 후에, output을 변수에 저장해줍니다.\n",
    "            \n",
    "            loss = self.criterion(loss_output, loss_trg)\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "            # gradient clipping\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        return epoch_loss / len(self.train_iter)\n",
    "    \n",
    "\n",
    "    def fit_by_iterate(self,clip) : \n",
    "        \n",
    "        for epoch in range(self.epoch):\n",
    "            print('epoch : ',epoch + 1)\n",
    "            train_loss= self.train(clip)\n",
    "            print(\"epoch's loss : {}\".format(train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitting_process = fit(model,train_iter,test_iter,epoch=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch :  1\n",
      "epoch's loss : 4.908042167795116\n",
      "epoch :  2\n",
      "epoch's loss : 4.811491160557188\n",
      "epoch :  3\n",
      "epoch's loss : 4.744127881938014\n",
      "epoch :  4\n",
      "epoch's loss : 4.666964218534273\n",
      "epoch :  5\n",
      "epoch's loss : 4.6055425117755755\n"
     ]
    }
   ],
   "source": [
    "fitting_process.fit_by_iterate(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
