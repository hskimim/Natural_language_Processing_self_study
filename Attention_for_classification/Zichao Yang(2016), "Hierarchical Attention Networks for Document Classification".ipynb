{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "해당 페이지는 Zichao Yang1, Diyi Yang1, Chris Dyer1, Xiaodong He2, Alex Smola1, Eduard Hovy1 (2016), \"Hierarchical Attention Networks for Document Classification\" 논문에 관한 구현입니다.\n",
    "http://www.cs.cmu.edu/~./hovy/papers/16HLT-hierarchical-attention-networks.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**해당 논문의 구현은, 계층적으로 $word \\rightarrow sentence \\rightarrow document$로 되어 있어서, 데이터를 새로 manipulation 시켜주어야 했기 때문에, 최적화 과정까지는 하지 않고, 아키텍처를 쌓아가면서 이해를 돕는 정도로 진행하도록 하겠습니다. 미천한 실력 죄송합니다ㅠㅠ 가능하다면 추후에 보강하도록 하겠습니다!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchtext import datasets\n",
    "from torchtext.data import Field, BucketIterator\n",
    "\n",
    "import string\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "\n",
    "SEED = 1\n",
    "\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "해당 논문에서 분석한 데이터 셋 중 하나인 IMDB REVIEW에 대해서 분류 작업을 진행하도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n",
      "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n"
     ]
    }
   ],
   "source": [
    "# set up fields\n",
    "TEXT = Field(lower=True,  batch_first=True)\n",
    "LABEL = Field(sequential=False)\n",
    "\n",
    "train, test = datasets.IMDB.splits(TEXT, LABEL)\n",
    "device = 'cpu'\n",
    "\n",
    "# build the vocabulary\n",
    "TEXT.build_vocab(train ) #get rid of vectors=GloVe(name='6B', dim=300)\n",
    "LABEL.build_vocab(train)\n",
    "\n",
    "batch_size = 1000\n",
    "train_iter, test_iter = BucketIterator.splits(\n",
    "    (train, test), batch_sizes=(batch_size,batch_size),\n",
    "    shuffle=False,device=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_idx_dict = TEXT.vocab.stoi\n",
    "idx_to_word_dict = {val:idx for idx,val in word_to_idx_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 1196])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(train_iter))\n",
    "batch.text.size()\n",
    "# batch_size : 1000\n",
    "# max_length : 1196"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the film exposes the blatant exploitation of the chinese worker - generally female - garnering footage from the chinese business owner who shares his unashamed and delusional viewpoint  his american counterpart also as unashamed and delusional  the oppressed workers who are given a voice and  of course  the drunken americans who wear the beaded necklaces mindlessly celebrating in new orleans. <br /><br />the glimmer of hope comes when some americans are actually outraged that people making their beaded necklaces were getting paid like $0.10 per hour to do so. you also have a feeling that the workers may have a chance to escape working in the bead factory  but will probably do so when they get fed up with the punishment treatment popular with the factory owner and/or they just get too exhausted to work up to 20 hours a day of hard labor.<br /><br />i have wondered where those necklaces came from  not realizing how completely grueling and arduous it would be to make them. i just truly appreciated this film as it beautifully portrays the impact american indulgence has over something we consider relatively innocuous in our society on peoples on the other side of the world. honorable mention goes to wal-mart. it is simply amazing. and clearly  just the tip of the iceberg!'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_doc = ','.join([idx_to_word_dict[i.item()] for i in batch.text[0] if i.item() != 1]).replace(\",\",' ')\n",
    "testing_doc\n",
    "# padding 을 제외하고, 문장을 뽑아낸 결과입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  문장과 단어의 길이에 대한 논의\n",
    "- 논문에서는 단어의 길이를 $T$라고 가정하고 수식을 전개하였고, 문장의 길이는 $L$로 가정하고 했습니다.\n",
    "- 단순화(simplify)을 위해서, $T = 5$, $L = 2$이라고 하고, 패딩을 전개합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def yield_num_to_str(batch) :  \n",
    "    for num_sent in batch.text : \n",
    "        yield ','.join([idx_to_word_dict[i.item()] for i in num_sent if i.item() != 1]).replace(\",\",' ')\n",
    "# <pad>가 word length 단위로 존재하는데 저희는 sentence length단위의 패딩과 \n",
    "# word length 단위의 패딩이 모두 존재하고 있어야 하기 때문에 없애줍니다.\n",
    "def return_str_ls(iterator) :\n",
    "    str_ls = []\n",
    "    for idx,batch in enumerate(iterator) : \n",
    "        batch_ls = list(yield_num_to_str(batch))\n",
    "        str_ls.append(batch_ls)\n",
    "    for idx1,i in enumerate(str_ls) : \n",
    "        for idx2,j in enumerate(i) : \n",
    "            str_ls[idx1][idx2] = j.replace('<br /><br />',' ') # br 토큰이 있는 것도 있고 없는 것도 있으니 없애줍니다.\n",
    "    return str_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 48s, sys: 21.2 ms, total: 1min 48s\n",
      "Wall time: 1min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_str_ls = return_str_ls(train_iter)\n",
    "test_str_ls = return_str_ls(test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def split_under_punctuation(str_ls) : \n",
    "    ls = str_ls.copy()\n",
    "    for idx1 in range(len(ls)) : \n",
    "        for idx2 in range(len(ls[idx1])) : \n",
    "            process_ls = [i for i in re.split('[?!.]', ls[idx1][idx2]) if i]\n",
    "            if len(process_ls) < 2 : \n",
    "                final_ls = [i for i in re.split('[?!.]', ls[idx1][idx2]) if i] + [\"<pad>\"] * (2-len(process_ls))\n",
    "            else : \n",
    "                final_ls = [i for i in re.split('[?!.]', ls[idx1][idx2]) if i][:2] # L이 2가 됩니다.\n",
    "            ls[idx1][idx2] = final_ls\n",
    "    return ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ls = split_under_punctuation(train_str_ls)\n",
    "test_ls = split_under_punctuation(test_str_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def slicing_the_ls(str_ls) : \n",
    "    ls = str_ls.copy()\n",
    "    for idx1,i in enumerate(ls) : \n",
    "        for idx2,j in enumerate(i) : \n",
    "            for idx3,sent in enumerate(j) : \n",
    "                process_ls = [i for i in sent.split(\" \") if i]\n",
    "                if len(process_ls) < 10 :\n",
    "                    final_ls = process_ls + [\"<pad>\"] * (10-len(process_ls))\n",
    "                else : final_ls = process_ls[:10]\n",
    "                ls[idx1][idx2][idx3] = final_ls\n",
    "    return ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_ls = slicing_the_ls(train_ls)\n",
    "new_test_ls = slicing_the_ls(test_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['the',\n",
       "  'film',\n",
       "  'exposes',\n",
       "  'the',\n",
       "  'blatant',\n",
       "  'exploitation',\n",
       "  'of',\n",
       "  'the',\n",
       "  'chinese',\n",
       "  'worker'],\n",
       " ['the',\n",
       "  'glimmer',\n",
       "  'of',\n",
       "  'hope',\n",
       "  'comes',\n",
       "  'when',\n",
       "  'some',\n",
       "  'americans',\n",
       "  'are',\n",
       "  'actually']]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_train_ls[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_word_to_idx(batch_ls) : \n",
    "    ls = batch_ls.copy()\n",
    "    for idx1,i in enumerate(ls) : \n",
    "        for idx2,batch in enumerate(i) :\n",
    "            for idx3,sent in enumerate(batch) :\n",
    "                for idx4,word in enumerate(sent) : \n",
    "                    ls[idx1][idx2][idx3][idx4] = word_to_idx_dict[word]\n",
    "    return ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ls = convert_word_to_idx(new_train_ls)\n",
    "test_ls = convert_word_to_idx(new_test_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 24, 11451, 2, 4466, 2659, 5, 2, 1683, 6363]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ls[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_train_ls = torch.tensor(train_ls,dtype=torch.long,device='cpu')\n",
    "long_test_ls = torch.tensor(test_ls,dtype=torch.long,device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([25, 1000, 2, 10]), torch.Size([25, 1000, 2, 10]))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "long_train_ls.size(),long_test_ls.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    2,    24, 11451,     2,  4466,  2659,     5,     2,  1683,  6363],\n",
       "        [    2, 16928,     5,   418,   239,    50,    45,  1928,    22,   157]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "long_train_ls[0][0]\n",
    "# 아래를 보시면 알 수 있듯, 각 문장은 10개의 단어로 구성되어 있고, 하나의 문서는 2개의 문장으로 이루어져있음을 알 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Attentional Networks 에 대한 논의\n",
    "- 문서를 문장 -> 단어 로 계층적으로 표현하는 네트워크를 사용합니다.\n",
    "- 단어를 임베딩하여, Bi_LSTM 에 학습시켜, hidden states를 반환, 이를 attention layer에 넣습니다.\n",
    "- attention layer에서 이전에 학습시킨 Bi_LSTM이 반환한 context vector와 각 step의 hidden state와의 alignment를 구하게 됩니다.\n",
    "- aligment model을 softmax 취해주게 되고, 이 값은 문장을 구성하는 단어들의 집합을 가장 잘 요약하는 단어에 가중치가 차등적으로 할당되는 값을 의미하게 됩니다. 최종적으로는 각 step의 hidden states에 softmax를 곱해 weighted sum을 취합니다.\n",
    "- 이렇게 나온 context vector 하나는 문장 하나를 대변하게 되고, 이러한 context vector로 다시 Bi_LSTM을 돌리면 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word_Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, num_layers, dropout):\n",
    "        \n",
    "        super().__init__()     \n",
    "        self.input_dim = input_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim,padding_idx=1)\n",
    "        self.num_layers = num_layers\n",
    "        self.bi_rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, \n",
    "                            dropout=dropout,bidirectional=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.hidden_dim = hidden_dim    \n",
    "\n",
    "    def forward(self, text):\n",
    "        # text 는 tuple 입니다.\n",
    "        embedded = self.dropout(self.embedding(text)) #[max_length, batch_size, embedding_dim]\n",
    "        # packed_embedded 또한 tuple형태입니다. 하지만 기존의 RNN based 모델에 인자로 넣어주면 됩니다!\n",
    "        outputs, (hidden,cell) = self.bi_rnn(embedded)\n",
    "        concated_hidden = torch.cat((hidden,cell),dim=2)\n",
    "        return outputs, concated_hidden \n",
    "    # [max_length , batch_size , hidden_layer_dim*2], [2 , batch_size , hidden_layer_dim]*2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hskimim/anaconda3/envs/dss_env/lib/python3.6/site-packages/torch/nn/modules/rnn.py:46: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "input_dim = len(TEXT.vocab.itos)\n",
    "embedding_dim = 256\n",
    "hidden_dim = 128\n",
    "num_layer = 1\n",
    "dropout = 0.5\n",
    "batch_size = 1000\n",
    "enc = Word_Encoder(input_dim,embedding_dim,hidden_dim,num_layer,dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1000])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "long_train_ls[0][:,0,:].permute(1,0).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1000, 256]) torch.Size([2, 1000, 256])\n",
      "torch.Size([10, 1000, 256]) torch.Size([2, 1000, 256])\n"
     ]
    }
   ],
   "source": [
    "first_outputs,first_hidden = enc(long_train_ls[0][:,0,:].permute(1,0))\n",
    "print(first_outputs.size(), first_hidden.size())\n",
    "\n",
    "second_outputs,second_hidden = enc(long_train_ls[0][:,1,:].permute(1,0))\n",
    "print(second_outputs.size(), second_hidden.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word_Attention(nn.Module) : \n",
    "    def __init__(self, outputs, hidden_dim,context,batch_size):\n",
    "        \n",
    "        super().__init__()        \n",
    "        self.outputs = outputs # Encoder가 출력한 outputs 입니다.\n",
    "        self.hidden_dim = hidden_dim # hidden_dim의 dimension 입니다.\n",
    "        self.score_fc1 = nn.Linear(hidden_dim*2*2,hidden_dim)\n",
    "        self.score_fc2 = nn.Linear(hidden_dim*2,hidden_dim)\n",
    "        self.softmax_fc = nn.Linear(hidden_dim,1)\n",
    "        self.batch_size = batch_size\n",
    "        self.context = context\n",
    "    def forward(self) : \n",
    "        context = self.context.view(1,self.batch_size,-1) # hidden,cell을 옆으로 connect시켜줍니다.\n",
    "        outputs = self.outputs.permute(1,0,2) # [batch_size, max_length, hidden_dim]\n",
    "        context = context.permute(1,0,2) # [batch_size,1, hidden_dim]\n",
    "        \n",
    "        attention_score = torch.tanh(self.score_fc2(outputs) + self.score_fc1(context))\n",
    "        # [batch_size, max_length, hidden_dim]\n",
    "        attention_weights = torch.softmax(self.softmax_fc(attention_score),dim=1) # [batch_size, max_length, 1]\n",
    "        \n",
    "        context_vector = attention_weights * outputs#[batch_size, max_length, hidden_dim]\n",
    "        new_context_vector = torch.sum(context_vector,dim=1)#[batch_size, hidden_dim]\n",
    "        \n",
    "        return new_context_vector.unsqueeze(1) #[batch_size, 1, hidden_dim*2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 1, 256])\n",
      "torch.Size([1000, 1, 256])\n"
     ]
    }
   ],
   "source": [
    "first_word_attn = Word_Attention(first_outputs,hidden_dim,first_hidden,1000)\n",
    "first_context_vector = first_word_attn()\n",
    "print(first_context_vector.size())\n",
    "\n",
    "second_word_attn = Word_Attention(second_outputs,hidden_dim,second_hidden,1000)\n",
    "second_context_vector = second_word_attn()\n",
    "print(second_context_vector.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence vector에 대한 논의\n",
    "- 각 문장 내의 단어들이 word encoder와 attention layer를 통해 나와 각각의 context vector를 형성하였습니다.\n",
    "- 이제 문장들을 aggregate해서 문서를 대표하는 값을 만들어야 합니다.\n",
    "- 아래와 같이 concatenate를 통해서, 하나의 문장 벡터를 생성해 다시 처음부터 해줍니다!\n",
    "- **다만 이때는 임베딩을 하지 않고 아래의 sentence vector를 바로 Bi_LSTM에 넣어줍니다!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 2, 256])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_vector = torch.cat((first_context_vector,second_context_vector),dim=1)\n",
    "sentence_vector.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1000, 256])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 아래 아키텍처에 들어갈 때에 [max_length, batch_size, hidden_dim]으로 들어가야 하기 때문에, permute시켜줍니다.\n",
    "sentence_vector = sentence_vector.permute(1,0,2)\n",
    "sentence_vector.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sentence_Encoder(nn.Module):\n",
    "    def __init__(self, sentence_vec_dim, hidden_dim, num_layers):\n",
    "        \n",
    "        super().__init__()     \n",
    "        self.sentence_vec_dim = sentence_vec_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.bi_rnn = nn.LSTM(sentence_vec_dim, hidden_dim, num_layers=num_layers, \n",
    "                            dropout=dropout,bidirectional=True)\n",
    "        self.hidden_dim = hidden_dim    \n",
    "        \n",
    "    def forward(self, sentence_vector):\n",
    "        outputs, (hidden,cell) = self.bi_rnn(sentence_vector)\n",
    "        concated_hidden = torch.cat((hidden,cell),dim=2)\n",
    "        return outputs, concated_hidden \n",
    "    # [max_length , batch_size , hidden_layer_dim*2], [2 , batch_size , hidden_layer_dim]*2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hskimim/anaconda3/envs/dss_env/lib/python3.6/site-packages/torch/nn/modules/rnn.py:46: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "sentence_vec_dim = sentence_vector.size()[2] #256\n",
    "hidden_dim = 128\n",
    "num_layers = 1 # \n",
    "\n",
    "enc = Sentence_Encoder(sentence_vec_dim,hidden_dim,num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 1000, 256]), torch.Size([2, 1000, 256]))"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs, hidden = enc(sentence_vector)\n",
    "outputs.size(), hidden.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sentence_Attention(nn.Module) : \n",
    "    def __init__(self, outputs, hidden_dim,context,batch_size):\n",
    "        \n",
    "        super().__init__()        \n",
    "        self.outputs = outputs # Encoder가 출력한 outputs 입니다.\n",
    "        self.hidden_dim = hidden_dim # hidden_dim의 dimension 입니다.\n",
    "        self.score_fc1 = nn.Linear(hidden_dim*2*2,hidden_dim)\n",
    "        self.score_fc2 = nn.Linear(hidden_dim*2,hidden_dim)\n",
    "        self.softmax_fc = nn.Linear(hidden_dim,1)\n",
    "        self.batch_size = batch_size\n",
    "        self.context = context\n",
    "    def forward(self) : \n",
    "        context = self.context.view(1,self.batch_size,-1)\n",
    "        outputs = self.outputs.permute(1,0,2) # [batch_size, max_length, hidden_dim]\n",
    "        context = context.permute(1,0,2) # [batch_size,1, hidden_dim]\n",
    "        \n",
    "        attention_score = torch.tanh(self.score_fc2(outputs) + self.score_fc1(context))\n",
    "        # [batch_size, max_length, hidden_dim]\n",
    "        attention_weights = torch.softmax(self.softmax_fc(attention_score),dim=1) # [batch_size, max_length, 1]\n",
    "        \n",
    "        context_vector = attention_weights * outputs#[batch_size, max_length, hidden_dim]\n",
    "        new_context_vector = torch.sum(context_vector,dim=1)#[batch_size, hidden_dim]\n",
    "        \n",
    "        return new_context_vector.unsqueeze(1) #[batch_size, 1, hidden_dim*2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 1, 256])\n"
     ]
    }
   ],
   "source": [
    "Sentence_attn = Sentence_Attention(outputs,hidden_dim,hidden,1000)\n",
    "context_vector = Sentence_attn()\n",
    "print(context_vector.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully connected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 1, 2])"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc = nn.Linear(hidden_dim*2,2)\n",
    "softmax = torch.softmax(fc(context_vector),dim=2)\n",
    "softmax.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5109, 0.4891]], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
