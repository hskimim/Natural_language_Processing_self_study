{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "해당 페이지는 Zichao Yang1, Diyi Yang1, Chris Dyer1, Xiaodong He2, Alex Smola1, Eduard Hovy1 (2016), \"Hierarchical Attention Networks for Document Classification\" 논문에 관한 구현입니다.\n",
    "http://www.cs.cmu.edu/~./hovy/papers/16HLT-hierarchical-attention-networks.pdf\n",
    "\n",
    "__________________________________\n",
    "\n",
    "references : \n",
    "- https://github.com/pandeykartikey/Hierarchical-Attention-Network/blob/master/HAN%20yelp.ipynb\n",
    "- https://github.com/vietnguyen91/Hierarchical-attention-networks-pytorch/blob/master/src/utils.py\n",
    "- https://github.com/EdGENetworks/attention-networks-for-classification/blob/master/attention_model_validation_experiments.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data\n",
    "\n",
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import string\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "from collections import Counter,defaultdict\n",
    "from imblearn.over_sampling import *\n",
    "from bs4 import BeautifulSoup\n",
    "import itertools\n",
    "\n",
    "SEED = 1\n",
    "\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('yelp.csv')\n",
    "# https://www.kaggle.com/yelp-dataset/yelp-dataset#yelp_academic_dataset_review.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(string, max_seq_len):\n",
    "    \"\"\"\n",
    "    adapted from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = BeautifulSoup(string, \"lxml\").text\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\\"\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\\"s\", \" \\\"s\", string)\n",
    "    string = re.sub(r\"\\\"ve\", \" \\\"ve\", string)\n",
    "    string = re.sub(r\"n\\\"t\", \" n\\\"t\", string)\n",
    "    string = re.sub(r\"\\\"re\", \" \\\"re\", string)\n",
    "    string = re.sub(r\"\\\"d\", \" \\\"d\", string)\n",
    "    string = re.sub(r\"\\\"ll\", \" \\\"ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    s =string.strip().lower().split(\" \")\n",
    "    if len(s) > max_seq_len:\n",
    "        return s[0:max_seq_len] \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['text'].tolist()\n",
    "Y = (df['stars'] - 1).tolist()\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "train_test_split(X,Y, test_size=0.33, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fin/anaconda3/envs/engine_3.6/lib/python3.6/site-packages/bs4/__init__.py:272: UserWarning: \"b'.'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "/home/fin/anaconda3/envs/engine_3.6/lib/python3.6/site-packages/bs4/__init__.py:335: UserWarning: \"http://cidse.engineering.asu.edu/\n",
      "\n",
      "3.\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/home/fin/anaconda3/envs/engine_3.6/lib/python3.6/site-packages/bs4/__init__.py:335: UserWarning: \"http://www.halorescue.org/index.html\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/home/fin/anaconda3/envs/engine_3.6/lib/python3.6/site-packages/bs4/__init__.py:335: UserWarning: \"http://www.maricopa.gov/parks/white_tank/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/home/fin/anaconda3/envs/engine_3.6/lib/python3.6/site-packages/bs4/__init__.py:335: UserWarning: \"http://www.azpbs.org/checkplease/.\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/home/fin/anaconda3/envs/engine_3.6/lib/python3.6/site-packages/bs4/__init__.py:335: UserWarning: \"http://www.yelp.com/biz/quintons-bar-and-deli-iowa-city\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/home/fin/anaconda3/envs/engine_3.6/lib/python3.6/site-packages/bs4/__init__.py:335: UserWarning: \"http://bit.ly/aWqVVJ.\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train: 6700\n",
      "x_test: 3300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fin/anaconda3/envs/engine_3.6/lib/python3.6/site-packages/bs4/__init__.py:335: UserWarning: \"http://www.bbfw.com/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    }
   ],
   "source": [
    "## creates a 3D list of format paragraph[sentence[word]]\n",
    "\n",
    "def create3DList(data, max_sent_len,max_seq_len):\n",
    "    x=[]\n",
    "    for docs in data:\n",
    "        x1=[]\n",
    "        idx = 0\n",
    "        for seq in sent_tokenize(docs) :\n",
    "            x1.append(clean_str(seq,max_sent_len))\n",
    "            if(idx>=max_seq_len-1):\n",
    "                break\n",
    "            idx= idx+1\n",
    "        x.append(x1)\n",
    "    return x\n",
    "\n",
    "## Fix the maximum length of sentences in a paragraph and words in a sentence\n",
    "max_sent_len = 12; max_seq_len = 25\n",
    "\n",
    "## divides review in sentences and sentences into word creating a 3DList\n",
    "x_train = create3DList(X_train, max_sent_len,max_seq_len)\n",
    "x_test = create3DList(X_test, max_sent_len,max_seq_len)\n",
    "\n",
    "print(\"x_train: {}\".format(len(x_train)))\n",
    "print(\"x_test: {}\".format(len(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0번째 문서 처리 중이며 word_to_idx_dict의 길이는 2입니다.\n",
      "1000번째 문서 처리 중이며 word_to_idx_dict의 길이는 7499입니다.\n",
      "2000번째 문서 처리 중이며 word_to_idx_dict의 길이는 10698입니다.\n",
      "3000번째 문서 처리 중이며 word_to_idx_dict의 길이는 13034입니다.\n",
      "4000번째 문서 처리 중이며 word_to_idx_dict의 길이는 15058입니다.\n",
      "5000번째 문서 처리 중이며 word_to_idx_dict의 길이는 16777입니다.\n",
      "6000번째 문서 처리 중이며 word_to_idx_dict의 길이는 18434입니다.\n"
     ]
    }
   ],
   "source": [
    "word_to_idx_dict = {'<unk>':0,'<pad>':1}\n",
    "\n",
    "for idx,doc in enumerate(x_train) : \n",
    "    if idx % 1000 == 0 : print(\"{}번째 문서 처리 중이며 word_to_idx_dict의 길이는 {}입니다.\"\\\n",
    "                               .format(idx,len(word_to_idx_dict)))\n",
    "    for sent in doc : \n",
    "        for word in sent : \n",
    "            if word not in word_to_idx_dict.keys() :                 \n",
    "                word_to_idx_dict[word] = len(word_to_idx_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0번째 문서 처리 중이며 word_to_freq_dict의 길이는 0입니다.\n",
      "1000번째 문서 처리 중이며 word_to_freq_dict의 길이는 7497입니다.\n",
      "2000번째 문서 처리 중이며 word_to_freq_dict의 길이는 10696입니다.\n",
      "3000번째 문서 처리 중이며 word_to_freq_dict의 길이는 13032입니다.\n",
      "4000번째 문서 처리 중이며 word_to_freq_dict의 길이는 15056입니다.\n",
      "5000번째 문서 처리 중이며 word_to_freq_dict의 길이는 16775입니다.\n",
      "6000번째 문서 처리 중이며 word_to_freq_dict의 길이는 18432입니다.\n"
     ]
    }
   ],
   "source": [
    "word_to_freq_dict = defaultdict(int)\n",
    "\n",
    "for idx,doc in enumerate(x_train) : \n",
    "    if idx % 1000 == 0 : print(\"{}번째 문서 처리 중이며 word_to_freq_dict의 길이는 {}입니다.\"\\\n",
    "                               .format(idx,len(word_to_freq_dict)))\n",
    "    for sent in doc : \n",
    "        for word in sent : \n",
    "            word_to_freq_dict[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_to_idx(doc,min_freq=5) : \n",
    "    \"\"\"\n",
    "    doc : train or validation or test datasets which are composed with list within list\n",
    "    \"\"\"\n",
    "\n",
    "    min_freq_ls = [[word for word in sent if word_to_freq_dict[word] > min_freq] for sent in doc]\n",
    "    idx_dict = \\\n",
    "    [[word_to_idx_dict[word] if word in word_to_idx_dict.keys() else 0 for word in sent]\\\n",
    "     for sent in min_freq_ls]\n",
    "    return idx_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = [word_to_idx(batch) for batch in x_train]\n",
    "test_X = [word_to_idx(batch) for batch in x_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- max_sent_len : 한 문서가 가지는 최대 문장 갯수이자, 최소 문장 갯수입니다.(패딩 적용)\n",
    "- max_seq_len : 한 문장이 가지는 최대 단어 갯수이자, 최소 단어 갯수입니다. (패딩 적용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Padding the number of sentence\n",
    "train_X = [doc + [[0]]*(max_sent_len - len(doc)) if len(doc) <= max_sent_len else doc[:max_sent_len]\\\n",
    "           for doc in train_X]\n",
    "test_X = [doc + [[0]]*(max_sent_len - len(doc)) if len(doc) <= max_sent_len else doc[:max_sent_len]\\\n",
    "           for doc in test_X]\n",
    "\n",
    "## Padding the number of word\n",
    "train_X = [[sent + [0] * (max_seq_len - len(sent)) for sent in doc] for doc in train_X]\n",
    "test_X = [[sent + [0] * (max_seq_len - len(sent)) for sent in doc] for doc in test_X]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Datasets with iterators "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "한 문장 안에 있는 단어의 길이 :  {25}\n",
      "한 문서 안에 있는 문장의 길이 :  {12}\n"
     ]
    }
   ],
   "source": [
    "print(\"한 문장 안에 있는 단어의 길이 : \",set([len(sent) for doc in train_X for sent in doc]))\n",
    "print(\"한 문서 안에 있는 문장의 길이 : \",set([len(doc) for doc in train_X]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 뒤에 나오는 DataLoader는 Cuda Tensor를 지원하지 않습니다.\n",
    "train_X = torch.LongTensor(train_X)\n",
    "train_y = torch.LongTensor(y_train)\n",
    "test_X = torch.LongTensor(test_X)\n",
    "test_y = torch.LongTensor(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6700, 12, 25]),\n",
       " torch.Size([6700]),\n",
       " torch.Size([3300, 12, 25]),\n",
       " torch.Size([3300]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape, train_y.shape, test_X.shape, test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        'Initialization'\n",
    "        self.y = y\n",
    "        self.X = X\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Load data and get label\n",
    "        'Generates one sample of data'\n",
    "        # Select sample\n",
    "        X = self.X[index]\n",
    "        y = self.y[index]\n",
    "\n",
    "        return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "params = {'batch_size': 50,\n",
    "          'shuffle': True,\n",
    "          'num_workers': 6}\n",
    "\n",
    "# Generators\n",
    "training_set = Dataset(train_X,train_y)\n",
    "train_iter = data.DataLoader(training_set, **params)\n",
    "\n",
    "testing_set = Dataset(test_X,test_y)\n",
    "test_iter = data.DataLoader(testing_set, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "for local_batch, local_labels in train_iter:\n",
    "    local_batch, local_labels = local_batch.to(device), local_labels.to(device)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([50, 12, 25]), torch.Size([50]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_batch.size(),local_labels.size()\n",
    "#[batch_size, sent_len, word_len]\n",
    "# 25개의 단어를 가지고 12개의 문장을 가진 64개의 문서가 있는 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordAttention(nn.Module) : \n",
    "    \n",
    "    def __init__(self,batch_size,hidden_size) : \n",
    "        \n",
    "        super(WordAttention,self).__init__() \n",
    "        self.batch_size = batch_size\n",
    "        self.linear = nn.Linear(hidden_size*2,hidden_size*2).to(device)\n",
    "        self.word_proj_params = nn.Parameter(torch.Tensor(hidden_size*2,1)).to(device)\n",
    "        \n",
    "    def forward(self,outputs) : \n",
    "        \n",
    "        outputs = outputs.permute(1,0,2) #[batch_size, sent_len, hidden_dim*2]\n",
    "\n",
    "        u = torch.tanh(self.linear(outputs)) #[batch_size, sent_len, hidden_dim*2]        \n",
    "        word_proj_params = self.word_proj_params.expand(self.batch_size,-1,-1) #[batch_size,hidden_dim*2,1]\n",
    "        atten = torch.bmm(u,word_proj_params) #[batch_size,sent_len,1]\n",
    "        a = torch.softmax(atten,dim=1) #[batch_size,sent_len,1]\n",
    "        s = torch.sum(torch.mul(a,outputs),dim=1) #[batch_size,hidden_dim*2]\n",
    "        \n",
    "        return s,a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = Variable(torch.randn(2,50,100).cuda())\n",
    "outputs = WordRNN(50,len(word_to_idx_dict),128,100,1,12)(local_batch,hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0374, 0.0321, 0.0264, 0.0292, 0.0293, 0.0334, 0.0330, 0.0370, 0.0439,\n",
       "        0.0442, 0.0438, 0.0434, 0.0432, 0.0431, 0.0430, 0.0430, 0.0430, 0.0430,\n",
       "        0.0431, 0.0433, 0.0435, 0.0439, 0.0444, 0.0450, 0.0453],\n",
       "       device='cuda:0', grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WordAttention(50,100)(outputs)[1].squeeze()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordRNN(nn.Module) : \n",
    "    \n",
    "    def __init__(self,batch_size,vocab_size,embed_size,hidden_size,num_layer,max_sent_len) : \n",
    "        \n",
    "        super(WordRNN,self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.gru_hidden_size = hidden_size\n",
    "        self.num_layer = num_layer\n",
    "        self.max_sent_len = max_sent_len\n",
    "        \n",
    "        self.embeddings = nn.Embedding(vocab_size,embed_size,padding_idx = 1).to(device)\n",
    "        self.gru = nn.GRU(embed_size,hidden_size,num_layer,bidirectional=True).to(device)\n",
    "        \n",
    "        self.word_atten = WordAttention(batch_size,hidden_size).to(device)\n",
    "    def forward(self,input_,hidden) : \n",
    "        \n",
    "        sent_vec_ls = []; word_attention_ls = []\n",
    "        \n",
    "        for i in range(self.max_sent_len) : \n",
    "            x = input_[:,i,:]  # x : [batch_size, T :(word length per sentence)]\n",
    "            embeds = self.embeddings(x).permute(1,0,2) # [T, batch_size, embed_dim] \n",
    "            outputs, hidden = self.gru(embeds,hidden)\n",
    "            sent_vec,word_attention = self.word_atten(outputs)\n",
    "        \n",
    "            sent_vec_ls.append(sent_vec.unsqueeze(1))\n",
    "            word_attention_ls.append(word_attention.permute(0,2,1))\n",
    "        \n",
    "        sent_vec = torch.cat(sent_vec_ls,dim=1)\n",
    "        word_attention = torch.cat(word_attention_ls,dim=1)\n",
    "                \n",
    "        return sent_vec,word_attention,hidden\n",
    "    # [batch_size,sent_len,hidden_size]\n",
    "    # [batch_size,sent_len,word_len]\n",
    "    # [num_layer*bidirectional(2), batch_size, hidden_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentAttention(nn.Module) : \n",
    "    \n",
    "    def __init__(self,batch_size,hidden_size) : \n",
    "        \n",
    "        super(SentAttention,self).__init__() \n",
    "        self.batch_size = batch_size\n",
    "        self.linear = nn.Linear(hidden_size*2,hidden_size*2).to(device)\n",
    "        self.sent_proj_params = nn.Parameter(torch.Tensor(hidden_size*2,1)).to(device)\n",
    "    \n",
    "    def forward(self,outputs) : \n",
    "        \n",
    "        outputs = outputs.permute(1,0,2) #[batch_size, doc_len, hidden_dim*2]\n",
    "        u = torch.tanh(self.linear(outputs)) #[batch_size, doc_len, hidden_dim*2]\n",
    "        sent_proj_params = self.sent_proj_params.expand(self.batch_size,-1,-1) #[batch_size,hidden_dim*2,1]\n",
    "        atten = torch.bmm(u,sent_proj_params) #[batch_size,doc_len,1]\n",
    "        a = torch.softmax(atten,dim=1) #[batch_size,doc_len,1]\n",
    "        v = torch.sum(a * outputs,dim=1) #[batch_size,hidden_dim*2]\n",
    "        return v,a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentRNN(nn.Module) : \n",
    "    \n",
    "    def __init__(self,batch_size,vocab_size,embed_size,hidden_size,num_layer) : \n",
    "        \n",
    "        super(SentRNN,self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.gru_hidden_size = hidden_size\n",
    "        self.num_layer = num_layer\n",
    "        \n",
    "        self.gru = nn.GRU(hidden_size*2,hidden_size,num_layer,bidirectional=True).to(device)\n",
    "        \n",
    "        self.word_atten = SentAttention(batch_size,hidden_size)\n",
    "    def forward(self,x,hidden) : \n",
    "        \n",
    "        x = x.permute(1,0,2) #x : [doc_len,batch_size, hidden*2]\n",
    "\n",
    "        outputs, hidden = self.gru(x,hidden)\n",
    "    \n",
    "        doc_vec,sent_attention = self.word_atten(outputs)\n",
    "        \n",
    "        return doc_vec,sent_attention,hidden\n",
    "    \n",
    "    #[batch_size,hidden_dim*2]\n",
    "    #[batch_size,doc_len,1]\n",
    "    #[num_layer*2,batch_size,hidden_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HAN(nn.Module) : \n",
    "    \n",
    "    def __init__(self,batch_size,vocab_size,embed_size,hidden_size,num_layer,max_sent_len,num_class) : \n",
    "        \n",
    "        super(HAN,self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layer\n",
    "        self.max_sent_len = max_sent_len\n",
    "        self.num_class = num_class\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "        self.word_encoder =\\\n",
    "        WordRNN(batch_size,vocab_size,embed_size,hidden_size,num_layer,max_sent_len).to(self.device)\n",
    "        \n",
    "        self.sent_encoder =\\\n",
    "        SentRNN(batch_size,vocab_size,embed_size,hidden_size,num_layer).to(self.device)\n",
    "        \n",
    "        self.proj_layer = nn.Linear(hidden_size*2,num_class).to(self.device)\n",
    "        \n",
    "    def init_hidden(self,batch_size):\n",
    "        hidden = \\\n",
    "        Variable(torch.randn(self.num_layers*2, batch_size, self.hidden_size, device=self.device))\n",
    "            \n",
    "        return hidden\n",
    "    \n",
    "    def forward(self,input_) : \n",
    "        \n",
    "        (batch_size,sent_len,doc_len) = input_.size()\n",
    "        \n",
    "        word_encoder_hidden = self.init_hidden(batch_size)\n",
    "        sent_vec,word_attention,hidden = self.word_encoder(input_,word_encoder_hidden)\n",
    "        \n",
    "        sent_encoder_hidden = self.init_hidden(batch_size)\n",
    "        doc_vec,sent_attention,hidden = self.sent_encoder(sent_vec,sent_encoder_hidden)\n",
    "        \n",
    "        logit = self.proj_layer(doc_vec)\n",
    "        log_softmax = torch.log_softmax(logit,dim=1)\n",
    "        \n",
    "        return log_softmax, word_attention, sent_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HAN(\n",
       "  (word_encoder): WordRNN(\n",
       "    (embeddings): Embedding(19371, 128, padding_idx=1)\n",
       "    (gru): GRU(128, 100, bidirectional=True)\n",
       "    (word_atten): WordAttention(\n",
       "      (linear): Linear(in_features=200, out_features=200, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (sent_encoder): SentRNN(\n",
       "    (gru): GRU(200, 100, bidirectional=True)\n",
       "    (word_atten): SentAttention(\n",
       "      (linear): Linear(in_features=200, out_features=200, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (proj_layer): Linear(in_features=200, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {'batch_size' : 50,\n",
    "'vocab_size' : len(word_to_idx_dict),\n",
    "'embed_size' : 128,\n",
    "'hidden_size' : 100,\n",
    "'num_layer' : 1,\n",
    "'max_sent_len' : 12,       \n",
    "'num_class' : 5\n",
    "}\n",
    "\n",
    "model = HAN(**params).to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_softmax, word_attention, sent_attention = model(local_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([50, 5]), torch.Size([50, 12, 25]), torch.Size([50, 12, 1]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_softmax.size(), word_attention.size(), sent_attention.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainig and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch, init_lr=0.1, decay = 0.1 ,per_epoch=10):\n",
    "    \"\"\"Decay learning rate by a factor of 0.1 every lr_decay_epoch epochs.\"\"\"\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] *= 1/(1 + decay)\n",
    "\n",
    "    return optimizer , float(param_group['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,train_loader , test_loader , epochs = 10, lr = 0.01, batch_size = 50) :\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr)\n",
    "    criterion = nn.NLLLoss().to(device)\n",
    "\n",
    "    for epoch in range(1,epochs+1) :\n",
    "        optimizer , lr_int = \\\n",
    "        adjust_learning_rate(optimizer, epoch, init_lr=lr, decay = 0.1 ,per_epoch=10)\n",
    "        model.train()        \n",
    "        n_correct = 0\n",
    "        \n",
    "        for local_batch, local_labels in train_loader:\n",
    "            \n",
    "            local_batch,local_labels = local_batch.to(device),local_labels.to(device)\n",
    "        \n",
    "            train_softmax, word_attention, sent_attention = model(local_batch)\n",
    "            train_predict = train_softmax.argmax(dim=1)\n",
    "            \n",
    "            n_correct += (train_predict == local_labels).sum().item()            \n",
    "            loss = criterion(train_softmax,local_labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        acc = n_correct / (len(train_loader) * batch_size)  \n",
    "        print('Train epoch : %s,  loss : %s,  accuracy :%.3f, learning rate :%.3f'%(epoch, loss.item(), acc,lr_int))\n",
    "        print('=================================================================================================')\n",
    "        \n",
    "        if (epoch) % 2 == 0:\n",
    "            model.eval()\n",
    "            n_correct = 0  # accuracy 계산을 위해 맞은 갯수 카운트\n",
    "            val_loss = 0\n",
    "\n",
    "            for local_batch, local_labels in test_loader:\n",
    "                local_batch,local_labels = local_batch.to(device),local_labels.to(device)\n",
    "                \n",
    "                test_softmax, word_attention, sent_attention = model(local_batch)\n",
    "                test_predict = test_softmax.argmax(dim = 1)\n",
    "\n",
    "                val_loss = criterion(test_softmax, local_labels)\n",
    "                \n",
    "                n_correct += (test_predict == local_labels).sum().item() #맞은 갯수                \n",
    "\n",
    "            val_acc = n_correct / (len(test_loader) * batch_size)\n",
    "\n",
    "            print('*************************************************************************************************')\n",
    "            print('*************************************************************************************************')\n",
    "            print('Val Epoch : %s, Val Loss : %.03f , Val Accuracy : %.03f'%(epoch, val_loss, val_acc))\n",
    "            print('*************************************************************************************************')\n",
    "            print('*************************************************************************************************')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch : 1,  loss : 1.303987741470337,  accuracy :0.369, learning rate :0.009\n",
      "=================================================================================================\n",
      "Train epoch : 2,  loss : 1.100485920906067,  accuracy :0.479, learning rate :0.008\n",
      "=================================================================================================\n",
      "*************************************************************************************************\n",
      "*************************************************************************************************\n",
      "Val Epoch : 2, Val Loss : 0.987 , Val Accuracy : 0.488\n",
      "*************************************************************************************************\n",
      "*************************************************************************************************\n",
      "Train epoch : 3,  loss : 1.028915286064148,  accuracy :0.603, learning rate :0.008\n",
      "=================================================================================================\n",
      "Train epoch : 4,  loss : 0.8593549132347107,  accuracy :0.701, learning rate :0.007\n",
      "=================================================================================================\n",
      "*************************************************************************************************\n",
      "*************************************************************************************************\n",
      "Val Epoch : 4, Val Loss : 1.365 , Val Accuracy : 0.485\n",
      "*************************************************************************************************\n",
      "*************************************************************************************************\n",
      "Train epoch : 5,  loss : 0.6134281158447266,  accuracy :0.796, learning rate :0.006\n",
      "=================================================================================================\n",
      "Train epoch : 6,  loss : 0.3501947820186615,  accuracy :0.866, learning rate :0.006\n",
      "=================================================================================================\n",
      "*************************************************************************************************\n",
      "*************************************************************************************************\n",
      "Val Epoch : 6, Val Loss : 2.639 , Val Accuracy : 0.473\n",
      "*************************************************************************************************\n",
      "*************************************************************************************************\n",
      "Train epoch : 7,  loss : 0.20512044429779053,  accuracy :0.919, learning rate :0.005\n",
      "=================================================================================================\n",
      "Train epoch : 8,  loss : 0.37401556968688965,  accuracy :0.943, learning rate :0.005\n",
      "=================================================================================================\n",
      "*************************************************************************************************\n",
      "*************************************************************************************************\n",
      "Val Epoch : 8, Val Loss : 2.818 , Val Accuracy : 0.469\n",
      "*************************************************************************************************\n",
      "*************************************************************************************************\n",
      "Train epoch : 9,  loss : 0.046706896275281906,  accuracy :0.975, learning rate :0.004\n",
      "=================================================================================================\n",
      "Train epoch : 10,  loss : 0.19592639803886414,  accuracy :0.987, learning rate :0.004\n",
      "=================================================================================================\n",
      "*************************************************************************************************\n",
      "*************************************************************************************************\n",
      "Val Epoch : 10, Val Loss : 2.575 , Val Accuracy : 0.478\n",
      "*************************************************************************************************\n",
      "*************************************************************************************************\n",
      "Train epoch : 11,  loss : 0.010971278883516788,  accuracy :0.994, learning rate :0.004\n",
      "=================================================================================================\n",
      "Train epoch : 12,  loss : 0.003160333726555109,  accuracy :0.996, learning rate :0.003\n",
      "=================================================================================================\n",
      "*************************************************************************************************\n",
      "*************************************************************************************************\n",
      "Val Epoch : 12, Val Loss : 3.960 , Val Accuracy : 0.472\n",
      "*************************************************************************************************\n",
      "*************************************************************************************************\n",
      "Train epoch : 13,  loss : 0.004094715230166912,  accuracy :0.998, learning rate :0.003\n",
      "=================================================================================================\n",
      "Train epoch : 14,  loss : 0.001987552735954523,  accuracy :0.999, learning rate :0.003\n",
      "=================================================================================================\n",
      "*************************************************************************************************\n",
      "*************************************************************************************************\n",
      "Val Epoch : 14, Val Loss : 4.021 , Val Accuracy : 0.476\n",
      "*************************************************************************************************\n",
      "*************************************************************************************************\n",
      "Train epoch : 15,  loss : 0.0019478797912597656,  accuracy :0.999, learning rate :0.002\n",
      "=================================================================================================\n",
      "Train epoch : 16,  loss : 0.0061741783283650875,  accuracy :0.999, learning rate :0.002\n",
      "=================================================================================================\n",
      "*************************************************************************************************\n",
      "*************************************************************************************************\n",
      "Val Epoch : 16, Val Loss : 2.284 , Val Accuracy : 0.469\n",
      "*************************************************************************************************\n",
      "*************************************************************************************************\n",
      "Train epoch : 17,  loss : 0.0011226368369534612,  accuracy :0.999, learning rate :0.002\n",
      "=================================================================================================\n",
      "Train epoch : 18,  loss : 0.0014141654828563333,  accuracy :0.999, learning rate :0.002\n",
      "=================================================================================================\n",
      "*************************************************************************************************\n",
      "*************************************************************************************************\n",
      "Val Epoch : 18, Val Loss : 5.802 , Val Accuracy : 0.468\n",
      "*************************************************************************************************\n",
      "*************************************************************************************************\n",
      "Train epoch : 19,  loss : 0.0016419983003288507,  accuracy :0.999, learning rate :0.002\n",
      "=================================================================================================\n",
      "Train epoch : 20,  loss : 0.010960903018712997,  accuracy :1.000, learning rate :0.001\n",
      "=================================================================================================\n",
      "*************************************************************************************************\n",
      "*************************************************************************************************\n",
      "Val Epoch : 20, Val Loss : 4.497 , Val Accuracy : 0.471\n",
      "*************************************************************************************************\n",
      "*************************************************************************************************\n",
      "Train epoch : 21,  loss : 0.0007567882421426475,  accuracy :1.000, learning rate :0.001\n",
      "=================================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch : 22,  loss : 0.0011363792000338435,  accuracy :1.000, learning rate :0.001\n",
      "=================================================================================================\n",
      "*************************************************************************************************\n",
      "*************************************************************************************************\n",
      "Val Epoch : 22, Val Loss : 4.237 , Val Accuracy : 0.470\n",
      "*************************************************************************************************\n",
      "*************************************************************************************************\n",
      "Train epoch : 23,  loss : 0.0007162761758081615,  accuracy :1.000, learning rate :0.001\n",
      "=================================================================================================\n",
      "Train epoch : 24,  loss : 0.0007084274548105896,  accuracy :1.000, learning rate :0.001\n",
      "=================================================================================================\n",
      "*************************************************************************************************\n",
      "*************************************************************************************************\n",
      "Val Epoch : 24, Val Loss : 3.103 , Val Accuracy : 0.472\n",
      "*************************************************************************************************\n",
      "*************************************************************************************************\n",
      "Train epoch : 25,  loss : 0.0007150268647819757,  accuracy :1.000, learning rate :0.001\n",
      "=================================================================================================\n",
      "Train epoch : 26,  loss : 0.0006727695581503212,  accuracy :1.000, learning rate :0.001\n",
      "=================================================================================================\n",
      "*************************************************************************************************\n",
      "*************************************************************************************************\n",
      "Val Epoch : 26, Val Loss : 3.956 , Val Accuracy : 0.468\n",
      "*************************************************************************************************\n",
      "*************************************************************************************************\n",
      "Train epoch : 27,  loss : 0.0006546020740643144,  accuracy :1.000, learning rate :0.001\n",
      "=================================================================================================\n",
      "Train epoch : 28,  loss : 0.008092498406767845,  accuracy :0.999, learning rate :0.001\n",
      "=================================================================================================\n",
      "*************************************************************************************************\n",
      "*************************************************************************************************\n",
      "Val Epoch : 28, Val Loss : 4.586 , Val Accuracy : 0.465\n",
      "*************************************************************************************************\n",
      "*************************************************************************************************\n",
      "Train epoch : 29,  loss : 0.0008013629703782499,  accuracy :1.000, learning rate :0.001\n",
      "=================================================================================================\n",
      "Train epoch : 30,  loss : 0.0005919361137785017,  accuracy :1.000, learning rate :0.001\n",
      "=================================================================================================\n",
      "*************************************************************************************************\n",
      "*************************************************************************************************\n",
      "Val Epoch : 30, Val Loss : 4.931 , Val Accuracy : 0.465\n",
      "*************************************************************************************************\n",
      "*************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "train(model, train_iter, test_iter, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(test_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = model(batch[0].cuda())[0].argmax(1)\n",
    "correct_answer = batch[1].cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(predict == correct_answer).item() / 50#batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0400, 0.0400, 0.0400, 0.0400, 0.0400, 0.0400, 0.0400, 0.0400, 0.0400,\n",
       "        0.0400, 0.0400, 0.0400, 0.0400, 0.0400, 0.0400, 0.0400, 0.0400, 0.0400,\n",
       "        0.0400, 0.0400, 0.0400, 0.0400, 0.0400, 0.0400, 0.0400],\n",
       "       device='cuda:0', grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(batch[0].cuda())[1][0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "어탠션의 분포가 uniform하게 형성되어 있다. 의도는 이것이 아니였기 때문에, 굉장히 당황스럽지만 데이터의 갯수가 적기 때문에(training : 6700) 이런 결과가 나왔다고 예상해본다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
