{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "해당 노트북은 \"Effective Approaches to Attention-based Neural Machine Translation\"을 기반으로 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'de'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-6ff080387697>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSEED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeterministic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mspacy_de\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'de'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0mspacy_en\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'en'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dss_env/lib/python3.6/site-packages/spacy-2.0.18-py3.6-linux-x86_64.egg/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdepr_path\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mdeprecation_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW001\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdepr_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dss_env/lib/python3.6/site-packages/spacy-2.0.18-py3.6-linux-x86_64.egg/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'exists'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Path or Path-like to model data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'de'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchtext.datasets import TranslationDataset, Multi30k\n",
    "from torchtext.data import Field, BucketIterator\n",
    "\n",
    "import spacy\n",
    "\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "\n",
    "SEED = 1\n",
    "\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "spacy_de = spacy.load('de')\n",
    "spacy_en = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_de(text):\n",
    "    \"\"\"\n",
    "    Tokenizes German text from a string into a list of strings (tokens) and reverses it\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)][::-1]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    \"\"\"\n",
    "    Tokenizes English text from a string into a list of strings (tokens)\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "SRC = Field(tokenize=tokenize_de, init_token='<sos>', eos_token='<eos>', lower=True,include_lengths=True)\n",
    "# SRC 데이터에는 include_lengths 인자를 넣어주게 되면서, Encoder 의 pad_pack_sequence를 원활하게끔 지원해줍니다.\n",
    "TRG = Field(tokenize=tokenize_en, init_token='<sos>', eos_token='<eos>', lower=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data, test_data = Multi30k.splits(exts=('.de', '.en'), fields=(SRC, TRG))\n",
    "# 영어와, 독일 데이터를 다운받으면서, train, validation, test 데이터로 나눠서 가져오게 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC.build_vocab(train_data, min_freq=2)\n",
    "TRG.build_vocab(train_data, min_freq=2)\n",
    "# 최소한 2번 이상 나오는 vocab에 대해서만, numericalize 시키게 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device = 'cpu'\n",
    "BATCH_SIZE = 1000\n",
    "\n",
    "train_iter, test_iter = BucketIterator.splits(datasets=(train_data,test_data),\n",
    "                                            batch_sizes=(BATCH_SIZE,BATCH_SIZE),  \n",
    "                                            sort_key=lambda x: len(x.src), \n",
    "                                            device=device,\n",
    "                                            sort_within_batch=True, \n",
    "                                            repeat=False)\n",
    "\n",
    "# device 는 cpu 또는 gpu 를 적용하게 되고, (pytorch에서는 Variable(device=device) 와 같은 형태로, \n",
    "#인풋 데이터에 대해서 device 를 할당해줍니다.\n",
    "# batch_size 를 할당해주면, 반환값에 randomly batch가 적용됩니다.(1000개씩 묶인 상태에서, 인덱스가 랜덤으로 섞인 iterator 가 됩니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(batch.src)) # torchtext 의  Field 클래스에서 include_lengths 를 True로 할당해주었기 때문에, turple 을 반환합니다.\n",
    "print(batch.src[0]) # 첫 번째 인자는 input_data가 batch_size에 맞춰 들어간 상태입니다.\n",
    "print(batch.src[1][:10]) # 두 번째 인자는 각 문장 별, 길이를 의미합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder 에 대한 논의\n",
    "- 네트워크는 기존 RNN encoder-decoder 모델에 사용된 Encoder와 같은 형태입니다.\n",
    "- Attention mechanism 을 사용하기 위해서는, 이번엔 hidden layer만 반환하는 것이 아닌, sequential output인 outputs 또한 반환합니다.\n",
    "- hidden 은 RNN encoder-decoder와 같이 context vector와 같은 역할을 합니다.\n",
    "- outputs 은 아래에 나올, Attention class의 인자로 들어가게 됩니다.\n",
    "\n",
    "### pack_padded_sequence, pad_packed_sequence 에 대한 논의\n",
    "- 이번에는 RNN 네트워크의 pytorch 의 utility function인 packing 과 padding 에 대한 함수를 사용하였습니다.\n",
    "- 사용하는 이유는, RNN 네트워크의 특징 상, 배치(batch)를 돌면서 다른 길이의 데이터를 받게 되는데, (CNN의 경우 데이터의 길이는 padding으로 일치시킵니다.) 그에 따라서, 0(n^2) 이라는 연산에 대한 비효율성이 발생하게 되는데, 이러한 문제를 줄여서 연산 효율성을 추구하기 위해 진행되는 프로세스입니다.\n",
    "\n",
    "```python\n",
    "nn.utils.rnn.pack_padded_sequence(embedded, seq_length)\n",
    "```\n",
    "위의 코드에서 보게 되면, embedding vector와 seq_length라는 인자가 함께 들어가게 되는데, sequence length 란, 각 문장의 길이를 의미하게 되는 것으로, 위의 torchtext 에서 Field 의 인자 ` include_length = True` 로 선언하여 length 또한 함께 이터레이터를 돌면서 반환하게끔 해놓은 상태이기 때문에, 인덱싱을 통해 쉽게 넣어줄 수 있게 됩니다.\n",
    "\n",
    "자세한 설명은 해당 페이지를 참고하시면 좋습니다. : https://gist.github.com/HarshTrivedi/f4e7293e941b17d19058f6fb90ab0fec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'attention_pic.png'>\n",
    "\n",
    "### Attention 에 대한 논의\n",
    "- 자세한 논의는 차후에 링크되는 블로그의 논문 요약에서 보다 자세히 다루도록 하겠습니다.\n",
    "- RNN Encoder-decoder의 문제는 바로, Encoder에서 context vector 로 input_data(source data라고 주로 칭합니다.)의 information을 하나의 벡터로 압축(compress)시킨다는 것에 있습니다. 이에 따라, 한정된 정보만을 담을 수 있다는 것이 주요 단점으로 지적됩니다.\n",
    "- 해당 Attention mechanism은 이러한 문제를 해결하기 위해 제시된 것으로, source data 와 target data의 각각의 sequence 에서 각각의 sentence 간의 유사성이 존재할 것이라는 아이디어에 기반합니다.\n",
    "- 예로 들어서, \"나는 딥러닝이 좋아\" 와 \"I love deep-learning\" 두 문장이 존재할 경우, \"좋아\" 라는 단어와 \"love\"라는 단어 간의 유사성이 문맥적(contextual) 의미적(semantic) 유사성이 높을 것입니다. 이에 따라, `Encoder 의 t스텝의 hidden과 Decoder의 t-1 hidden 간의 유사성을 계산`하고, (위의 등식에서 score() 라는 함수로 계산됩니다.) 이를 `확률 형태로 반환`하여(Attention weights) source data 중에 어떤 단어가 target data의 특정 스텝에서 특정 단어와 `유사성이 가장 높은지를 확률 형태`로 알려주게 됩니다. \n",
    "- 기존에는, context vector를 Decoder의 initial hidden layer로 사용하면서, target data를 그대로 RNN 네트워크를 통해 연산을 진행해주었는데, Attention weights 를 Context vector에 곱해준 즉, 가중합(weighted sum)을 진행해주면서, 학습에 유의한(significant) 데이터에 \"집중\" 하게 만드는 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p(y_i\\vert y_1,...,y_{i-1},\\mathbf{x})=g(y_{y-1},s_i,c_i)$$\n",
    "$$\\mathbf{x} = context\\ vector$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 해당 논문의 두 가지 approach에 대한 논의\n",
    "- 해당 논문은 vanila attention mechanism 에서 보다 효과적인 접근법 두 가지를 제시합니다.\n",
    "- **global approach**\n",
    "$$score({ h }_{ t },{ \\bar { h } }_{ s })=\\begin{cases} { h }_{ t }^{ T }{ \\bar { h } }_{ s } \\\\ { h }_{ t }^{ T }{ { W }_{ a }\\bar { h } }_{ s } \\\\ { W }_{ a }[{ h }_{ t }^{ };{ \\bar { h } }_{ s }] \\end{cases}$$\n",
    "\n",
    "    - 위와 같이 총 세가지 방식을 제시합니다. 이번 노트북에서는 가장 마지막 방식인 concat 방식을 구현하겠습니다.\n",
    "- 기존 논문에서는 hidden unit을 이전 스텝에서 어탠션을 적용해 현 스텝의 hidden을 업데이트하는 $h_{t-1} \\rightarrow a_{t} \\rightarrow c_{t} \\rightarrow h_{t}$의 경로를 따랐지만, 해당 논문에서는 현 스텝에 대해 global approach를 적용하는 $h_{t} \\rightarrow a_{t} \\rightarrow c_{t} \\rightarrow h_{t}$ 과 같은 경로를 따릅니다.\n",
    "- 나머지는 기존의 어탠션 메카니즘과 같습니다.\n",
    "- **local approach**h\n",
    "- monotonic approach 와 predictive approach로 다시 나뉘게 되는데, monotonic approach는 source sentence와 target sentence의 어순이 같다고 가정하는 것으로 나이브한 방법이기 때문에, 해당 노트북에서는 predictive approach를 다루겠습니다.\n",
    "$${ p }_{ t }=S\\cdot sigmoid({ v }_{ p }^{ T } \\ tanh({ W }_{ p }{ h }_{ t }))$$\n",
    "- 위의 식은 학습을 통해서 최적화되는 $p_{t}$를 measure하는 식입니다. $p_{t}$는 중심 단어입니다. 중심 단어에서 사용자가 정한 $D$만큼의 window를 지니고 local한 영역에서만의 집중적인 attention을 가하게 됩니다.\n",
    "$${ a }_{ t }(s)=align({ h }_{ t },{ \\bar { h } }_{ s })exp(-\\frac { { (s- }{ p }_{ t })^{ 2 } }{ 2{ \\sigma }^{ 2 } } )$$\n",
    "- 기존의 어탠션 메커니즘은 전체 source sentence의 모든 hidden에 softmax를 적용하였지만, local approach에는 $p_{t}$를 중심으로 가지는 가우시안 정규분포를 따른다고 가정하고 윈도우 내에 해당하는 시퀀스만큼에만 alignment model에 가중치를 부여합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, num_layers, dropout):\n",
    "        \n",
    "        super().__init__()     \n",
    "        self.input_dim = input_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim,padding_idx=1)\n",
    "        self.num_layers = num_layers\n",
    "        self.bi_rnn = nn.GRU(embedding_dim, hidden_dim, num_layers=num_layers, \n",
    "                            dropout=dropout,bidirectional=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.hidden_dim = hidden_dim    \n",
    "        \n",
    "    def forward(self, text):\n",
    "        # text 는 tuple 입니다.\n",
    "        input_ = text[0] # tuple 의 첫 번째 엘리먼트는 input_data가 배치로 들어온 형태입니다.\n",
    "        seq_length = text[1] # tuple 의 두 번째 엘리먼트는 input_data 각각의 문장의 길이입니다.\n",
    "        embedded = self.dropout(self.embedding(input_)) #[max_length, batch_size, embedding_dim]\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, seq_length) \n",
    "        # packed_embedded 또한 tuple형태입니다. 하지만 기존의 RNN based 모델에 인자로 넣어주면 됩니다!\n",
    "        outputs, hidden = self.bi_rnn(packed_embedded)\n",
    "        outputs,_ = nn.utils.rnn.pad_packed_sequence(outputs)\n",
    "        # hidden 은 가변 길이의 문장을 하나의 정보로 압축시키는 context vector 라고 생각하면 된다. \n",
    "        # 이러한 convext vector는 num_layer 의 갯수만큼 있고, decoder part에서 풀게 된다.\n",
    "        return outputs, hidden \n",
    "    # [max_length , batch_size , hidden_layer_dim], [1 , batch_size , hidden_layer_dim] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = len(SRC.vocab.itos)\n",
    "output_dim = len(TRG.vocab.itos)\n",
    "embedding_dim = 256\n",
    "hidden_dim = 128\n",
    "num_layer = 1\n",
    "dropout = 0.5\n",
    "batch_size = 1000\n",
    "# device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device = 'cpu'\n",
    "enc = Encoder(input_dim,embedding_dim,hidden_dim,num_layer,dropout).to(device)\n",
    "batch = next(iter(train_iter))\n",
    "\n",
    "enc(batch.src)[0].size(),enc(batch.src)[1].size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global attention\n",
    "- 어탠션 함수 내에서 이전과 비교해서 변해야 할 것은 없습니다.\n",
    "$$v^T\\tanh{(WF+Vs_{i-1})}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class global_Attention(nn.Module) : \n",
    "    def __init__(self, outputs, hidden, hidden_dim,batch_size):\n",
    "        \n",
    "        super().__init__()        \n",
    "        self.outputs = outputs # Encoder가 출력한 outputs 입니다.\n",
    "        self.hidden = hidden # Encoder가 출력한 hidden 입니다.\n",
    "        self.hidden_dim = hidden_dim # hidden_dim의 dimension 입니다.\n",
    "        self.score_fc = nn.Linear(hidden_dim,hidden_dim)\n",
    "        self.softmax_fc = nn.Linear(hidden_dim,1)\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def forward(self) : \n",
    "        if self.hidden.size()[0] == 2 : self.hidden = self.hidden.view(1,self.batch_size,-1)\n",
    "        outputs = self.outputs.permute(1,0,2) # [batch_size, max_length, hidden_dim]\n",
    "        hidden = self.hidden.permute(1,0,2) # [batch_size,1, hidden_dim]\n",
    "        attention_score = torch.tanh(self.score_fc(outputs) + self.score_fc(hidden)) # [batch_size, max_length, hidden_dim]\n",
    "        attention_weights = torch.softmax(self.softmax_fc(attention_score),dim=1) # [batch_size, max_length, 1]\n",
    "        context_vector = attention_weights * outputs#[batch_size, max_length, hidden_dim]\n",
    "        new_context_vector = torch.sum(context_vector,dim=1)#[batch_size, hidden_dim]\n",
    "        return new_context_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 256])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = enc(batch.src)[0]\n",
    "hidden = enc(batch.src)[1]\n",
    "attention_obj = global_Attention(outputs, hidden, 128,1000).to(device)\n",
    "attention_obj().size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### local attention\n",
    "- 아래의 시퀀스에서 정해주어야 할 것은 로컬 어탠션 가중치입니다. \n",
    "- 윈도우 $D$를 인풋으로 받아, outputs을 인덱싱한 후에, hidden unit을 받으면 중심단어 인덱스 $p_{t}$를 최적화시키고, 이에 따른 가중치를 반환하게 해주도록 하겠습니다.\n",
    "$${ p }_{ t }=S\\cdot sigmoid({ v }_{ p }^{ T } \\ tanh({ W }_{ p }{ h }_{ t }))$$\n",
    "$${ a }_{ t }(s)=align({ h }_{ t },{ \\bar { h } }_{ s })exp(-\\frac { { (s- }{ p }_{ t })^{ 2 } }{ 2{ \\sigma }^{ 2 } } )$$\n",
    "- 논문에서 가우시안 정규 분포에 사용되는 $\\sigma$가 $D$/2 가 경험적으로 선택되었다고 해서, 이를 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class local_Attention(nn.Module) : \n",
    "    def __init__(self, outputs, hidden, hidden_dim,batch_size,window):\n",
    "        \n",
    "        super().__init__()        \n",
    "        self.outputs = outputs # Encoder가 출력한 outputs 입니다.\n",
    "        self.source_length = len(outputs) # 위의 식에서 S를 의미합니다. 인풋 시퀀스의 길이를 뜻합니다.\n",
    "        self.hidden = hidden # Encoder가 출력한 hidden 입니다.\n",
    "        self.hidden_dim = hidden_dim # hidden_dim의 dimension 입니다.\n",
    "        self.score_fc1 = nn.Linear(hidden_dim*2,hidden_dim*2)\n",
    "        self.score_fc2 = nn.Linear(hidden_dim*2,1)\n",
    "        self.softmax_fc = nn.Linear(hidden_dim*2,1)\n",
    "        self.batch_size = batch_size\n",
    "        self.window = window\n",
    "        \n",
    "    def forward(self) : \n",
    "        if self.hidden.size()[0] == 2 : self.hidden = self.hidden.view(1,self.batch_size,-1)\n",
    "        outputs = self.outputs.permute(1,0,2) # [batch_size, max_length, hidden_dim]\n",
    "        hidden = self.hidden.permute(1,0,2) # [batch_size,1, hidden_dim]\n",
    "        attention_score = self.score_fc2(torch.tanh(self.score_fc1(hidden))) # [batch_size, max_length, hidden_dim]\n",
    "        p_t = self.source_length * torch.sigmoid(attention_score).squeeze(2) #[batch_size,1]\n",
    "        return p_t\n",
    "        attention_weights = torch.softmax(self.softmax_fc(attention_score),dim=1) # [batch_size, max_length, 1]\n",
    "        context_vector = attention_weights * outputs#[batch_size, max_length, hidden_dim]\n",
    "        new_context_vector = torch.sum(context_vector,dim=1)#[batch_size, hidden_dim]\n",
    "        return new_context_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = enc(batch.src)[0]\n",
    "hidden = enc(batch.src)[1]\n",
    "attention_obj = local_Attention(outputs, hidden, 128,1000).to(device)\n",
    "p_t = attention_obj()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[36.9823],\n",
       "        [36.9860],\n",
       "        [38.7712],\n",
       "        [25.8937],\n",
       "        [36.2650],\n",
       "        [31.0808],\n",
       "        [28.3475],\n",
       "        [31.5634],\n",
       "        [37.4224],\n",
       "        [27.0372],\n",
       "        [28.0633],\n",
       "        [31.1744],\n",
       "        [34.6793],\n",
       "        [38.3863],\n",
       "        [32.6594],\n",
       "        [38.1912],\n",
       "        [31.5040],\n",
       "        [34.3540],\n",
       "        [28.7990],\n",
       "        [31.9180],\n",
       "        [33.8221],\n",
       "        [34.2336],\n",
       "        [35.1369],\n",
       "        [35.1361],\n",
       "        [31.5343],\n",
       "        [34.5316],\n",
       "        [33.8611],\n",
       "        [30.1502],\n",
       "        [34.9695],\n",
       "        [39.6244],\n",
       "        [28.1263],\n",
       "        [32.5445],\n",
       "        [33.7330],\n",
       "        [27.1689],\n",
       "        [38.5289],\n",
       "        [31.5620],\n",
       "        [38.5984],\n",
       "        [38.5842],\n",
       "        [40.5076],\n",
       "        [25.9898],\n",
       "        [32.4672],\n",
       "        [25.6716],\n",
       "        [36.7430],\n",
       "        [32.4331],\n",
       "        [32.0177],\n",
       "        [34.2443],\n",
       "        [37.1057],\n",
       "        [33.9113],\n",
       "        [33.1456],\n",
       "        [32.2619],\n",
       "        [30.6717],\n",
       "        [28.7509],\n",
       "        [31.1030],\n",
       "        [31.1006],\n",
       "        [32.9922],\n",
       "        [32.9790],\n",
       "        [33.3726],\n",
       "        [33.1270],\n",
       "        [35.3191],\n",
       "        [36.9083],\n",
       "        [35.6005],\n",
       "        [28.0009],\n",
       "        [40.3236],\n",
       "        [30.2188],\n",
       "        [29.2532],\n",
       "        [32.8041],\n",
       "        [32.4771],\n",
       "        [33.5103],\n",
       "        [32.3414],\n",
       "        [32.4129],\n",
       "        [30.1713],\n",
       "        [37.3682],\n",
       "        [39.3984],\n",
       "        [32.7929],\n",
       "        [37.7507],\n",
       "        [35.6907],\n",
       "        [31.9294],\n",
       "        [39.4914],\n",
       "        [31.8439],\n",
       "        [28.7206],\n",
       "        [27.8325],\n",
       "        [29.5405],\n",
       "        [42.9203],\n",
       "        [28.0237],\n",
       "        [36.2803],\n",
       "        [31.9941],\n",
       "        [31.4748],\n",
       "        [30.0035],\n",
       "        [29.5141],\n",
       "        [35.1979],\n",
       "        [32.8248],\n",
       "        [31.6005],\n",
       "        [31.2360],\n",
       "        [36.2709],\n",
       "        [36.8297],\n",
       "        [38.6020],\n",
       "        [27.9707],\n",
       "        [28.2170],\n",
       "        [30.3069],\n",
       "        [38.4327],\n",
       "        [34.3045],\n",
       "        [34.9204],\n",
       "        [29.1381],\n",
       "        [30.1738],\n",
       "        [35.0222],\n",
       "        [33.6263],\n",
       "        [27.6526],\n",
       "        [32.3588],\n",
       "        [30.1207],\n",
       "        [34.3509],\n",
       "        [32.8481],\n",
       "        [31.8728],\n",
       "        [30.6189],\n",
       "        [34.0960],\n",
       "        [34.9408],\n",
       "        [34.6620],\n",
       "        [31.0796],\n",
       "        [28.3393],\n",
       "        [31.2495],\n",
       "        [34.0246],\n",
       "        [40.1113],\n",
       "        [38.4642],\n",
       "        [36.4371],\n",
       "        [35.1264],\n",
       "        [35.9022],\n",
       "        [28.0159],\n",
       "        [26.7277],\n",
       "        [33.9851],\n",
       "        [34.8585],\n",
       "        [25.1024],\n",
       "        [35.3027],\n",
       "        [35.0997],\n",
       "        [34.2684],\n",
       "        [33.0553],\n",
       "        [41.4734],\n",
       "        [29.6684],\n",
       "        [34.6714],\n",
       "        [29.4907],\n",
       "        [28.2145],\n",
       "        [29.7737],\n",
       "        [34.9235],\n",
       "        [30.4881],\n",
       "        [32.3339],\n",
       "        [31.3703],\n",
       "        [31.2527],\n",
       "        [31.7339],\n",
       "        [29.6043],\n",
       "        [37.0836],\n",
       "        [31.9972],\n",
       "        [39.1379],\n",
       "        [40.7058],\n",
       "        [34.3699],\n",
       "        [37.7245],\n",
       "        [31.8245],\n",
       "        [28.2269],\n",
       "        [28.9947],\n",
       "        [33.7500],\n",
       "        [34.5880],\n",
       "        [24.6118],\n",
       "        [39.5011],\n",
       "        [28.0165],\n",
       "        [28.5379],\n",
       "        [36.5606],\n",
       "        [30.8387],\n",
       "        [36.1258],\n",
       "        [34.5295],\n",
       "        [31.1669],\n",
       "        [32.9228],\n",
       "        [35.7293],\n",
       "        [31.5988],\n",
       "        [38.8901],\n",
       "        [32.5873],\n",
       "        [28.3470],\n",
       "        [37.3191],\n",
       "        [26.7352],\n",
       "        [42.9510],\n",
       "        [34.3012],\n",
       "        [32.3466],\n",
       "        [30.4988],\n",
       "        [30.2280],\n",
       "        [35.9580],\n",
       "        [29.3798],\n",
       "        [31.4798],\n",
       "        [36.0374],\n",
       "        [31.3774],\n",
       "        [31.2309],\n",
       "        [29.8990],\n",
       "        [30.4300],\n",
       "        [30.6318],\n",
       "        [36.1903],\n",
       "        [31.9916],\n",
       "        [26.8337],\n",
       "        [35.7207],\n",
       "        [28.5659],\n",
       "        [39.3838],\n",
       "        [38.5798],\n",
       "        [31.1202],\n",
       "        [39.8931],\n",
       "        [36.9595],\n",
       "        [37.6329],\n",
       "        [39.4986],\n",
       "        [32.6517],\n",
       "        [33.0182],\n",
       "        [33.4018],\n",
       "        [39.0506],\n",
       "        [32.0886],\n",
       "        [32.7138],\n",
       "        [41.5986],\n",
       "        [30.3493],\n",
       "        [34.2017],\n",
       "        [31.9431],\n",
       "        [34.9745],\n",
       "        [34.3499],\n",
       "        [34.1308],\n",
       "        [42.5830],\n",
       "        [32.7128],\n",
       "        [27.2311],\n",
       "        [26.3098],\n",
       "        [32.1576],\n",
       "        [27.8081],\n",
       "        [34.1491],\n",
       "        [30.6054],\n",
       "        [27.3226],\n",
       "        [32.9622],\n",
       "        [28.2326],\n",
       "        [31.6693],\n",
       "        [38.3758],\n",
       "        [28.6813],\n",
       "        [37.3785],\n",
       "        [35.1575],\n",
       "        [35.1564],\n",
       "        [38.0585],\n",
       "        [35.1914],\n",
       "        [35.1763],\n",
       "        [36.5807],\n",
       "        [33.2981],\n",
       "        [32.8341],\n",
       "        [30.3387],\n",
       "        [30.0692],\n",
       "        [32.8584],\n",
       "        [30.7029],\n",
       "        [31.9086],\n",
       "        [34.3811],\n",
       "        [25.9161],\n",
       "        [26.9147],\n",
       "        [34.0369],\n",
       "        [36.7211],\n",
       "        [29.5145],\n",
       "        [31.7892],\n",
       "        [32.5843],\n",
       "        [32.5463],\n",
       "        [30.8800],\n",
       "        [35.7354],\n",
       "        [35.8028],\n",
       "        [36.8178],\n",
       "        [35.1300],\n",
       "        [33.9008],\n",
       "        [25.6406],\n",
       "        [31.2294],\n",
       "        [35.8235],\n",
       "        [28.8653],\n",
       "        [31.2225],\n",
       "        [32.0690],\n",
       "        [32.3517],\n",
       "        [38.7701],\n",
       "        [32.6153],\n",
       "        [30.1790],\n",
       "        [44.5125],\n",
       "        [24.0148],\n",
       "        [36.5796],\n",
       "        [26.6577],\n",
       "        [35.4973],\n",
       "        [31.5672],\n",
       "        [28.7624],\n",
       "        [27.6405],\n",
       "        [33.0660],\n",
       "        [28.8335],\n",
       "        [27.3825],\n",
       "        [32.5256],\n",
       "        [27.2823],\n",
       "        [29.6157],\n",
       "        [27.9631],\n",
       "        [39.3989],\n",
       "        [30.6520],\n",
       "        [30.4756],\n",
       "        [35.2831],\n",
       "        [31.2883],\n",
       "        [39.8003],\n",
       "        [40.8087],\n",
       "        [34.0364],\n",
       "        [34.1435],\n",
       "        [30.4550],\n",
       "        [25.5279],\n",
       "        [37.1620],\n",
       "        [35.7476],\n",
       "        [36.1990],\n",
       "        [28.2869],\n",
       "        [35.2889],\n",
       "        [37.6959],\n",
       "        [34.5702],\n",
       "        [30.0273],\n",
       "        [32.4090],\n",
       "        [22.8211],\n",
       "        [32.8817],\n",
       "        [30.4484],\n",
       "        [28.8158],\n",
       "        [32.2833],\n",
       "        [35.8072],\n",
       "        [36.4747],\n",
       "        [30.5129],\n",
       "        [32.7002],\n",
       "        [33.1825],\n",
       "        [29.2077],\n",
       "        [33.1226],\n",
       "        [32.7528],\n",
       "        [32.7067],\n",
       "        [30.2049],\n",
       "        [28.8462],\n",
       "        [46.7678],\n",
       "        [29.9616],\n",
       "        [40.5038],\n",
       "        [34.6056],\n",
       "        [28.4109],\n",
       "        [31.1343],\n",
       "        [38.4427],\n",
       "        [37.0378],\n",
       "        [27.3334],\n",
       "        [36.0720],\n",
       "        [31.1955],\n",
       "        [33.6025],\n",
       "        [33.0946],\n",
       "        [34.0383],\n",
       "        [30.1152],\n",
       "        [37.9537],\n",
       "        [39.1714],\n",
       "        [31.7238],\n",
       "        [30.9499],\n",
       "        [35.6709],\n",
       "        [43.5537],\n",
       "        [30.4235],\n",
       "        [35.4215],\n",
       "        [28.7151],\n",
       "        [36.2849],\n",
       "        [32.8419],\n",
       "        [25.9363],\n",
       "        [41.0048],\n",
       "        [27.8042],\n",
       "        [35.2734],\n",
       "        [30.8850],\n",
       "        [30.6022],\n",
       "        [33.5369],\n",
       "        [26.3074],\n",
       "        [39.4832],\n",
       "        [33.7872],\n",
       "        [29.9575],\n",
       "        [32.0425],\n",
       "        [30.1406],\n",
       "        [41.6516],\n",
       "        [37.7809],\n",
       "        [31.5603],\n",
       "        [31.8289],\n",
       "        [40.7403],\n",
       "        [38.5935],\n",
       "        [34.2882],\n",
       "        [34.1743],\n",
       "        [34.2664],\n",
       "        [39.0085],\n",
       "        [32.8169],\n",
       "        [33.9128],\n",
       "        [32.8705],\n",
       "        [28.7801],\n",
       "        [30.8879],\n",
       "        [42.7868],\n",
       "        [32.9744],\n",
       "        [35.6143],\n",
       "        [38.8700],\n",
       "        [32.1213],\n",
       "        [34.2247],\n",
       "        [26.7512],\n",
       "        [31.8332],\n",
       "        [27.9260],\n",
       "        [36.2875],\n",
       "        [34.6282],\n",
       "        [36.8628],\n",
       "        [30.3209],\n",
       "        [34.9651],\n",
       "        [30.7887],\n",
       "        [34.4535],\n",
       "        [36.3174],\n",
       "        [29.8879],\n",
       "        [36.4124],\n",
       "        [33.1563],\n",
       "        [37.6347],\n",
       "        [32.2281],\n",
       "        [27.9121],\n",
       "        [35.6233],\n",
       "        [25.2918],\n",
       "        [31.3593],\n",
       "        [36.2855],\n",
       "        [34.5135],\n",
       "        [36.4933],\n",
       "        [30.8558],\n",
       "        [33.0270],\n",
       "        [31.1164],\n",
       "        [32.4162],\n",
       "        [30.1181],\n",
       "        [32.4223],\n",
       "        [37.7089],\n",
       "        [34.9962],\n",
       "        [25.0506],\n",
       "        [25.9041],\n",
       "        [34.7719],\n",
       "        [27.8121],\n",
       "        [30.9211],\n",
       "        [36.2743],\n",
       "        [26.6677],\n",
       "        [33.8131],\n",
       "        [31.5078],\n",
       "        [31.9738],\n",
       "        [30.5111],\n",
       "        [32.0443],\n",
       "        [26.6503],\n",
       "        [24.6226],\n",
       "        [36.3288],\n",
       "        [40.1531],\n",
       "        [33.2017],\n",
       "        [30.0226],\n",
       "        [33.0185],\n",
       "        [32.3376],\n",
       "        [35.4331],\n",
       "        [40.3569],\n",
       "        [33.0046],\n",
       "        [29.5793],\n",
       "        [29.5448],\n",
       "        [25.5603],\n",
       "        [35.9090],\n",
       "        [36.4728],\n",
       "        [32.3365],\n",
       "        [29.1036],\n",
       "        [29.3427],\n",
       "        [35.0272],\n",
       "        [33.7938],\n",
       "        [25.4784],\n",
       "        [36.9416],\n",
       "        [28.7809],\n",
       "        [36.0838],\n",
       "        [41.1412],\n",
       "        [37.6484],\n",
       "        [37.8629],\n",
       "        [33.4436],\n",
       "        [36.1379],\n",
       "        [31.9584],\n",
       "        [28.0604],\n",
       "        [35.1946],\n",
       "        [37.6111],\n",
       "        [29.1537],\n",
       "        [29.8884],\n",
       "        [25.4229],\n",
       "        [35.1972],\n",
       "        [32.9871],\n",
       "        [36.5074],\n",
       "        [34.8614],\n",
       "        [32.6238],\n",
       "        [27.8881],\n",
       "        [32.8418],\n",
       "        [30.7547],\n",
       "        [31.2961],\n",
       "        [28.9438],\n",
       "        [31.7531],\n",
       "        [26.7792],\n",
       "        [31.1109],\n",
       "        [33.9092],\n",
       "        [42.0328],\n",
       "        [37.7182],\n",
       "        [28.0472],\n",
       "        [35.8244],\n",
       "        [28.7684],\n",
       "        [34.1086],\n",
       "        [40.7203],\n",
       "        [38.5154],\n",
       "        [30.9887],\n",
       "        [21.2924],\n",
       "        [38.4682],\n",
       "        [33.8693],\n",
       "        [31.5781],\n",
       "        [28.4909],\n",
       "        [25.3559],\n",
       "        [36.8601],\n",
       "        [29.4086],\n",
       "        [34.1443],\n",
       "        [37.5106],\n",
       "        [28.7743],\n",
       "        [33.3455],\n",
       "        [41.9851],\n",
       "        [37.1696],\n",
       "        [36.7578],\n",
       "        [34.9642],\n",
       "        [31.8113],\n",
       "        [35.5546],\n",
       "        [40.0519],\n",
       "        [35.3113],\n",
       "        [40.9580],\n",
       "        [34.5345],\n",
       "        [38.6179],\n",
       "        [33.5637],\n",
       "        [35.6695],\n",
       "        [33.8169],\n",
       "        [43.7034],\n",
       "        [35.0637],\n",
       "        [25.4642],\n",
       "        [42.4939],\n",
       "        [33.1821],\n",
       "        [43.0445],\n",
       "        [35.3885],\n",
       "        [34.8595],\n",
       "        [31.9122],\n",
       "        [33.5833],\n",
       "        [34.7542],\n",
       "        [36.4849],\n",
       "        [37.5787],\n",
       "        [31.5216],\n",
       "        [39.7272],\n",
       "        [30.9217],\n",
       "        [36.3654],\n",
       "        [35.2857],\n",
       "        [35.0425],\n",
       "        [42.3886],\n",
       "        [36.7050],\n",
       "        [29.4949],\n",
       "        [37.7321],\n",
       "        [37.2367],\n",
       "        [33.6669],\n",
       "        [41.2241],\n",
       "        [37.6498],\n",
       "        [33.5528],\n",
       "        [33.6681],\n",
       "        [35.0084],\n",
       "        [38.2408],\n",
       "        [36.1110],\n",
       "        [30.9568],\n",
       "        [35.2891],\n",
       "        [34.6134],\n",
       "        [29.6221],\n",
       "        [37.4649],\n",
       "        [42.6023],\n",
       "        [36.3855],\n",
       "        [30.4809],\n",
       "        [31.9309],\n",
       "        [29.8788],\n",
       "        [39.3573],\n",
       "        [27.6149],\n",
       "        [28.4664],\n",
       "        [34.9760],\n",
       "        [39.4717],\n",
       "        [36.4030],\n",
       "        [36.3651],\n",
       "        [34.2397],\n",
       "        [28.5367],\n",
       "        [42.3324],\n",
       "        [34.5394],\n",
       "        [33.2589],\n",
       "        [39.1553],\n",
       "        [34.1922],\n",
       "        [43.0840],\n",
       "        [35.5095],\n",
       "        [36.6992],\n",
       "        [34.6330],\n",
       "        [31.0324],\n",
       "        [30.2231],\n",
       "        [33.0773],\n",
       "        [33.8679],\n",
       "        [33.5648],\n",
       "        [29.9160],\n",
       "        [40.7742],\n",
       "        [31.8355],\n",
       "        [32.3867],\n",
       "        [33.2670],\n",
       "        [31.9616],\n",
       "        [33.0283],\n",
       "        [35.0171],\n",
       "        [39.9094],\n",
       "        [42.7906],\n",
       "        [33.2553],\n",
       "        [32.5579],\n",
       "        [41.3581],\n",
       "        [35.9876],\n",
       "        [34.4790],\n",
       "        [35.9234],\n",
       "        [30.4710],\n",
       "        [35.5289],\n",
       "        [35.6934],\n",
       "        [38.1311],\n",
       "        [35.5076],\n",
       "        [36.6857],\n",
       "        [38.1073],\n",
       "        [39.0135],\n",
       "        [33.4985],\n",
       "        [36.5725],\n",
       "        [37.2239],\n",
       "        [42.3195],\n",
       "        [40.3770],\n",
       "        [29.7236],\n",
       "        [34.3829],\n",
       "        [36.2951],\n",
       "        [37.1326],\n",
       "        [32.8303],\n",
       "        [37.4723],\n",
       "        [35.4640],\n",
       "        [41.7738],\n",
       "        [33.3640],\n",
       "        [34.6764],\n",
       "        [39.5201],\n",
       "        [35.4367],\n",
       "        [37.1649],\n",
       "        [36.6560],\n",
       "        [33.7856],\n",
       "        [35.8015],\n",
       "        [30.3935],\n",
       "        [36.8262],\n",
       "        [33.0974],\n",
       "        [40.2252],\n",
       "        [37.4356],\n",
       "        [39.8460],\n",
       "        [34.0235],\n",
       "        [38.5029],\n",
       "        [37.0476],\n",
       "        [32.3998],\n",
       "        [34.6011],\n",
       "        [38.5368],\n",
       "        [37.0109],\n",
       "        [33.9326],\n",
       "        [44.5189],\n",
       "        [39.8475],\n",
       "        [37.1813],\n",
       "        [34.6369],\n",
       "        [42.0463],\n",
       "        [39.7195],\n",
       "        [39.3248],\n",
       "        [40.5720],\n",
       "        [35.3752],\n",
       "        [32.2253],\n",
       "        [37.8041],\n",
       "        [33.5190],\n",
       "        [35.6138],\n",
       "        [31.0740],\n",
       "        [35.1091],\n",
       "        [39.7297],\n",
       "        [39.3017],\n",
       "        [37.2073],\n",
       "        [39.3718],\n",
       "        [33.1226],\n",
       "        [35.0863],\n",
       "        [33.4143],\n",
       "        [38.6295],\n",
       "        [33.7237],\n",
       "        [31.9644],\n",
       "        [35.3352],\n",
       "        [40.6344],\n",
       "        [37.6877],\n",
       "        [38.2948],\n",
       "        [43.9783],\n",
       "        [40.1082],\n",
       "        [38.2003],\n",
       "        [35.2467],\n",
       "        [38.0002],\n",
       "        [34.8008],\n",
       "        [33.8354],\n",
       "        [38.9472],\n",
       "        [38.8664],\n",
       "        [29.9137],\n",
       "        [33.4458],\n",
       "        [35.0740],\n",
       "        [36.9205],\n",
       "        [27.6449],\n",
       "        [40.2136],\n",
       "        [32.4251],\n",
       "        [38.3886],\n",
       "        [37.1839],\n",
       "        [37.3144],\n",
       "        [36.0970],\n",
       "        [35.5092],\n",
       "        [39.2762],\n",
       "        [37.1801],\n",
       "        [32.8690],\n",
       "        [35.0965],\n",
       "        [31.8819],\n",
       "        [31.0702],\n",
       "        [37.0401],\n",
       "        [39.4991],\n",
       "        [33.8626],\n",
       "        [29.1997],\n",
       "        [31.6126],\n",
       "        [38.5122],\n",
       "        [42.2635],\n",
       "        [35.5399],\n",
       "        [31.8699],\n",
       "        [36.0067],\n",
       "        [40.8076],\n",
       "        [34.5131],\n",
       "        [37.5951],\n",
       "        [40.1218],\n",
       "        [32.2921],\n",
       "        [35.8153],\n",
       "        [39.6439],\n",
       "        [38.0836],\n",
       "        [31.3600],\n",
       "        [30.4304],\n",
       "        [39.1272],\n",
       "        [30.3999],\n",
       "        [41.5773],\n",
       "        [37.0373],\n",
       "        [34.9962],\n",
       "        [34.7083],\n",
       "        [33.1857],\n",
       "        [34.2285],\n",
       "        [37.2115],\n",
       "        [36.1215],\n",
       "        [35.8593],\n",
       "        [35.7144],\n",
       "        [38.7711],\n",
       "        [34.7725],\n",
       "        [34.5084],\n",
       "        [36.1937],\n",
       "        [32.1289],\n",
       "        [31.9611],\n",
       "        [43.8135],\n",
       "        [33.2854],\n",
       "        [34.5629],\n",
       "        [34.5171],\n",
       "        [44.4548],\n",
       "        [32.2641],\n",
       "        [36.1163],\n",
       "        [28.4481],\n",
       "        [34.4537],\n",
       "        [37.0787],\n",
       "        [34.1772],\n",
       "        [35.9578],\n",
       "        [35.4098],\n",
       "        [42.2202],\n",
       "        [31.7215],\n",
       "        [40.4387],\n",
       "        [39.0903],\n",
       "        [38.7122],\n",
       "        [39.1039],\n",
       "        [36.1912],\n",
       "        [43.0142],\n",
       "        [39.4576],\n",
       "        [36.8967],\n",
       "        [32.3549],\n",
       "        [33.9790],\n",
       "        [39.1806],\n",
       "        [38.5827],\n",
       "        [38.0134],\n",
       "        [37.9068],\n",
       "        [36.7896],\n",
       "        [38.0539],\n",
       "        [33.5722],\n",
       "        [38.4625],\n",
       "        [36.0365],\n",
       "        [33.0413],\n",
       "        [31.2793],\n",
       "        [46.3761],\n",
       "        [41.7375],\n",
       "        [41.4866],\n",
       "        [40.0311],\n",
       "        [38.6246],\n",
       "        [38.7741],\n",
       "        [31.7418],\n",
       "        [40.4846],\n",
       "        [34.8425],\n",
       "        [43.4747],\n",
       "        [39.8794],\n",
       "        [30.4522],\n",
       "        [37.5038],\n",
       "        [39.0781],\n",
       "        [37.9875],\n",
       "        [30.3675],\n",
       "        [41.8507],\n",
       "        [37.0136],\n",
       "        [36.5894],\n",
       "        [37.4876],\n",
       "        [40.5191],\n",
       "        [34.7974],\n",
       "        [36.2550],\n",
       "        [39.9491],\n",
       "        [36.9225],\n",
       "        [38.6309],\n",
       "        [39.0076],\n",
       "        [40.3879],\n",
       "        [30.2901],\n",
       "        [38.9103],\n",
       "        [38.4855],\n",
       "        [37.1423],\n",
       "        [39.7228],\n",
       "        [35.2144],\n",
       "        [40.0057],\n",
       "        [28.8260],\n",
       "        [35.7541],\n",
       "        [34.5250],\n",
       "        [31.6983],\n",
       "        [36.5494],\n",
       "        [37.0972],\n",
       "        [35.4679],\n",
       "        [29.4133],\n",
       "        [42.6565],\n",
       "        [34.7386],\n",
       "        [38.9108],\n",
       "        [32.1761],\n",
       "        [33.4625],\n",
       "        [39.7890],\n",
       "        [38.9625],\n",
       "        [31.0571],\n",
       "        [33.3649],\n",
       "        [45.1655],\n",
       "        [34.7896],\n",
       "        [33.2755],\n",
       "        [34.6628],\n",
       "        [32.1117],\n",
       "        [34.4595],\n",
       "        [45.1199],\n",
       "        [34.4452],\n",
       "        [36.3888],\n",
       "        [37.5539],\n",
       "        [33.9954],\n",
       "        [38.4090],\n",
       "        [31.8697],\n",
       "        [39.5908],\n",
       "        [29.1190],\n",
       "        [35.7132],\n",
       "        [28.6351],\n",
       "        [34.9757],\n",
       "        [34.6355],\n",
       "        [37.2505],\n",
       "        [37.9304],\n",
       "        [36.6435],\n",
       "        [34.3047],\n",
       "        [32.8267],\n",
       "        [38.8973],\n",
       "        [43.5405],\n",
       "        [34.9878],\n",
       "        [33.4135],\n",
       "        [34.1334],\n",
       "        [33.2280],\n",
       "        [38.8216],\n",
       "        [34.4055],\n",
       "        [39.1970],\n",
       "        [28.0004],\n",
       "        [41.2233],\n",
       "        [40.2061],\n",
       "        [40.5143],\n",
       "        [46.0484],\n",
       "        [35.4808],\n",
       "        [32.6547],\n",
       "        [35.6937],\n",
       "        [40.7468],\n",
       "        [35.7149],\n",
       "        [34.2043],\n",
       "        [34.6717],\n",
       "        [42.0210],\n",
       "        [32.5719],\n",
       "        [33.0258],\n",
       "        [37.7095],\n",
       "        [33.1764],\n",
       "        [40.0421],\n",
       "        [34.8793],\n",
       "        [34.6110],\n",
       "        [39.9526],\n",
       "        [30.8189],\n",
       "        [30.3659],\n",
       "        [45.2901],\n",
       "        [31.2950],\n",
       "        [38.1020],\n",
       "        [43.3236],\n",
       "        [33.4893],\n",
       "        [31.4591],\n",
       "        [36.9057],\n",
       "        [44.1577],\n",
       "        [38.5124],\n",
       "        [27.9081],\n",
       "        [33.3884],\n",
       "        [28.4880],\n",
       "        [31.9686],\n",
       "        [42.9386],\n",
       "        [42.1837],\n",
       "        [40.9613],\n",
       "        [36.9649],\n",
       "        [38.0022],\n",
       "        [33.0419],\n",
       "        [34.1651],\n",
       "        [41.6462],\n",
       "        [38.5611],\n",
       "        [35.8257],\n",
       "        [30.3617],\n",
       "        [36.0132],\n",
       "        [35.6318],\n",
       "        [33.6166],\n",
       "        [33.6358],\n",
       "        [36.1486],\n",
       "        [40.4883],\n",
       "        [34.7140],\n",
       "        [36.0666],\n",
       "        [33.4357],\n",
       "        [38.7336],\n",
       "        [33.5632],\n",
       "        [36.2855],\n",
       "        [40.9829],\n",
       "        [42.7214],\n",
       "        [31.7834],\n",
       "        [33.3903],\n",
       "        [38.5258],\n",
       "        [37.9544],\n",
       "        [37.4592],\n",
       "        [39.1852],\n",
       "        [30.7065],\n",
       "        [35.6518],\n",
       "        [35.7533],\n",
       "        [36.9751],\n",
       "        [30.4586],\n",
       "        [37.7683],\n",
       "        [34.5691],\n",
       "        [40.3244],\n",
       "        [39.1917],\n",
       "        [31.9914],\n",
       "        [30.9914],\n",
       "        [42.0890],\n",
       "        [37.1102],\n",
       "        [33.8129],\n",
       "        [38.0855],\n",
       "        [32.3419],\n",
       "        [34.6346],\n",
       "        [41.6271],\n",
       "        [32.5116],\n",
       "        [33.8952],\n",
       "        [32.7064],\n",
       "        [40.1824],\n",
       "        [31.6201],\n",
       "        [36.3871],\n",
       "        [37.0485],\n",
       "        [28.7342],\n",
       "        [41.7170],\n",
       "        [26.0126],\n",
       "        [43.9566],\n",
       "        [34.5047],\n",
       "        [35.4204],\n",
       "        [37.0999],\n",
       "        [36.6231],\n",
       "        [32.6283],\n",
       "        [37.0158],\n",
       "        [33.8166],\n",
       "        [36.3436],\n",
       "        [38.3833],\n",
       "        [39.6226],\n",
       "        [36.0240],\n",
       "        [36.5926],\n",
       "        [40.4638],\n",
       "        [34.4602],\n",
       "        [37.6847],\n",
       "        [31.1550],\n",
       "        [33.9860],\n",
       "        [29.4102],\n",
       "        [26.4939],\n",
       "        [33.7824],\n",
       "        [40.0096],\n",
       "        [27.9065],\n",
       "        [31.4991],\n",
       "        [31.7245],\n",
       "        [34.0943],\n",
       "        [31.9107],\n",
       "        [40.9951],\n",
       "        [36.8335],\n",
       "        [37.7960],\n",
       "        [37.6336],\n",
       "        [32.6604],\n",
       "        [35.0101],\n",
       "        [36.6050],\n",
       "        [38.8393],\n",
       "        [39.7194],\n",
       "        [38.4220],\n",
       "        [36.4390],\n",
       "        [30.3126],\n",
       "        [34.2657],\n",
       "        [32.0830],\n",
       "        [32.3083],\n",
       "        [34.1576],\n",
       "        [32.7328],\n",
       "        [37.7673],\n",
       "        [40.6033],\n",
       "        [34.6364],\n",
       "        [32.9274],\n",
       "        [40.0985],\n",
       "        [37.5098],\n",
       "        [35.8441],\n",
       "        [35.1000],\n",
       "        [43.8765],\n",
       "        [36.0510],\n",
       "        [39.1452],\n",
       "        [33.3294],\n",
       "        [34.3922],\n",
       "        [38.1882],\n",
       "        [40.6694]], grad_fn=<PowBackward0>)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((len(outputs) - p_t).pow(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(-2*(10/2)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'pow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-78b38715375f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mp_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'pow'"
     ]
    }
   ],
   "source": [
    "torch.exp(((len(outputs) - p_t).pow(2)) / (-2*(10/2).pow(2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention score size : torch.Size([1000, 1, 1])\n",
      "p size : torch.Size([1000, 1, 1])\n",
      "tensor([[[6.1985]],\n",
      "\n",
      "        [[6.4650]],\n",
      "\n",
      "        [[6.4767]],\n",
      "\n",
      "        [[6.2295]],\n",
      "\n",
      "        [[5.5962]],\n",
      "\n",
      "        [[6.3978]],\n",
      "\n",
      "        [[6.2481]],\n",
      "\n",
      "        [[5.8138]],\n",
      "\n",
      "        [[5.0910]],\n",
      "\n",
      "        [[5.9473]],\n",
      "\n",
      "        [[6.5418]],\n",
      "\n",
      "        [[5.3306]],\n",
      "\n",
      "        [[5.4659]],\n",
      "\n",
      "        [[5.6439]],\n",
      "\n",
      "        [[6.6474]],\n",
      "\n",
      "        [[5.9536]],\n",
      "\n",
      "        [[6.6605]],\n",
      "\n",
      "        [[6.2689]],\n",
      "\n",
      "        [[6.0662]],\n",
      "\n",
      "        [[7.0427]],\n",
      "\n",
      "        [[6.7242]],\n",
      "\n",
      "        [[6.2808]],\n",
      "\n",
      "        [[5.8809]],\n",
      "\n",
      "        [[6.2272]],\n",
      "\n",
      "        [[6.4323]],\n",
      "\n",
      "        [[5.8317]],\n",
      "\n",
      "        [[6.3107]],\n",
      "\n",
      "        [[6.0996]],\n",
      "\n",
      "        [[6.3170]],\n",
      "\n",
      "        [[5.6193]],\n",
      "\n",
      "        [[5.4264]],\n",
      "\n",
      "        [[6.2114]],\n",
      "\n",
      "        [[6.3755]],\n",
      "\n",
      "        [[6.8644]],\n",
      "\n",
      "        [[6.1906]],\n",
      "\n",
      "        [[5.7584]],\n",
      "\n",
      "        [[6.4097]],\n",
      "\n",
      "        [[6.0544]],\n",
      "\n",
      "        [[6.5370]],\n",
      "\n",
      "        [[6.1170]],\n",
      "\n",
      "        [[6.0004]],\n",
      "\n",
      "        [[6.3225]],\n",
      "\n",
      "        [[6.1462]],\n",
      "\n",
      "        [[4.9495]],\n",
      "\n",
      "        [[6.8915]],\n",
      "\n",
      "        [[6.1852]],\n",
      "\n",
      "        [[5.6332]],\n",
      "\n",
      "        [[6.3206]],\n",
      "\n",
      "        [[6.5497]],\n",
      "\n",
      "        [[6.1367]],\n",
      "\n",
      "        [[5.8374]],\n",
      "\n",
      "        [[5.8301]],\n",
      "\n",
      "        [[6.3824]],\n",
      "\n",
      "        [[6.2407]],\n",
      "\n",
      "        [[6.1601]],\n",
      "\n",
      "        [[6.2764]],\n",
      "\n",
      "        [[5.8985]],\n",
      "\n",
      "        [[6.3026]],\n",
      "\n",
      "        [[5.9849]],\n",
      "\n",
      "        [[6.2665]],\n",
      "\n",
      "        [[6.0802]],\n",
      "\n",
      "        [[6.2358]],\n",
      "\n",
      "        [[6.5274]],\n",
      "\n",
      "        [[6.3060]],\n",
      "\n",
      "        [[6.1166]],\n",
      "\n",
      "        [[5.7978]],\n",
      "\n",
      "        [[6.5723]],\n",
      "\n",
      "        [[6.8886]],\n",
      "\n",
      "        [[6.6264]],\n",
      "\n",
      "        [[5.9438]],\n",
      "\n",
      "        [[5.9276]],\n",
      "\n",
      "        [[7.2416]],\n",
      "\n",
      "        [[6.2576]],\n",
      "\n",
      "        [[6.7506]],\n",
      "\n",
      "        [[6.3757]],\n",
      "\n",
      "        [[5.8474]],\n",
      "\n",
      "        [[6.5441]],\n",
      "\n",
      "        [[6.6140]],\n",
      "\n",
      "        [[6.2852]],\n",
      "\n",
      "        [[6.3072]],\n",
      "\n",
      "        [[6.2852]],\n",
      "\n",
      "        [[6.3547]],\n",
      "\n",
      "        [[6.7991]],\n",
      "\n",
      "        [[5.9463]],\n",
      "\n",
      "        [[5.7544]],\n",
      "\n",
      "        [[5.6376]],\n",
      "\n",
      "        [[6.4084]],\n",
      "\n",
      "        [[6.1629]],\n",
      "\n",
      "        [[5.9012]],\n",
      "\n",
      "        [[6.2216]],\n",
      "\n",
      "        [[6.3726]],\n",
      "\n",
      "        [[6.2927]],\n",
      "\n",
      "        [[5.2366]],\n",
      "\n",
      "        [[6.2669]],\n",
      "\n",
      "        [[5.9646]],\n",
      "\n",
      "        [[5.4867]],\n",
      "\n",
      "        [[5.5499]],\n",
      "\n",
      "        [[6.0065]],\n",
      "\n",
      "        [[5.5039]],\n",
      "\n",
      "        [[5.3911]],\n",
      "\n",
      "        [[5.5377]],\n",
      "\n",
      "        [[6.1559]],\n",
      "\n",
      "        [[6.3279]],\n",
      "\n",
      "        [[6.0923]],\n",
      "\n",
      "        [[6.9087]],\n",
      "\n",
      "        [[6.4368]],\n",
      "\n",
      "        [[6.3488]],\n",
      "\n",
      "        [[5.8237]],\n",
      "\n",
      "        [[6.5509]],\n",
      "\n",
      "        [[6.4843]],\n",
      "\n",
      "        [[6.3209]],\n",
      "\n",
      "        [[5.9138]],\n",
      "\n",
      "        [[6.0908]],\n",
      "\n",
      "        [[6.5772]],\n",
      "\n",
      "        [[5.9227]],\n",
      "\n",
      "        [[5.7333]],\n",
      "\n",
      "        [[6.4309]],\n",
      "\n",
      "        [[6.6158]],\n",
      "\n",
      "        [[6.5719]],\n",
      "\n",
      "        [[6.3296]],\n",
      "\n",
      "        [[5.6116]],\n",
      "\n",
      "        [[6.0294]],\n",
      "\n",
      "        [[7.1140]],\n",
      "\n",
      "        [[5.9438]],\n",
      "\n",
      "        [[5.9043]],\n",
      "\n",
      "        [[5.9188]],\n",
      "\n",
      "        [[5.7729]],\n",
      "\n",
      "        [[6.3498]],\n",
      "\n",
      "        [[6.0892]],\n",
      "\n",
      "        [[5.4191]],\n",
      "\n",
      "        [[6.5533]],\n",
      "\n",
      "        [[5.8099]],\n",
      "\n",
      "        [[5.4464]],\n",
      "\n",
      "        [[6.2659]],\n",
      "\n",
      "        [[6.8730]],\n",
      "\n",
      "        [[6.5520]],\n",
      "\n",
      "        [[6.0593]],\n",
      "\n",
      "        [[6.0949]],\n",
      "\n",
      "        [[6.2403]],\n",
      "\n",
      "        [[6.9108]],\n",
      "\n",
      "        [[5.6992]],\n",
      "\n",
      "        [[6.2176]],\n",
      "\n",
      "        [[6.2254]],\n",
      "\n",
      "        [[5.8370]],\n",
      "\n",
      "        [[5.9527]],\n",
      "\n",
      "        [[6.4584]],\n",
      "\n",
      "        [[6.4341]],\n",
      "\n",
      "        [[5.9281]],\n",
      "\n",
      "        [[5.9623]],\n",
      "\n",
      "        [[5.9474]],\n",
      "\n",
      "        [[6.3736]],\n",
      "\n",
      "        [[5.6953]],\n",
      "\n",
      "        [[6.2793]],\n",
      "\n",
      "        [[5.5100]],\n",
      "\n",
      "        [[5.7366]],\n",
      "\n",
      "        [[6.0098]],\n",
      "\n",
      "        [[5.7801]],\n",
      "\n",
      "        [[6.8872]],\n",
      "\n",
      "        [[5.7635]],\n",
      "\n",
      "        [[6.2150]],\n",
      "\n",
      "        [[6.7871]],\n",
      "\n",
      "        [[6.5115]],\n",
      "\n",
      "        [[6.2465]],\n",
      "\n",
      "        [[6.0400]],\n",
      "\n",
      "        [[5.7329]],\n",
      "\n",
      "        [[6.4464]],\n",
      "\n",
      "        [[6.2511]],\n",
      "\n",
      "        [[6.0962]],\n",
      "\n",
      "        [[5.7283]],\n",
      "\n",
      "        [[6.9913]],\n",
      "\n",
      "        [[5.2238]],\n",
      "\n",
      "        [[5.5496]],\n",
      "\n",
      "        [[5.4177]],\n",
      "\n",
      "        [[6.3277]],\n",
      "\n",
      "        [[6.0147]],\n",
      "\n",
      "        [[6.1773]],\n",
      "\n",
      "        [[6.2243]],\n",
      "\n",
      "        [[5.9162]],\n",
      "\n",
      "        [[5.9617]],\n",
      "\n",
      "        [[5.8185]],\n",
      "\n",
      "        [[7.1418]],\n",
      "\n",
      "        [[7.0497]],\n",
      "\n",
      "        [[6.3827]],\n",
      "\n",
      "        [[6.6382]],\n",
      "\n",
      "        [[6.3851]],\n",
      "\n",
      "        [[6.6022]],\n",
      "\n",
      "        [[5.9486]],\n",
      "\n",
      "        [[5.6067]],\n",
      "\n",
      "        [[5.7278]],\n",
      "\n",
      "        [[6.6132]],\n",
      "\n",
      "        [[6.4857]],\n",
      "\n",
      "        [[6.3258]],\n",
      "\n",
      "        [[6.0633]],\n",
      "\n",
      "        [[6.0913]],\n",
      "\n",
      "        [[5.5786]],\n",
      "\n",
      "        [[6.4168]],\n",
      "\n",
      "        [[5.4746]],\n",
      "\n",
      "        [[6.3208]],\n",
      "\n",
      "        [[6.7959]],\n",
      "\n",
      "        [[6.3729]],\n",
      "\n",
      "        [[5.6382]],\n",
      "\n",
      "        [[6.0562]],\n",
      "\n",
      "        [[6.2976]],\n",
      "\n",
      "        [[5.7794]],\n",
      "\n",
      "        [[6.3839]],\n",
      "\n",
      "        [[6.2648]],\n",
      "\n",
      "        [[6.9507]],\n",
      "\n",
      "        [[6.9073]],\n",
      "\n",
      "        [[5.9085]],\n",
      "\n",
      "        [[6.0956]],\n",
      "\n",
      "        [[6.0277]],\n",
      "\n",
      "        [[6.8091]],\n",
      "\n",
      "        [[6.3202]],\n",
      "\n",
      "        [[5.9505]],\n",
      "\n",
      "        [[6.3344]],\n",
      "\n",
      "        [[5.9544]],\n",
      "\n",
      "        [[5.6038]],\n",
      "\n",
      "        [[6.4759]],\n",
      "\n",
      "        [[5.8274]],\n",
      "\n",
      "        [[6.0577]],\n",
      "\n",
      "        [[6.2943]],\n",
      "\n",
      "        [[6.9585]],\n",
      "\n",
      "        [[5.8083]],\n",
      "\n",
      "        [[6.1212]],\n",
      "\n",
      "        [[5.9330]],\n",
      "\n",
      "        [[6.2136]],\n",
      "\n",
      "        [[5.4791]],\n",
      "\n",
      "        [[6.4762]],\n",
      "\n",
      "        [[5.4152]],\n",
      "\n",
      "        [[6.0515]],\n",
      "\n",
      "        [[5.9721]],\n",
      "\n",
      "        [[6.0866]],\n",
      "\n",
      "        [[5.7555]],\n",
      "\n",
      "        [[5.5293]],\n",
      "\n",
      "        [[5.7475]],\n",
      "\n",
      "        [[6.1167]],\n",
      "\n",
      "        [[5.7575]],\n",
      "\n",
      "        [[5.9001]],\n",
      "\n",
      "        [[5.5745]],\n",
      "\n",
      "        [[5.8814]],\n",
      "\n",
      "        [[6.6689]],\n",
      "\n",
      "        [[6.1330]],\n",
      "\n",
      "        [[6.2309]],\n",
      "\n",
      "        [[6.0950]],\n",
      "\n",
      "        [[6.1336]],\n",
      "\n",
      "        [[5.5151]],\n",
      "\n",
      "        [[6.1762]],\n",
      "\n",
      "        [[6.7611]],\n",
      "\n",
      "        [[6.1660]],\n",
      "\n",
      "        [[5.6869]],\n",
      "\n",
      "        [[6.6388]],\n",
      "\n",
      "        [[6.6710]],\n",
      "\n",
      "        [[6.4358]],\n",
      "\n",
      "        [[6.6013]],\n",
      "\n",
      "        [[6.4644]],\n",
      "\n",
      "        [[5.9159]],\n",
      "\n",
      "        [[6.1827]],\n",
      "\n",
      "        [[6.2198]],\n",
      "\n",
      "        [[5.7442]],\n",
      "\n",
      "        [[5.9571]],\n",
      "\n",
      "        [[6.0145]],\n",
      "\n",
      "        [[5.9104]],\n",
      "\n",
      "        [[6.2351]],\n",
      "\n",
      "        [[6.3660]],\n",
      "\n",
      "        [[5.6544]],\n",
      "\n",
      "        [[5.6730]],\n",
      "\n",
      "        [[6.0668]],\n",
      "\n",
      "        [[6.1012]],\n",
      "\n",
      "        [[6.4944]],\n",
      "\n",
      "        [[5.9911]],\n",
      "\n",
      "        [[6.4311]],\n",
      "\n",
      "        [[6.0605]],\n",
      "\n",
      "        [[5.6960]],\n",
      "\n",
      "        [[6.5520]],\n",
      "\n",
      "        [[6.2734]],\n",
      "\n",
      "        [[6.8359]],\n",
      "\n",
      "        [[6.4369]],\n",
      "\n",
      "        [[6.4912]],\n",
      "\n",
      "        [[6.2073]],\n",
      "\n",
      "        [[6.1553]],\n",
      "\n",
      "        [[6.3807]],\n",
      "\n",
      "        [[6.4020]],\n",
      "\n",
      "        [[5.6480]],\n",
      "\n",
      "        [[5.9769]],\n",
      "\n",
      "        [[5.9952]],\n",
      "\n",
      "        [[6.0450]],\n",
      "\n",
      "        [[6.7025]],\n",
      "\n",
      "        [[6.1976]],\n",
      "\n",
      "        [[6.6713]],\n",
      "\n",
      "        [[6.1688]],\n",
      "\n",
      "        [[5.4587]],\n",
      "\n",
      "        [[5.9877]],\n",
      "\n",
      "        [[6.2782]],\n",
      "\n",
      "        [[5.7380]],\n",
      "\n",
      "        [[6.0175]],\n",
      "\n",
      "        [[6.9129]],\n",
      "\n",
      "        [[6.3141]],\n",
      "\n",
      "        [[6.6898]],\n",
      "\n",
      "        [[6.4886]],\n",
      "\n",
      "        [[6.1597]],\n",
      "\n",
      "        [[5.9872]],\n",
      "\n",
      "        [[5.7832]],\n",
      "\n",
      "        [[6.5172]],\n",
      "\n",
      "        [[6.8469]],\n",
      "\n",
      "        [[6.4370]],\n",
      "\n",
      "        [[5.5857]],\n",
      "\n",
      "        [[6.2192]],\n",
      "\n",
      "        [[6.1709]],\n",
      "\n",
      "        [[6.1221]],\n",
      "\n",
      "        [[5.8828]],\n",
      "\n",
      "        [[6.4155]],\n",
      "\n",
      "        [[5.9954]],\n",
      "\n",
      "        [[6.2085]],\n",
      "\n",
      "        [[5.8728]],\n",
      "\n",
      "        [[6.1303]],\n",
      "\n",
      "        [[6.2773]],\n",
      "\n",
      "        [[6.6746]],\n",
      "\n",
      "        [[6.1427]],\n",
      "\n",
      "        [[5.9120]],\n",
      "\n",
      "        [[5.6930]],\n",
      "\n",
      "        [[7.1822]],\n",
      "\n",
      "        [[6.4741]],\n",
      "\n",
      "        [[6.4716]],\n",
      "\n",
      "        [[5.8239]],\n",
      "\n",
      "        [[6.3039]],\n",
      "\n",
      "        [[5.4191]],\n",
      "\n",
      "        [[6.4823]],\n",
      "\n",
      "        [[6.0705]],\n",
      "\n",
      "        [[6.4715]],\n",
      "\n",
      "        [[6.0999]],\n",
      "\n",
      "        [[6.4324]],\n",
      "\n",
      "        [[5.8204]],\n",
      "\n",
      "        [[6.8286]],\n",
      "\n",
      "        [[6.6693]],\n",
      "\n",
      "        [[6.3364]],\n",
      "\n",
      "        [[6.6184]],\n",
      "\n",
      "        [[6.1196]],\n",
      "\n",
      "        [[6.5945]],\n",
      "\n",
      "        [[6.2973]],\n",
      "\n",
      "        [[6.3959]],\n",
      "\n",
      "        [[6.4903]],\n",
      "\n",
      "        [[5.7048]],\n",
      "\n",
      "        [[5.9383]],\n",
      "\n",
      "        [[5.5166]],\n",
      "\n",
      "        [[5.6776]],\n",
      "\n",
      "        [[6.1937]],\n",
      "\n",
      "        [[6.5404]],\n",
      "\n",
      "        [[6.8026]],\n",
      "\n",
      "        [[6.4723]],\n",
      "\n",
      "        [[6.1346]],\n",
      "\n",
      "        [[5.8160]],\n",
      "\n",
      "        [[5.8276]],\n",
      "\n",
      "        [[6.2223]],\n",
      "\n",
      "        [[6.0757]],\n",
      "\n",
      "        [[5.5484]],\n",
      "\n",
      "        [[6.2206]],\n",
      "\n",
      "        [[6.2944]],\n",
      "\n",
      "        [[5.8138]],\n",
      "\n",
      "        [[6.5984]],\n",
      "\n",
      "        [[6.5752]],\n",
      "\n",
      "        [[6.6563]],\n",
      "\n",
      "        [[6.3266]],\n",
      "\n",
      "        [[6.3338]],\n",
      "\n",
      "        [[6.0187]],\n",
      "\n",
      "        [[5.9112]],\n",
      "\n",
      "        [[6.3115]],\n",
      "\n",
      "        [[5.7796]],\n",
      "\n",
      "        [[6.1417]],\n",
      "\n",
      "        [[6.6414]],\n",
      "\n",
      "        [[6.4603]],\n",
      "\n",
      "        [[6.7215]],\n",
      "\n",
      "        [[6.5952]],\n",
      "\n",
      "        [[6.4000]],\n",
      "\n",
      "        [[6.0375]],\n",
      "\n",
      "        [[6.5147]],\n",
      "\n",
      "        [[6.0833]],\n",
      "\n",
      "        [[5.5939]],\n",
      "\n",
      "        [[5.3360]],\n",
      "\n",
      "        [[7.1250]],\n",
      "\n",
      "        [[6.7377]],\n",
      "\n",
      "        [[6.0354]],\n",
      "\n",
      "        [[6.8144]],\n",
      "\n",
      "        [[5.9318]],\n",
      "\n",
      "        [[6.6433]],\n",
      "\n",
      "        [[5.5352]],\n",
      "\n",
      "        [[5.7207]],\n",
      "\n",
      "        [[5.6319]],\n",
      "\n",
      "        [[5.7693]],\n",
      "\n",
      "        [[5.8877]],\n",
      "\n",
      "        [[6.8190]],\n",
      "\n",
      "        [[5.6777]],\n",
      "\n",
      "        [[6.4939]],\n",
      "\n",
      "        [[6.0293]],\n",
      "\n",
      "        [[7.0409]],\n",
      "\n",
      "        [[5.5087]],\n",
      "\n",
      "        [[5.7840]],\n",
      "\n",
      "        [[7.0237]],\n",
      "\n",
      "        [[6.5913]],\n",
      "\n",
      "        [[6.0049]],\n",
      "\n",
      "        [[6.2381]],\n",
      "\n",
      "        [[6.5430]],\n",
      "\n",
      "        [[6.3406]],\n",
      "\n",
      "        [[6.5601]],\n",
      "\n",
      "        [[6.4081]],\n",
      "\n",
      "        [[6.2631]],\n",
      "\n",
      "        [[6.5869]],\n",
      "\n",
      "        [[6.3787]],\n",
      "\n",
      "        [[6.5874]],\n",
      "\n",
      "        [[5.5141]],\n",
      "\n",
      "        [[5.9554]],\n",
      "\n",
      "        [[6.1138]],\n",
      "\n",
      "        [[5.5075]],\n",
      "\n",
      "        [[5.8250]],\n",
      "\n",
      "        [[6.1356]],\n",
      "\n",
      "        [[6.4831]],\n",
      "\n",
      "        [[6.1552]],\n",
      "\n",
      "        [[6.2469]],\n",
      "\n",
      "        [[6.4412]],\n",
      "\n",
      "        [[6.0471]],\n",
      "\n",
      "        [[5.9762]],\n",
      "\n",
      "        [[5.5413]],\n",
      "\n",
      "        [[5.8312]],\n",
      "\n",
      "        [[5.1619]],\n",
      "\n",
      "        [[6.1837]],\n",
      "\n",
      "        [[5.3375]],\n",
      "\n",
      "        [[6.1742]],\n",
      "\n",
      "        [[6.1756]],\n",
      "\n",
      "        [[6.0174]],\n",
      "\n",
      "        [[5.8005]],\n",
      "\n",
      "        [[5.2699]],\n",
      "\n",
      "        [[6.2755]],\n",
      "\n",
      "        [[6.6137]],\n",
      "\n",
      "        [[5.8244]],\n",
      "\n",
      "        [[6.2046]],\n",
      "\n",
      "        [[6.3695]],\n",
      "\n",
      "        [[6.1041]],\n",
      "\n",
      "        [[6.2331]],\n",
      "\n",
      "        [[6.6024]],\n",
      "\n",
      "        [[6.0890]],\n",
      "\n",
      "        [[6.3588]],\n",
      "\n",
      "        [[5.5306]],\n",
      "\n",
      "        [[6.3285]],\n",
      "\n",
      "        [[6.1261]],\n",
      "\n",
      "        [[6.4931]],\n",
      "\n",
      "        [[6.7399]],\n",
      "\n",
      "        [[6.3467]],\n",
      "\n",
      "        [[5.8980]],\n",
      "\n",
      "        [[6.5705]],\n",
      "\n",
      "        [[5.3531]],\n",
      "\n",
      "        [[6.5573]],\n",
      "\n",
      "        [[6.0178]],\n",
      "\n",
      "        [[5.7552]],\n",
      "\n",
      "        [[6.3033]],\n",
      "\n",
      "        [[6.2059]],\n",
      "\n",
      "        [[5.8416]],\n",
      "\n",
      "        [[5.5671]],\n",
      "\n",
      "        [[6.4189]],\n",
      "\n",
      "        [[6.1130]],\n",
      "\n",
      "        [[6.8225]],\n",
      "\n",
      "        [[6.6562]],\n",
      "\n",
      "        [[6.1296]],\n",
      "\n",
      "        [[6.0761]],\n",
      "\n",
      "        [[6.0563]],\n",
      "\n",
      "        [[6.6857]],\n",
      "\n",
      "        [[6.0847]],\n",
      "\n",
      "        [[5.8821]],\n",
      "\n",
      "        [[6.2947]],\n",
      "\n",
      "        [[5.8562]],\n",
      "\n",
      "        [[5.6273]],\n",
      "\n",
      "        [[5.9568]],\n",
      "\n",
      "        [[5.8365]],\n",
      "\n",
      "        [[6.1860]],\n",
      "\n",
      "        [[6.0636]],\n",
      "\n",
      "        [[6.2488]],\n",
      "\n",
      "        [[6.3800]],\n",
      "\n",
      "        [[5.7490]],\n",
      "\n",
      "        [[6.0624]],\n",
      "\n",
      "        [[6.2311]],\n",
      "\n",
      "        [[6.4789]],\n",
      "\n",
      "        [[6.2006]],\n",
      "\n",
      "        [[5.8609]],\n",
      "\n",
      "        [[5.5247]],\n",
      "\n",
      "        [[6.3641]],\n",
      "\n",
      "        [[6.2672]],\n",
      "\n",
      "        [[6.2918]],\n",
      "\n",
      "        [[6.1618]],\n",
      "\n",
      "        [[6.9133]],\n",
      "\n",
      "        [[5.8922]],\n",
      "\n",
      "        [[6.0141]],\n",
      "\n",
      "        [[5.9364]],\n",
      "\n",
      "        [[5.9406]],\n",
      "\n",
      "        [[5.2460]],\n",
      "\n",
      "        [[5.7165]],\n",
      "\n",
      "        [[5.6030]],\n",
      "\n",
      "        [[5.4718]],\n",
      "\n",
      "        [[6.2211]],\n",
      "\n",
      "        [[6.5715]],\n",
      "\n",
      "        [[6.0976]],\n",
      "\n",
      "        [[6.3066]],\n",
      "\n",
      "        [[5.8617]],\n",
      "\n",
      "        [[5.6394]],\n",
      "\n",
      "        [[5.5366]],\n",
      "\n",
      "        [[5.6695]],\n",
      "\n",
      "        [[5.3450]],\n",
      "\n",
      "        [[5.6429]],\n",
      "\n",
      "        [[5.8113]],\n",
      "\n",
      "        [[5.4811]],\n",
      "\n",
      "        [[5.7554]],\n",
      "\n",
      "        [[5.4353]],\n",
      "\n",
      "        [[5.0372]],\n",
      "\n",
      "        [[5.9206]],\n",
      "\n",
      "        [[5.6842]],\n",
      "\n",
      "        [[5.3008]],\n",
      "\n",
      "        [[5.4706]],\n",
      "\n",
      "        [[5.1196]],\n",
      "\n",
      "        [[5.2353]],\n",
      "\n",
      "        [[5.2587]],\n",
      "\n",
      "        [[5.5599]],\n",
      "\n",
      "        [[5.3190]],\n",
      "\n",
      "        [[5.3122]],\n",
      "\n",
      "        [[5.1099]],\n",
      "\n",
      "        [[5.1046]],\n",
      "\n",
      "        [[5.4857]],\n",
      "\n",
      "        [[5.6339]],\n",
      "\n",
      "        [[4.7963]],\n",
      "\n",
      "        [[5.3045]],\n",
      "\n",
      "        [[5.1624]],\n",
      "\n",
      "        [[5.5257]],\n",
      "\n",
      "        [[5.3650]],\n",
      "\n",
      "        [[5.5855]],\n",
      "\n",
      "        [[5.5832]],\n",
      "\n",
      "        [[5.5238]],\n",
      "\n",
      "        [[6.0253]],\n",
      "\n",
      "        [[5.6486]],\n",
      "\n",
      "        [[5.4446]],\n",
      "\n",
      "        [[5.0196]],\n",
      "\n",
      "        [[5.7150]],\n",
      "\n",
      "        [[4.7051]],\n",
      "\n",
      "        [[5.5202]],\n",
      "\n",
      "        [[5.7382]],\n",
      "\n",
      "        [[5.3168]],\n",
      "\n",
      "        [[5.5514]],\n",
      "\n",
      "        [[5.1563]],\n",
      "\n",
      "        [[5.3496]],\n",
      "\n",
      "        [[5.4337]],\n",
      "\n",
      "        [[5.8429]],\n",
      "\n",
      "        [[5.3228]],\n",
      "\n",
      "        [[5.1029]],\n",
      "\n",
      "        [[5.9803]],\n",
      "\n",
      "        [[5.5433]],\n",
      "\n",
      "        [[5.3539]],\n",
      "\n",
      "        [[5.6267]],\n",
      "\n",
      "        [[5.2312]],\n",
      "\n",
      "        [[5.7078]],\n",
      "\n",
      "        [[5.6788]],\n",
      "\n",
      "        [[5.0944]],\n",
      "\n",
      "        [[4.9092]],\n",
      "\n",
      "        [[5.0991]],\n",
      "\n",
      "        [[5.6530]],\n",
      "\n",
      "        [[5.6643]],\n",
      "\n",
      "        [[5.3398]],\n",
      "\n",
      "        [[5.4205]],\n",
      "\n",
      "        [[5.8415]],\n",
      "\n",
      "        [[5.5696]],\n",
      "\n",
      "        [[5.5055]],\n",
      "\n",
      "        [[5.3667]],\n",
      "\n",
      "        [[5.4032]],\n",
      "\n",
      "        [[5.4198]],\n",
      "\n",
      "        [[5.6857]],\n",
      "\n",
      "        [[5.4096]],\n",
      "\n",
      "        [[5.4652]],\n",
      "\n",
      "        [[5.8012]],\n",
      "\n",
      "        [[5.4382]],\n",
      "\n",
      "        [[5.3639]],\n",
      "\n",
      "        [[6.1777]],\n",
      "\n",
      "        [[5.6491]],\n",
      "\n",
      "        [[5.2001]],\n",
      "\n",
      "        [[5.6410]],\n",
      "\n",
      "        [[5.2252]],\n",
      "\n",
      "        [[5.2114]],\n",
      "\n",
      "        [[5.5096]],\n",
      "\n",
      "        [[5.6156]],\n",
      "\n",
      "        [[5.7293]],\n",
      "\n",
      "        [[5.1879]],\n",
      "\n",
      "        [[5.4667]],\n",
      "\n",
      "        [[5.3627]],\n",
      "\n",
      "        [[5.6726]],\n",
      "\n",
      "        [[5.6585]],\n",
      "\n",
      "        [[5.4890]],\n",
      "\n",
      "        [[5.8377]],\n",
      "\n",
      "        [[5.3896]],\n",
      "\n",
      "        [[5.8132]],\n",
      "\n",
      "        [[5.0567]],\n",
      "\n",
      "        [[5.2491]],\n",
      "\n",
      "        [[5.6769]],\n",
      "\n",
      "        [[5.2488]],\n",
      "\n",
      "        [[4.9885]],\n",
      "\n",
      "        [[5.6306]],\n",
      "\n",
      "        [[5.6424]],\n",
      "\n",
      "        [[5.4847]],\n",
      "\n",
      "        [[5.6540]],\n",
      "\n",
      "        [[5.9459]],\n",
      "\n",
      "        [[5.9002]],\n",
      "\n",
      "        [[5.3345]],\n",
      "\n",
      "        [[4.6969]],\n",
      "\n",
      "        [[5.8381]],\n",
      "\n",
      "        [[5.5071]],\n",
      "\n",
      "        [[5.7738]],\n",
      "\n",
      "        [[5.5353]],\n",
      "\n",
      "        [[5.4659]],\n",
      "\n",
      "        [[5.5783]],\n",
      "\n",
      "        [[4.9977]],\n",
      "\n",
      "        [[5.4165]],\n",
      "\n",
      "        [[5.8172]],\n",
      "\n",
      "        [[5.6202]],\n",
      "\n",
      "        [[5.2113]],\n",
      "\n",
      "        [[5.0303]],\n",
      "\n",
      "        [[5.5580]],\n",
      "\n",
      "        [[4.8892]],\n",
      "\n",
      "        [[6.0293]],\n",
      "\n",
      "        [[5.1833]],\n",
      "\n",
      "        [[5.1339]],\n",
      "\n",
      "        [[5.2083]],\n",
      "\n",
      "        [[5.1108]],\n",
      "\n",
      "        [[5.1724]],\n",
      "\n",
      "        [[5.4978]],\n",
      "\n",
      "        [[5.5770]],\n",
      "\n",
      "        [[5.0122]],\n",
      "\n",
      "        [[5.1115]],\n",
      "\n",
      "        [[5.3237]],\n",
      "\n",
      "        [[5.6374]],\n",
      "\n",
      "        [[6.2659]],\n",
      "\n",
      "        [[6.2074]],\n",
      "\n",
      "        [[5.8285]],\n",
      "\n",
      "        [[4.9105]],\n",
      "\n",
      "        [[5.0305]],\n",
      "\n",
      "        [[5.0233]],\n",
      "\n",
      "        [[4.8970]],\n",
      "\n",
      "        [[5.3334]],\n",
      "\n",
      "        [[5.7210]],\n",
      "\n",
      "        [[5.5370]],\n",
      "\n",
      "        [[5.4284]],\n",
      "\n",
      "        [[5.7077]],\n",
      "\n",
      "        [[4.7195]],\n",
      "\n",
      "        [[5.5928]],\n",
      "\n",
      "        [[5.3955]],\n",
      "\n",
      "        [[5.5157]],\n",
      "\n",
      "        [[5.8049]],\n",
      "\n",
      "        [[5.3419]],\n",
      "\n",
      "        [[5.5272]],\n",
      "\n",
      "        [[5.7208]],\n",
      "\n",
      "        [[5.1510]],\n",
      "\n",
      "        [[5.1089]],\n",
      "\n",
      "        [[5.0850]],\n",
      "\n",
      "        [[5.3410]],\n",
      "\n",
      "        [[5.6492]],\n",
      "\n",
      "        [[6.3724]],\n",
      "\n",
      "        [[5.1115]],\n",
      "\n",
      "        [[5.2892]],\n",
      "\n",
      "        [[5.5917]],\n",
      "\n",
      "        [[5.4409]],\n",
      "\n",
      "        [[4.7247]],\n",
      "\n",
      "        [[5.8330]],\n",
      "\n",
      "        [[6.1923]],\n",
      "\n",
      "        [[4.7377]],\n",
      "\n",
      "        [[5.2053]],\n",
      "\n",
      "        [[5.8238]],\n",
      "\n",
      "        [[5.1153]],\n",
      "\n",
      "        [[4.9785]],\n",
      "\n",
      "        [[5.6907]],\n",
      "\n",
      "        [[5.2906]],\n",
      "\n",
      "        [[5.2591]],\n",
      "\n",
      "        [[5.4022]],\n",
      "\n",
      "        [[5.3827]],\n",
      "\n",
      "        [[5.5141]],\n",
      "\n",
      "        [[5.3219]],\n",
      "\n",
      "        [[5.6306]],\n",
      "\n",
      "        [[5.3998]],\n",
      "\n",
      "        [[6.0014]],\n",
      "\n",
      "        [[5.2717]],\n",
      "\n",
      "        [[5.5375]],\n",
      "\n",
      "        [[5.0748]],\n",
      "\n",
      "        [[5.5519]],\n",
      "\n",
      "        [[5.4887]],\n",
      "\n",
      "        [[5.7050]],\n",
      "\n",
      "        [[5.4481]],\n",
      "\n",
      "        [[5.5527]],\n",
      "\n",
      "        [[5.7587]],\n",
      "\n",
      "        [[5.7339]],\n",
      "\n",
      "        [[5.6528]],\n",
      "\n",
      "        [[5.5749]],\n",
      "\n",
      "        [[5.5614]],\n",
      "\n",
      "        [[5.6009]],\n",
      "\n",
      "        [[5.0735]],\n",
      "\n",
      "        [[5.5256]],\n",
      "\n",
      "        [[5.0831]],\n",
      "\n",
      "        [[5.8010]],\n",
      "\n",
      "        [[5.4961]],\n",
      "\n",
      "        [[5.4457]],\n",
      "\n",
      "        [[5.5621]],\n",
      "\n",
      "        [[4.9877]],\n",
      "\n",
      "        [[5.2833]],\n",
      "\n",
      "        [[4.6289]],\n",
      "\n",
      "        [[5.5448]],\n",
      "\n",
      "        [[5.1629]],\n",
      "\n",
      "        [[5.2955]],\n",
      "\n",
      "        [[5.7051]],\n",
      "\n",
      "        [[5.4226]],\n",
      "\n",
      "        [[5.8724]],\n",
      "\n",
      "        [[5.7109]],\n",
      "\n",
      "        [[5.6512]],\n",
      "\n",
      "        [[5.3592]],\n",
      "\n",
      "        [[5.6393]],\n",
      "\n",
      "        [[5.5558]],\n",
      "\n",
      "        [[5.7157]],\n",
      "\n",
      "        [[5.4380]],\n",
      "\n",
      "        [[5.4402]],\n",
      "\n",
      "        [[5.7061]],\n",
      "\n",
      "        [[5.2036]],\n",
      "\n",
      "        [[5.5507]],\n",
      "\n",
      "        [[5.6232]],\n",
      "\n",
      "        [[6.0347]],\n",
      "\n",
      "        [[5.4612]],\n",
      "\n",
      "        [[5.8593]],\n",
      "\n",
      "        [[5.5651]],\n",
      "\n",
      "        [[4.9684]],\n",
      "\n",
      "        [[5.8366]],\n",
      "\n",
      "        [[4.9514]],\n",
      "\n",
      "        [[5.2026]],\n",
      "\n",
      "        [[5.1124]],\n",
      "\n",
      "        [[5.1910]],\n",
      "\n",
      "        [[5.8329]],\n",
      "\n",
      "        [[5.7170]],\n",
      "\n",
      "        [[5.1732]],\n",
      "\n",
      "        [[5.9732]],\n",
      "\n",
      "        [[5.7328]],\n",
      "\n",
      "        [[5.2677]],\n",
      "\n",
      "        [[5.7410]],\n",
      "\n",
      "        [[5.3782]],\n",
      "\n",
      "        [[5.4265]],\n",
      "\n",
      "        [[5.4062]],\n",
      "\n",
      "        [[5.3154]],\n",
      "\n",
      "        [[5.0986]],\n",
      "\n",
      "        [[6.1404]],\n",
      "\n",
      "        [[4.8087]],\n",
      "\n",
      "        [[5.5956]],\n",
      "\n",
      "        [[5.2406]],\n",
      "\n",
      "        [[5.6068]],\n",
      "\n",
      "        [[5.6609]],\n",
      "\n",
      "        [[5.5253]],\n",
      "\n",
      "        [[6.0413]],\n",
      "\n",
      "        [[4.9906]],\n",
      "\n",
      "        [[5.0516]],\n",
      "\n",
      "        [[5.6164]],\n",
      "\n",
      "        [[5.3845]],\n",
      "\n",
      "        [[5.8733]],\n",
      "\n",
      "        [[5.4174]],\n",
      "\n",
      "        [[5.4296]],\n",
      "\n",
      "        [[5.4312]],\n",
      "\n",
      "        [[5.8137]],\n",
      "\n",
      "        [[5.5025]],\n",
      "\n",
      "        [[5.5478]],\n",
      "\n",
      "        [[5.7739]],\n",
      "\n",
      "        [[5.4476]],\n",
      "\n",
      "        [[5.4349]],\n",
      "\n",
      "        [[5.6724]],\n",
      "\n",
      "        [[5.5192]],\n",
      "\n",
      "        [[5.3624]],\n",
      "\n",
      "        [[6.0286]],\n",
      "\n",
      "        [[5.4591]],\n",
      "\n",
      "        [[5.5544]],\n",
      "\n",
      "        [[6.1714]],\n",
      "\n",
      "        [[6.0619]],\n",
      "\n",
      "        [[5.3952]],\n",
      "\n",
      "        [[5.5102]],\n",
      "\n",
      "        [[5.8102]],\n",
      "\n",
      "        [[4.8068]],\n",
      "\n",
      "        [[5.0535]],\n",
      "\n",
      "        [[5.5339]],\n",
      "\n",
      "        [[5.6746]],\n",
      "\n",
      "        [[5.4650]],\n",
      "\n",
      "        [[5.7893]],\n",
      "\n",
      "        [[5.6388]],\n",
      "\n",
      "        [[5.8086]],\n",
      "\n",
      "        [[5.4011]],\n",
      "\n",
      "        [[4.5810]],\n",
      "\n",
      "        [[5.2588]],\n",
      "\n",
      "        [[5.7170]],\n",
      "\n",
      "        [[5.7982]],\n",
      "\n",
      "        [[5.0441]],\n",
      "\n",
      "        [[6.0467]],\n",
      "\n",
      "        [[5.2136]],\n",
      "\n",
      "        [[5.6791]],\n",
      "\n",
      "        [[5.2874]],\n",
      "\n",
      "        [[5.3883]],\n",
      "\n",
      "        [[5.1294]],\n",
      "\n",
      "        [[5.4129]],\n",
      "\n",
      "        [[5.1259]],\n",
      "\n",
      "        [[5.1963]],\n",
      "\n",
      "        [[4.9324]],\n",
      "\n",
      "        [[5.5335]],\n",
      "\n",
      "        [[5.8288]],\n",
      "\n",
      "        [[5.3853]],\n",
      "\n",
      "        [[4.7336]],\n",
      "\n",
      "        [[5.4109]],\n",
      "\n",
      "        [[5.3212]],\n",
      "\n",
      "        [[5.4064]],\n",
      "\n",
      "        [[6.3077]],\n",
      "\n",
      "        [[5.2648]],\n",
      "\n",
      "        [[5.1603]],\n",
      "\n",
      "        [[5.1766]],\n",
      "\n",
      "        [[5.4784]],\n",
      "\n",
      "        [[5.6933]],\n",
      "\n",
      "        [[5.0154]],\n",
      "\n",
      "        [[5.7258]],\n",
      "\n",
      "        [[5.3036]],\n",
      "\n",
      "        [[5.5613]],\n",
      "\n",
      "        [[5.1856]],\n",
      "\n",
      "        [[5.4695]],\n",
      "\n",
      "        [[5.4697]],\n",
      "\n",
      "        [[5.3626]],\n",
      "\n",
      "        [[5.6931]],\n",
      "\n",
      "        [[5.6655]],\n",
      "\n",
      "        [[5.3161]],\n",
      "\n",
      "        [[5.2766]],\n",
      "\n",
      "        [[5.2370]],\n",
      "\n",
      "        [[5.1341]],\n",
      "\n",
      "        [[5.5315]],\n",
      "\n",
      "        [[5.3909]],\n",
      "\n",
      "        [[5.0656]],\n",
      "\n",
      "        [[5.6484]],\n",
      "\n",
      "        [[6.0193]],\n",
      "\n",
      "        [[4.6666]],\n",
      "\n",
      "        [[5.8230]],\n",
      "\n",
      "        [[5.7737]],\n",
      "\n",
      "        [[5.5079]],\n",
      "\n",
      "        [[5.2509]],\n",
      "\n",
      "        [[5.2120]],\n",
      "\n",
      "        [[5.3163]],\n",
      "\n",
      "        [[5.5671]],\n",
      "\n",
      "        [[5.0204]],\n",
      "\n",
      "        [[5.7300]],\n",
      "\n",
      "        [[5.7409]],\n",
      "\n",
      "        [[6.0118]],\n",
      "\n",
      "        [[5.8111]],\n",
      "\n",
      "        [[5.2111]],\n",
      "\n",
      "        [[5.6309]],\n",
      "\n",
      "        [[4.8595]],\n",
      "\n",
      "        [[5.4044]],\n",
      "\n",
      "        [[6.2356]],\n",
      "\n",
      "        [[5.6802]],\n",
      "\n",
      "        [[5.2137]],\n",
      "\n",
      "        [[5.2957]],\n",
      "\n",
      "        [[4.9823]],\n",
      "\n",
      "        [[5.4952]],\n",
      "\n",
      "        [[4.7050]],\n",
      "\n",
      "        [[5.5205]],\n",
      "\n",
      "        [[4.7468]],\n",
      "\n",
      "        [[5.6534]],\n",
      "\n",
      "        [[5.5929]],\n",
      "\n",
      "        [[5.6379]],\n",
      "\n",
      "        [[5.5575]],\n",
      "\n",
      "        [[5.1981]],\n",
      "\n",
      "        [[5.0828]],\n",
      "\n",
      "        [[6.1688]],\n",
      "\n",
      "        [[4.3425]],\n",
      "\n",
      "        [[5.3153]],\n",
      "\n",
      "        [[5.2939]],\n",
      "\n",
      "        [[5.2922]],\n",
      "\n",
      "        [[5.2789]],\n",
      "\n",
      "        [[5.4584]],\n",
      "\n",
      "        [[5.7721]],\n",
      "\n",
      "        [[5.2075]],\n",
      "\n",
      "        [[5.5823]],\n",
      "\n",
      "        [[5.2031]],\n",
      "\n",
      "        [[4.9857]],\n",
      "\n",
      "        [[5.5883]],\n",
      "\n",
      "        [[5.8914]],\n",
      "\n",
      "        [[5.3667]],\n",
      "\n",
      "        [[5.4908]],\n",
      "\n",
      "        [[6.1354]],\n",
      "\n",
      "        [[5.5505]],\n",
      "\n",
      "        [[5.7100]],\n",
      "\n",
      "        [[5.5199]],\n",
      "\n",
      "        [[5.5125]],\n",
      "\n",
      "        [[5.5072]],\n",
      "\n",
      "        [[5.7794]],\n",
      "\n",
      "        [[5.2226]],\n",
      "\n",
      "        [[5.3025]],\n",
      "\n",
      "        [[5.4627]],\n",
      "\n",
      "        [[5.2707]],\n",
      "\n",
      "        [[5.2760]],\n",
      "\n",
      "        [[5.5085]],\n",
      "\n",
      "        [[5.0931]],\n",
      "\n",
      "        [[5.3422]],\n",
      "\n",
      "        [[5.6069]],\n",
      "\n",
      "        [[5.1735]],\n",
      "\n",
      "        [[5.3073]],\n",
      "\n",
      "        [[5.7478]],\n",
      "\n",
      "        [[5.5281]],\n",
      "\n",
      "        [[5.6593]],\n",
      "\n",
      "        [[5.7063]],\n",
      "\n",
      "        [[5.4491]],\n",
      "\n",
      "        [[5.6144]],\n",
      "\n",
      "        [[5.4568]],\n",
      "\n",
      "        [[6.1127]],\n",
      "\n",
      "        [[5.4725]],\n",
      "\n",
      "        [[5.4767]],\n",
      "\n",
      "        [[5.2483]],\n",
      "\n",
      "        [[5.3173]],\n",
      "\n",
      "        [[5.1614]],\n",
      "\n",
      "        [[5.6233]],\n",
      "\n",
      "        [[5.1976]],\n",
      "\n",
      "        [[5.6863]],\n",
      "\n",
      "        [[5.4812]],\n",
      "\n",
      "        [[5.4751]],\n",
      "\n",
      "        [[5.4988]],\n",
      "\n",
      "        [[5.7502]],\n",
      "\n",
      "        [[5.2165]],\n",
      "\n",
      "        [[5.7511]],\n",
      "\n",
      "        [[5.7465]],\n",
      "\n",
      "        [[5.0288]],\n",
      "\n",
      "        [[5.3429]],\n",
      "\n",
      "        [[5.4775]],\n",
      "\n",
      "        [[5.5282]],\n",
      "\n",
      "        [[5.4255]],\n",
      "\n",
      "        [[5.3701]],\n",
      "\n",
      "        [[5.9291]],\n",
      "\n",
      "        [[5.2454]],\n",
      "\n",
      "        [[5.5076]],\n",
      "\n",
      "        [[5.2802]],\n",
      "\n",
      "        [[5.8473]],\n",
      "\n",
      "        [[5.7828]],\n",
      "\n",
      "        [[5.4684]],\n",
      "\n",
      "        [[4.8018]],\n",
      "\n",
      "        [[5.4415]],\n",
      "\n",
      "        [[5.3968]],\n",
      "\n",
      "        [[5.4934]],\n",
      "\n",
      "        [[6.2602]],\n",
      "\n",
      "        [[5.5731]],\n",
      "\n",
      "        [[5.4386]],\n",
      "\n",
      "        [[5.4934]],\n",
      "\n",
      "        [[5.5126]],\n",
      "\n",
      "        [[5.6851]],\n",
      "\n",
      "        [[5.4833]],\n",
      "\n",
      "        [[5.7851]],\n",
      "\n",
      "        [[5.9502]],\n",
      "\n",
      "        [[5.3733]],\n",
      "\n",
      "        [[5.4661]],\n",
      "\n",
      "        [[5.5261]],\n",
      "\n",
      "        [[5.5275]],\n",
      "\n",
      "        [[5.3772]],\n",
      "\n",
      "        [[5.5196]],\n",
      "\n",
      "        [[5.4545]],\n",
      "\n",
      "        [[5.5870]],\n",
      "\n",
      "        [[5.4884]],\n",
      "\n",
      "        [[5.4507]],\n",
      "\n",
      "        [[5.4650]],\n",
      "\n",
      "        [[5.8732]],\n",
      "\n",
      "        [[5.4730]],\n",
      "\n",
      "        [[5.6162]],\n",
      "\n",
      "        [[5.9741]],\n",
      "\n",
      "        [[5.0889]],\n",
      "\n",
      "        [[5.5771]],\n",
      "\n",
      "        [[4.7598]],\n",
      "\n",
      "        [[5.0864]],\n",
      "\n",
      "        [[4.8433]],\n",
      "\n",
      "        [[5.6536]],\n",
      "\n",
      "        [[5.0303]],\n",
      "\n",
      "        [[5.5017]],\n",
      "\n",
      "        [[5.6943]],\n",
      "\n",
      "        [[5.9384]],\n",
      "\n",
      "        [[5.5077]],\n",
      "\n",
      "        [[5.9158]],\n",
      "\n",
      "        [[5.8072]],\n",
      "\n",
      "        [[6.0229]],\n",
      "\n",
      "        [[5.4394]],\n",
      "\n",
      "        [[5.6707]],\n",
      "\n",
      "        [[5.1167]],\n",
      "\n",
      "        [[4.9873]],\n",
      "\n",
      "        [[5.3053]],\n",
      "\n",
      "        [[5.9829]],\n",
      "\n",
      "        [[5.9335]],\n",
      "\n",
      "        [[5.2963]],\n",
      "\n",
      "        [[5.4957]],\n",
      "\n",
      "        [[5.8600]],\n",
      "\n",
      "        [[5.1320]],\n",
      "\n",
      "        [[5.7141]],\n",
      "\n",
      "        [[5.3607]],\n",
      "\n",
      "        [[5.9218]],\n",
      "\n",
      "        [[5.3203]],\n",
      "\n",
      "        [[5.3786]],\n",
      "\n",
      "        [[5.9457]],\n",
      "\n",
      "        [[5.0531]],\n",
      "\n",
      "        [[5.9111]],\n",
      "\n",
      "        [[5.3598]],\n",
      "\n",
      "        [[5.9089]],\n",
      "\n",
      "        [[4.9055]],\n",
      "\n",
      "        [[5.2553]],\n",
      "\n",
      "        [[5.0190]]], grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch, m1: [1000 x 1], m2: [256 x 1] at /opt/conda/conda-bld/pytorch_1544174967633/work/aten/src/TH/generic/THTensorMath.cpp:940",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-8fbfcd4492cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mattention_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlocal_Attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mattention_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/engine_3.6/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-58-deae57fd0f9f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"p size :\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mp_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mattention_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax_fc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# [batch_size, max_length, 1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mcontext_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_weights\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;31m#[batch_size, max_length, hidden_dim]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mnew_context_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext_vector\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#[batch_size, hidden_dim]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/engine_3.6/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/engine_3.6/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mweak_script_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/engine_3.6/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1352\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unwrap_optional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1353\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1354\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1355\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1356\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unwrap_optional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: size mismatch, m1: [1000 x 1], m2: [256 x 1] at /opt/conda/conda-bld/pytorch_1544174967633/work/aten/src/TH/generic/THTensorMath.cpp:940"
     ]
    }
   ],
   "source": [
    "outputs = enc(batch.src)[0]\n",
    "hidden = enc(batch.src)[1]\n",
    "attention_obj = local_Attention(outputs, hidden, 128,1000).to(device)\n",
    "attention_obj().size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, embedding_dim, hidden_dim, num_layers,\\\n",
    "                 dropout, attention_obj,concat=False):\n",
    "        \n",
    "        super().__init__()        \n",
    "        self.embedding = nn.Embedding(output_dim, embedding_dim,padding_idx=1)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_layers = num_layers        \n",
    "        self.rnn = nn.RNN(embedding_dim+hidden_dim,hidden_dim, num_layers=num_layers, \\\n",
    "                              dropout=dropout,batch_first=True)    \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fully_connect = nn.Linear(hidden_dim,output_dim)\n",
    "        self.attention_obj = attention_obj\n",
    "        \n",
    "    def forward(self, inputs, hidden, enc_outputs):\n",
    "        \n",
    "        atten = self.attention_obj(enc_outputs,hidden,self.hidden_dim)\n",
    "        context_vector = atten.forward()\n",
    "        # 함수 내에서 Attention class을 호출해, context_vector를 계산해줍니다.\n",
    "        # 이 때, attention_obj 에서 변하는 argument 는 hidden layer의 값입니다.\n",
    "        embedding_vector = self.dropout(self.embedding(inputs)).permute(1,0,2)\n",
    "        \n",
    "        if not self.concat : \n",
    "            dec_hidden = context_vector.unsqueeze(1).permute(1,0,2)\n",
    "            outputs,hidden = self.rnn(embedding_vector,dec_hidden)            \n",
    "        else : \n",
    "            concated_vector = torch.cat((context_vector.unsqueeze(1), embedding_vector), -1)\n",
    "            outputs,hidden = self.rnn(concated_vector)\n",
    "            \n",
    "        outputs,hidden = self.rnn(embedding_vector,dec_hidden)\n",
    "        final_outputs = outputs.squeeze(1)\n",
    "        fc_layer = self.fully_connect(final_outputs)\n",
    "        if len(fc_layer.size()) < 3 :\n",
    "            fc_layer = fc_layer.unsqueeze(1)\n",
    "            fc_layer = fc_layer.permute(1,0,2)\n",
    "        else :\n",
    "            fc_layer = fc_layer.permute(1,0,2)\n",
    "        \n",
    "        return fc_layer, hidden\n",
    "#     [max_length, batch_size, output_dim] , [1, batch_size, output_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([23, 1000, 5893]), torch.Size([1, 1000, 128]))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim = len(SRC.vocab.itos)\n",
    "output_dim = len(TRG.vocab.itos)\n",
    "embedding_dim = 256\n",
    "hidden_dim = 128\n",
    "num_layer = 1\n",
    "dropout = 0.5\n",
    "attention_obj = Attention\n",
    "batch_size = 1000\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "batch = next(iter(train_iter))\n",
    "dec = Decoder(output_dim,embedding_dim,hidden_dim,num_layer,dropout,attention_obj).to(device)\n",
    "\n",
    "dec(batch.trg,enc(batch.src)[1],enc(batch.src)[0])[0].size(),\\\n",
    "dec(batch.trg,enc(batch.src)[1],enc(batch.src)[0])[1].size()\n",
    "#[max_length, batch_size, output_dim] , [1, batch_size, output_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "                                \n",
    "        batch_size = trg.shape[1]\n",
    "        max_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        # decoder 의 맨처음에는 encoder 에서 나온 hidden,cell을 넣어주어야 합니다! 이때, num_layer와 hidden_dim은 같아야 합니다!\n",
    "        enc_output, enc_hidden = self.encoder(src)\n",
    "        \n",
    "        # decoder 를 돌면서, 각 단어에 대한 outputs값(벡터 형태)이 나오게 되는데, 이러한 값들을 아래의 outputs 변수에 저장해줍니다\n",
    "        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        # 맨 처음에는 문장의 시작을 알리는 sos(start of sentence) 토큰을 넣어주어야 합니다.\n",
    "        dec_inputs = trg[0,:].unsqueeze(0)\n",
    "        dec_hidden = enc_hidden\n",
    "        for t in range(1, max_len):\n",
    "\n",
    "            fc_layer, dec_hidden = self.decoder(dec_inputs,dec_hidden,enc_output)\n",
    "#            output'dimension : [batch_size , output_dim], 여기서 output_dim 은 출현 가능한 모든 target lang 의 수 입니다.\n",
    "            outputs[t] = fc_layer\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = fc_layer.max(1)[1] # 해당 글자의 numericalized index 를 넣어주어야 합니다.\n",
    "            input_ = (trg[t] if teacher_force else top1)\n",
    "            # teacher_forcing 을 할 경우, 실제 trg데이터를 다음 input으로 사용, 그렇지 않을 경우, 이전 state에서 가장 높은 \n",
    "            # 값을 가진[나올 수 있는 모든 target vocab 리스트 중에서를 의미합니다. 확률값의 형태는 아니지만, 가장 개연성이 높은 단어를 의미합니다.]\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameter settings\n",
    "input_dim = len(SRC.vocab.itos)\n",
    "output_dim = len(TRG.vocab.itos)\n",
    "embedding_dim = 256\n",
    "hidden_dim = 128\n",
    "num_layer = 1\n",
    "dropout = 0.5\n",
    "attention_obj = Attention\n",
    "batch_size = 1000\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "enc = Encoder(input_dim,embedding_dim,hidden_dim,num_layer,dropout)\n",
    "dec = Decoder(output_dim,embedding_dim,hidden_dim,num_layer,dropout,attention_obj)\n",
    "enc.to(device);dec.to(device)\n",
    "batch = next(iter(train_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([27, 1000, 5893])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq = Seq2Seq(enc,dec,device)\n",
    "seq(batch.src,batch.trg).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gradient clipping 에 대한 논의\n",
    "- https://hskimim.github.io/Avoid_Exploding_gradient_in_Neural_Net_with_gradient_clipping/ 에 보다 자세히 기록하였습니다.\n",
    "- RNN 베이스의 네트워크의 특징인 gradient exploding 을 방지해주는 방법론입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class fit() : \n",
    "    \n",
    "    def __init__(self, model, train_iter, test_iter, epoch = 5) : \n",
    "        self.optimizer = optim.Adam(model.parameters())\n",
    "        # <pad> 토큰은 임베딩 벡터와, loss_function에 argument 로 들어가서, training 과정에서 제외됩니다.\n",
    "        self.pad_idx = TRG.vocab.stoi['<pad>'] \n",
    "        self.criterion = nn.CrossEntropyLoss(ignore_index=self.pad_idx)\n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model = model.to(device)\n",
    "        self.train_iter = train_iter\n",
    "        self.test_iter = test_iter\n",
    "        self.epoch = epoch\n",
    "            \n",
    "    def train(self,clip):\n",
    "    \n",
    "        epoch_loss = 0 # loss per epoch\n",
    "        self.model.train()\n",
    "        \n",
    "        for i, batch in enumerate(self.train_iter):\n",
    "            print('batch : ',i,end='\\r')\n",
    "            src = batch.src\n",
    "            trg = batch.trg\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            output = model(src, trg)        \n",
    "\n",
    "            loss_output = output[1:].view(-1, output.shape[-1])\n",
    "            loss_trg = trg[1:].view(-1)\n",
    "            # sos 토큰을 제외하고, 차원을 맞춘 후에, output을 변수에 저장해줍니다.\n",
    "            \n",
    "            loss = self.criterion(loss_output, loss_trg)\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "            # gradient clipping\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        return epoch_loss / len(self.train_iter)\n",
    "    \n",
    "\n",
    "    def fit_by_iterate(self,clip) : \n",
    "        \n",
    "        for epoch in range(self.epoch):\n",
    "            print('epoch : ',epoch + 1)\n",
    "            train_loss= self.train(clip)\n",
    "            print(\"epoch's loss : {}\".format(train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitting_process = fit(seq,train_iter,test_iter,epoch=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch :  1\n",
      "epoch's loss : 6.89602072485562\n"
     ]
    }
   ],
   "source": [
    "fitting_process.fit_by_iterate(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
