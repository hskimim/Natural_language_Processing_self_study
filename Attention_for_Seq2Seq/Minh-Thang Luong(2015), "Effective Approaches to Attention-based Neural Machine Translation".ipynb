{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "해당 노트북은 \"Effective Approaches to Attention-based Neural Machine Translation\"을 기반으로 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchtext.datasets import TranslationDataset, Multi30k\n",
    "from torchtext.data import Field, BucketIterator\n",
    "\n",
    "import spacy\n",
    "\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "\n",
    "SEED = 1\n",
    "\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "spacy_de = spacy.load('de')\n",
    "spacy_en = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_de(text):\n",
    "    \"\"\"\n",
    "    Tokenizes German text from a string into a list of strings (tokens) and reverses it\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)][::-1]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    \"\"\"\n",
    "    Tokenizes English text from a string into a list of strings (tokens)\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "SRC = Field(tokenize=tokenize_de, init_token='<sos>', eos_token='<eos>', lower=True,include_lengths=True)\n",
    "# SRC 데이터에는 include_lengths 인자를 넣어주게 되면서, Encoder 의 pad_pack_sequence를 원활하게끔 지원해줍니다.\n",
    "TRG = Field(tokenize=tokenize_en, init_token='<sos>', eos_token='<eos>', lower=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data, valid_data, test_data = Multi30k.splits(exts=('.de', '.en'), fields=(SRC, TRG))\n",
    "# 영어와, 독일 데이터를 다운받으면서, train, validation, test 데이터로 나눠서 가져오게 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SRC.build_vocab(train_data, min_freq=2)\n",
    "TRG.build_vocab(train_data, min_freq=2)\n",
    "# 최소한 2번 이상 나오는 vocab에 대해서만, numericalize 시키게 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "BATCH_SIZE = 1000\n",
    "\n",
    "train_iter, test_iter = BucketIterator.splits(datasets=(train_data,test_data),\n",
    "                                            batch_sizes=(BATCH_SIZE,BATCH_SIZE),  \n",
    "                                            sort_key=lambda x: len(x.src), \n",
    "                                            device=device,\n",
    "                                            sort_within_batch=True, \n",
    "                                            repeat=False)\n",
    "\n",
    "# device 는 cpu 또는 gpu 를 적용하게 되고, (pytorch에서는 Variable(device=device) 와 같은 형태로, \n",
    "#인풋 데이터에 대해서 device 를 할당해줍니다.\n",
    "# batch_size 를 할당해주면, 반환값에 randomly batch가 적용됩니다.(1000개씩 묶인 상태에서, 인덱스가 랜덤으로 섞인 iterator 가 됩니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch = next(iter(train_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n",
      "tensor([[   2,    2,    2,  ...,    2,    2,    2],\n",
      "        [   4,    4,    4,  ...,    4,    4,    4],\n",
      "        [  74,  239, 1476,  ..., 1598,  140,  319],\n",
      "        ...,\n",
      "        [  73,  951,  717,  ...,    3,    3,    3],\n",
      "        [  26,    5,    5,  ...,    1,    1,    1],\n",
      "        [   3,    3,    3,  ...,    1,    1,    1]], device='cuda:0')\n",
      "tensor([23, 23, 23, 23, 23, 23, 23, 23, 23, 23], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(type(batch.src)) # torchtext 의  Field 클래스에서 include_lengths 를 True로 할당해주었기 때문에, turple 을 반환합니다.\n",
    "print(batch.src[0]) # 첫 번째 인자는 input_data가 batch_size에 맞춰 들어간 상태입니다.\n",
    "print(batch.src[1][:10]) # 두 번째 인자는 각 문장 별, 길이를 의미합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder 에 대한 논의\n",
    "- 네트워크는 기존 RNN encoder-decoder 모델에 사용된 Encoder와 같은 형태입니다.\n",
    "- Attention mechanism 을 사용하기 위해서는, 이번엔 hidden layer만 반환하는 것이 아닌, sequential output인 outputs 또한 반환합니다.\n",
    "- hidden 은 RNN encoder-decoder와 같이 context vector와 같은 역할을 합니다.\n",
    "- outputs 은 아래에 나올, Attention class의 인자로 들어가게 됩니다.\n",
    "\n",
    "### pack_padded_sequence, pad_packed_sequence 에 대한 논의\n",
    "- 이번에는 RNN 네트워크의 pytorch 의 utility function인 packing 과 padding 에 대한 함수를 사용하였습니다.\n",
    "- 사용하는 이유는, RNN 네트워크의 특징 상, 배치(batch)를 돌면서 다른 길이의 데이터를 받게 되는데, (CNN의 경우 데이터의 길이는 padding으로 일치시킵니다.) 그에 따라서, 0(n^2) 이라는 연산에 대한 비효율성이 발생하게 되는데, 이러한 문제를 줄여서 연산 효율성을 추구하기 위해 진행되는 프로세스입니다.\n",
    "\n",
    "```python\n",
    "nn.utils.rnn.pack_padded_sequence(embedded, seq_length)\n",
    "```\n",
    "위의 코드에서 보게 되면, embedding vector와 seq_length라는 인자가 함께 들어가게 되는데, sequence length 란, 각 문장의 길이를 의미하게 되는 것으로, 위의 torchtext 에서 Field 의 인자 ` include_length = True` 로 선언하여 length 또한 함께 이터레이터를 돌면서 반환하게끔 해놓은 상태이기 때문에, 인덱싱을 통해 쉽게 넣어줄 수 있게 됩니다.\n",
    "\n",
    "자세한 설명은 해당 페이지를 참고하시면 좋습니다. : https://gist.github.com/HarshTrivedi/f4e7293e941b17d19058f6fb90ab0fec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'attention_pic.png'>\n",
    "\n",
    "### Attention 에 대한 논의\n",
    "- 자세한 논의는 차후에 링크되는 블로그의 논문 요약에서 보다 자세히 다루도록 하겠습니다.\n",
    "- RNN Encoder-decoder의 문제는 바로, Encoder에서 context vector 로 input_data(source data라고 주로 칭합니다.)의 information을 하나의 벡터로 압축(compress)시킨다는 것에 있습니다. 이에 따라, 한정된 정보만을 담을 수 있다는 것이 주요 단점으로 지적됩니다.\n",
    "- 해당 Attention mechanism은 이러한 문제를 해결하기 위해 제시된 것으로, source data 와 target data의 각각의 sequence 에서 각각의 sentence 간의 유사성이 존재할 것이라는 아이디어에 기반합니다.\n",
    "- 예로 들어서, \"나는 딥러닝이 좋아\" 와 \"I love deep-learning\" 두 문장이 존재할 경우, \"좋아\" 라는 단어와 \"love\"라는 단어 간의 유사성이 문맥적(contextual) 의미적(semantic) 유사성이 높을 것입니다. 이에 따라, `Encoder 의 t스텝의 hidden과 Decoder의 t-1 hidden 간의 유사성을 계산`하고, (위의 등식에서 score() 라는 함수로 계산됩니다.) 이를 `확률 형태로 반환`하여(Attention weights) source data 중에 어떤 단어가 target data의 특정 스텝에서 특정 단어와 `유사성이 가장 높은지를 확률 형태`로 알려주게 됩니다. \n",
    "- 기존에는, context vector를 Decoder의 initial hidden layer로 사용하면서, target data를 그대로 RNN 네트워크를 통해 연산을 진행해주었는데, Attention weights 를 Context vector에 곱해준 즉, 가중합(weighted sum)을 진행해주면서, 학습에 유의한(significant) 데이터에 \"집중\" 하게 만드는 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p(y_i\\vert y_1,...,y_{i-1},\\mathbf{x})=g(y_{y-1},s_i,c_i)$$\n",
    "$$\\mathbf{x} = context\\ vector$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 해당 논문의 두 가지 approach에 대한 논의\n",
    "- 해당 논문은 vanila attention mechanism 에서 보다 효과적인 접근법 두 가지를 제시합니다.\n",
    "- **global approach**\n",
    "$$score({ h }_{ t },{ \\bar { h } }_{ s })=\\begin{cases} { h }_{ t }^{ T }{ \\bar { h } }_{ s } \\\\ { h }_{ t }^{ T }{ { W }_{ a }\\bar { h } }_{ s } \\\\ { W }_{ a }[{ h }_{ t }^{ };{ \\bar { h } }_{ s }] \\end{cases}$$\n",
    "\n",
    "    - 위와 같이 총 세가지 방식을 제시합니다. 이번 노트북에서는 가장 마지막 방식인 concat 방식을 구현하겠습니다.\n",
    "- 기존 논문에서는 hidden unit을 이전 스텝에서 어탠션을 적용해 현 스텝의 hidden을 업데이트하는 $h_{t-1} \\rightarrow a_{t} \\rightarrow c_{t} \\rightarrow h_{t}$의 경로를 따랐지만, 해당 논문에서는 현 스텝에 대해 global approach를 적용하는 $h_{t} \\rightarrow a_{t} \\rightarrow c_{t} \\rightarrow h_{t}$ 과 같은 경로를 따릅니다.\n",
    "- 나머지는 기존의 어탠션 메카니즘과 같습니다.\n",
    "- **local approach**h\n",
    "- monotonic approach 와 predictive approach로 다시 나뉘게 되는데, monotonic approach는 source sentence와 target sentence의 어순이 같다고 가정하는 것으로 나이브한 방법이기 때문에, 해당 노트북에서는 predictive approach를 다루겠습니다.\n",
    "$${ p }_{ t }=S\\cdot sigmoid({ v }_{ p }^{ T } \\ tanh({ W }_{ p }{ h }_{ t }))$$\n",
    "- 위의 식은 학습을 통해서 최적화되는 $p_{t}$를 measure하는 식입니다. $p_{t}$는 중심 단어입니다. 중심 단어에서 사용자가 정한 $D$만큼의 window를 지니고 local한 영역에서만의 집중적인 attention을 가하게 됩니다.\n",
    "$${ a }_{ t }(s)=align({ h }_{ t },{ \\bar { h } }_{ s })exp(-\\frac { { (s- }{ p }_{ t })^{ 2 } }{ 2{ \\sigma }^{ 2 } } )$$\n",
    "- 기존의 어탠션 메커니즘은 전체 source sentence의 모든 hidden에 softmax를 적용하였지만, local approach에는 $p_{t}$를 중심으로 가지는 가우시안 정규분포를 따른다고 가정하고 윈도우 내에 해당하는 시퀀스만큼에만 alignment model에 가중치를 부여합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, num_layers, dropout):\n",
    "        \n",
    "        super().__init__()     \n",
    "        self.input_dim = input_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim,padding_idx=1)\n",
    "        self.num_layers = num_layers\n",
    "        self.bi_rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, \n",
    "                            dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.hidden_dim = hidden_dim    \n",
    "        \n",
    "    def forward(self, text):\n",
    "        # text 는 tuple 입니다.\n",
    "        input_ = text[0] # tuple 의 첫 번째 엘리먼트는 input_data가 배치로 들어온 형태입니다.\n",
    "        seq_length = text[1] # tuple 의 두 번째 엘리먼트는 input_data 각각의 문장의 길이입니다.\n",
    "        embedded = self.dropout(self.embedding(input_)) #[max_length, batch_size, embedding_dim]\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, seq_length) \n",
    "        # packed_embedded 또한 tuple형태입니다. 하지만 기존의 RNN based 모델에 인자로 넣어주면 됩니다!\n",
    "        outputs, (hidden,cell) = self.bi_rnn(packed_embedded)\n",
    "        outputs,_ = nn.utils.rnn.pad_packed_sequence(outputs)\n",
    "        # hidden 은 가변 길이의 문장을 하나의 정보로 압축시키는 context vector 라고 생각하면 된다. \n",
    "        # 이러한 convext vector는 num_layer 의 갯수만큼 있고, decoder part에서 풀게 된다.\n",
    "        concated_hidden = torch.cat((hidden,cell),dim=2)\n",
    "        return outputs.permute(1,0,2), concated_hidden.permute(1,0,2)\n",
    "    # [max_length , batch_size , hidden_layer_dim], [1 , batch_size , hidden_layer_dim] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1000, 12, 128]), torch.Size([1000, 4, 256]))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim = len(SRC.vocab.itos)\n",
    "output_dim = len(TRG.vocab.itos)\n",
    "embedding_dim = 256\n",
    "hidden_dim = 128\n",
    "num_layer = 4\n",
    "dropout = 0.5\n",
    "batch_size = 1000\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "enc = Encoder(input_dim,embedding_dim,hidden_dim,num_layer,dropout).to(device)\n",
    "batch = next(iter(train_iter))\n",
    "\n",
    "outputs,enc_hidden = enc(batch.src)\n",
    "outputs.size(),enc_hidden.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global attention\n",
    "- 어탠션 함수 내에서 이전과 비교해서 변해야 할 것은 없습니다.\n",
    "$$v^T\\tanh{(WF+Vs_{i-1})}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class global_Attention(nn.Module) : \n",
    "    def __init__(self, outputs, hidden_dim, num_layer,batch_size):\n",
    "        \n",
    "        super().__init__()        \n",
    "        self.outputs = outputs # Encoder가 출력한 outputs 입니다.\n",
    "        self.hidden_dim = hidden_dim # hidden_dim의 dimension 입니다.\n",
    "        self.num_layer = num_layer\n",
    "        self.score_fc = nn.Linear(hidden_dim*num_layer*2 + hidden_dim,hidden_dim)\n",
    "        self.concated_fc = nn.Linear(hidden_dim*num_layer*2,hidden_dim)\n",
    "        self.softmax_fc = nn.Linear(hidden_dim,1)\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def forward(self,hidden) : \n",
    "                \n",
    "        hidden = hidden.contiguous().view(self.batch_size,-1).unsqueeze(1)\n",
    "        repeated_hidden = hidden.repeat(1,self.outputs.size(1),1)\n",
    "        \n",
    "        alignment_model = torch.tanh(self.score_fc(torch.cat((repeated_hidden, self.outputs), dim = 2))) \n",
    "        attention_weights = torch.softmax(self.softmax_fc(alignment_model),dim=1) # [batch_size, max_length, 1]\n",
    "        \n",
    "        context_vector = attention_weights * outputs#[batch_size, max_length, hidden_dim]\n",
    "        new_context_vector = torch.sum(context_vector,dim=1)#[batch_size, hidden_dim]\n",
    "        \n",
    "        return new_context_vector.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 1, 128])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = enc(batch.src)[0]\n",
    "hidden = enc(batch.src)[1]\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "hidden_dim = 128\n",
    "num_layer = 4\n",
    "batch_size = 1000\n",
    "\n",
    "attention_obj = global_Attention(outputs, hidden_dim, num_layer,batch_size).to(device)\n",
    "new_context_vector = attention_obj(hidden)\n",
    "new_context_vector.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### local attention\n",
    "- 아래의 시퀀스에서 정해주어야 할 것은 로컬 어탠션 가중치입니다. \n",
    "- 윈도우 $D$를 인풋으로 받아, outputs을 인덱싱한 후에, hidden unit을 받으면 중심단어 인덱스 $p_{t}$를 최적화시키고, 이에 따른 가중치를 반환하게 해주도록 하겠습니다.\n",
    "$${ p }_{ t }=S\\cdot sigmoid({ v }_{ p }^{ T } \\ tanh({ W }_{ p }{ h }_{ t }))$$\n",
    "$${ a }_{ t }(s)=align({ h }_{ t },{ \\bar { h } }_{ s })exp(-\\frac { { (s- }{ p }_{ t })^{ 2 } }{ 2{ \\sigma }^{ 2 } } )$$\n",
    "- 논문에서 가우시안 정규 분포에 사용되는 표준편차가 $D/2$로 경험적으로 선택되었다고 명시되었기 때문에, 이를 사용하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class local_Attention(nn.Module) : \n",
    "    def __init__(self, outputs, hidden_dim,batch_size,window):\n",
    "        \n",
    "        super().__init__()        \n",
    "        self.outputs = outputs # Encoder가 출력한 outputs 입니다. [max_length,batch_size, hidden_dim]\n",
    "        self.source_length = outputs.size(1) # 위의 식에서 S를 의미합니다. 인풋 시퀀스의 길이를 뜻합니다.\n",
    "        self.window = window\n",
    "        self.num_layer = num_layer\n",
    "        self.hidden_dim = hidden_dim # hidden_dim의 dimension 입니다.\n",
    "        self.batch_size = batch_size\n",
    "        self.window = window\n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        self.score_fc1 = nn.Linear(hidden_dim*num_layer*2,hidden_dim)\n",
    "        self.score_fc2 = nn.Linear(hidden_dim,1)\n",
    "        self.attn = nn.Linear(hidden_dim*num_layer*2+hidden_dim,hidden_dim)\n",
    "        self.softmax_fc = nn.Linear(hidden_dim,1)\n",
    "        \n",
    "        \n",
    "    def indexing_the_source(self,outputs,p_t) : \n",
    "        low_win = [int(i) if int(i) > 0 else 0 for i in torch.round(p_t - self.window)]\n",
    "        high_win = [int(i) for i in torch.round(p_t + self.window)]\n",
    "        print(\"해당 시퀀스의 로컬 중심 인덱스는 {} 입니다. \".format(torch.round(p_t)[0][0][0]))\n",
    "        indexed_outputs = torch.zeros(self.batch_size,self.window*2, self.hidden_dim).to(self.device)\n",
    "        gauss_tensor = torch.zeros(self.batch_size,1,1).to(self.device)\n",
    "        \n",
    "        for idx in range(self.batch_size) : \n",
    "            indexed_outputs[idx] = outputs[idx,low_win[idx]:high_win[idx],:].unsqueeze(0)\n",
    "        \n",
    "        for batch in range(4) :\n",
    "            for s in range(low_win[batch],high_win[batch]) :\n",
    "                gauss_tensor[batch] = torch.exp(-(((s - p_t[batch]).pow(2)) / (2*int(window/2))**2))\n",
    "\n",
    "        return indexed_outputs, gauss_tensor\n",
    "    \n",
    "    def forward(self,hidden) : \n",
    "        \n",
    "        hidden = hidden.contiguous().view(self.batch_size,-1).unsqueeze(1) # [batch_size, 1, hid_dim*2*num_layer]\n",
    "        repeated_hidden = hidden.repeat(1,self.window*2,1) # [batch_size, window*2, hid_dim*2*num_layer]\n",
    "        \n",
    "        in_sigmoid = self.score_fc2(torch.tanh(self.score_fc1(hidden))) # [batch_size, max_length, hidden_dim]\n",
    "        p_t = self.source_length * torch.sigmoid(in_sigmoid) #[batch_size,1,1]\n",
    "        local_outputs,gauss = self.indexing_the_source(outputs,p_t) #torch.Size([batch_size, window*2, hidden_dim]\n",
    "        \n",
    "        alignment_model = torch.tanh(self.attn(torch.cat((repeated_hidden, local_outputs), dim = 2))) \n",
    "        attention_vector = alignment_model * gauss #[batch_size,window*2,hidden_dim]\n",
    "        attention_weights = torch.softmax(self.softmax_fc(attention_vector),dim=1) # [batch_size, max_length, 1]\n",
    "\n",
    "        context_vector = attention_weights * local_outputs #[batch_size, max_length, hidden_dim]\n",
    "        new_context_vector = torch.sum(context_vector,dim=1)#[batch_size, hidden_dim]\n",
    "        return new_context_vector.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "해당 시퀀스의 로컬 중심 인덱스는 8.0 입니다. \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 1, 128])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(train_iter))\n",
    "outputs = enc(batch.src)[0]\n",
    "hidden = enc(batch.src)[1]\n",
    "hidden_dim = 128\n",
    "batch_size = 1000\n",
    "window = 3\n",
    "\n",
    "attention_obj = local_Attention(outputs, hidden_dim,batch_size,window).to(device)\n",
    "new_context_vector = attention_obj(hidden)\n",
    "new_context_vector.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, embedding_dim, hidden_dim, num_layers,\\\n",
    "                 dropout, attention_obj,window,local=True):\n",
    "        \n",
    "        super().__init__()        \n",
    "        self.embedding = nn.Embedding(output_dim, embedding_dim,padding_idx=1)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_layers = num_layers        \n",
    "        self.rnn = nn.LSTM(embedding_dim+hidden_dim,hidden_dim, num_layers=num_layers, \\\n",
    "                              dropout=dropout,batch_first=True)    \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fully_connect = nn.Linear(hidden_dim,output_dim)\n",
    "        self.attention_obj = attention_obj\n",
    "        self.batch_size = batch_size\n",
    "        self.local = local\n",
    "        self.window = window\n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    def forward(self, inputs, hidden, enc_outputs):\n",
    "        \n",
    "        if self.local :\n",
    "            atten = self.attention_obj(enc_outputs,self.hidden_dim,self.batch_size,self.window).to(device)\n",
    "        else : \n",
    "            atten = self.attention_obj(enc_outputs,self.hidden_dim,self.batch_size).to(device)\n",
    "            \n",
    "        context_vector = atten(hidden) \n",
    "        #[batch_size,1, hidden_dim]\n",
    "        # 함수 내에서 Attention class을 호출해, context_vector를 계산해줍니다.\n",
    "        # 이 때, attention_obj 에서 변하는 argument 는 hidden layer의 값입니다.\n",
    "\n",
    "        embedding_vector = \\\n",
    "        self.dropout(self.embedding(inputs.unsqueeze(0))).permute(1,0,2) # [batch_size, max_length, embed_dim]\n",
    "        concated_vector = torch.cat((context_vector, embedding_vector), dim=2) # input feeding approach\n",
    "        # [batch_size, max_length, embed_dim+hid_dim]\n",
    "        outputs,(hidden,cell) = self.rnn(concated_vector)\n",
    "\n",
    "        final_outputs = outputs.squeeze(1)\n",
    "        fc_layer = self.fully_connect(final_outputs) #[batch_size, output_dim]\n",
    "        \n",
    "        fc_layer = fc_layer #[batch_size, output_dim]\n",
    "        concated_hidden = torch.cat((hidden,cell),dim=2).permute(1,0,2) #[batch_size,1,hid_dim*2]\n",
    "        \n",
    "        return fc_layer, concated_hidden\n",
    "#     [max_length, batch_size, output_dim] , [1, batch_size, output_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1000, 5893]), torch.Size([1000, 4, 256]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim = len(SRC.vocab.itos)\n",
    "output_dim = len(TRG.vocab.itos)\n",
    "embedding_dim = 256\n",
    "hidden_dim = 128\n",
    "num_layer = 4\n",
    "dropout = 0.5\n",
    "attention_obj = local_Attention\n",
    "batch_size = 1000\n",
    "window = 3\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "batch = next(iter(train_iter))\n",
    "dec = Decoder(output_dim,embedding_dim,hidden_dim,num_layer,dropout,attention_obj,window).to(device)\n",
    "outputs = enc(batch.src)[0]\n",
    "hidden = enc(batch.src)[1]\n",
    "\n",
    "fc_layer, hidden = dec(batch.trg[0],hidden,outputs)\n",
    "fc_layer.size(), hidden.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "                                \n",
    "        batch_size = trg.shape[1]\n",
    "        max_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        # decoder 의 맨처음에는 encoder 에서 나온 hidden을 넣어주어야 합니다! 이때, num_layer와 hidden_dim은 같아야 합니다!\n",
    "        enc_output, enc_hidden = self.encoder(src)\n",
    "        \n",
    "        # decoder 를 돌면서, 각 단어에 대한 outputs값(벡터 형태)이 나오게 되는데, 이러한 값들을 아래의 outputs 변수에 저장해줍니다\n",
    "        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        # 맨 처음에는 문장의 시작을 알리는 sos(start of sentence) 토큰을 넣어주어야 합니다.\n",
    "        dec_inputs = trg[0,:]\n",
    "        dec_hidden = enc_hidden\n",
    "        \n",
    "        for t in range(1, max_len):\n",
    "\n",
    "            fc_layer, dec_hidden = self.decoder(dec_inputs,dec_hidden,enc_output)\n",
    "#            output'dimension : [batch_size , output_dim], 여기서 output_dim 은 출현 가능한 모든 target lang 의 수 입니다.\n",
    "            outputs[t] = fc_layer\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = fc_layer.max(1)[1] # 해당 글자의 numericalized index 를 넣어주어야 합니다.\n",
    "            input_ = (trg[t] if teacher_force else top1)\n",
    "            # teacher_forcing 을 할 경우, 실제 trg데이터를 다음 input으로 사용, 그렇지 않을 경우, 이전 state에서 가장 높은 \n",
    "            # 값을 가진[나올 수 있는 모든 target vocab 리스트 중에서를 의미합니다. 확률값의 형태는 아니지만, 가장 개연성이 높은 단어를 의미합니다.]\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fin/anaconda3/envs/engine_3.6/lib/python3.6/site-packages/torch/nn/modules/rnn.py:46: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "#parameter settings\n",
    "input_dim = len(SRC.vocab.itos)\n",
    "output_dim = len(TRG.vocab.itos)\n",
    "embedding_dim = 256\n",
    "hidden_dim = 128\n",
    "num_layer = 1\n",
    "dropout = 0.5\n",
    "attention_obj = local_Attention\n",
    "batch_size = 1000\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "enc = Encoder(input_dim,embedding_dim,hidden_dim,num_layer,dropout).to(device)\n",
    "dec = Decoder(output_dim,embedding_dim,hidden_dim,num_layer,dropout,attention_obj,window).to(device)\n",
    "batch = next(iter(train_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([23, 1000, 5893])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq = Seq2Seq(enc,dec,device)\n",
    "seq(batch.src,batch.trg).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gradient clipping 에 대한 논의\n",
    "- https://hskimim.github.io/Avoid_Exploding_gradient_in_Neural_Net_with_gradient_clipping/ 에 보다 자세히 기록하였습니다.\n",
    "- RNN 베이스의 네트워크의 특징인 gradient exploding 을 방지해주는 방법론입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class fit() : \n",
    "    \n",
    "    def __init__(self, model, train_iter, test_iter, epoch = 5) : \n",
    "        self.optimizer = optim.Adam(model.parameters())\n",
    "        # <pad> 토큰은 임베딩 벡터와, loss_function에 argument 로 들어가서, training 과정에서 제외됩니다.\n",
    "        self.pad_idx = TRG.vocab.stoi['<pad>'] \n",
    "        self.criterion = nn.CrossEntropyLoss(ignore_index=self.pad_idx)\n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model = model.to(device)\n",
    "        self.train_iter = train_iter\n",
    "        self.test_iter = test_iter\n",
    "        self.epoch = epoch\n",
    "            \n",
    "    def train(self,clip):\n",
    "    \n",
    "        epoch_loss = 0 # loss per epoch\n",
    "        self.model.train()\n",
    "        \n",
    "        for i, batch in enumerate(self.train_iter):\n",
    "            print('batch : ',i,end='\\r')\n",
    "            src = batch.src\n",
    "            trg = batch.trg\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            output = model(src, trg)        \n",
    "\n",
    "            loss_output = output[1:].view(-1, output.shape[-1])\n",
    "            loss_trg = trg[1:].view(-1)\n",
    "            # sos 토큰을 제외하고, 차원을 맞춘 후에, output을 변수에 저장해줍니다.\n",
    "            \n",
    "            loss = self.criterion(loss_output, loss_trg)\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "            # gradient clipping\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        return epoch_loss / len(self.train_iter)\n",
    "    \n",
    "\n",
    "    def fit_by_iterate(self,clip) : \n",
    "        \n",
    "        for epoch in range(self.epoch):\n",
    "            print('epoch : ',epoch + 1)\n",
    "            train_loss= self.train(clip)\n",
    "            print(\"epoch's loss : {}\".format(train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#parameter settings\n",
    "input_dim = len(SRC.vocab.itos)\n",
    "output_dim = len(TRG.vocab.itos)\n",
    "embedding_dim = 256\n",
    "hidden_dim = 128\n",
    "num_layer = 1\n",
    "dropout = 0.5\n",
    "attention_obj = local_Attention\n",
    "batch_size = 1000\n",
    "window = 3\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "enc = Encoder(input_dim,embedding_dim,hidden_dim,num_layer,dropout).to(device)\n",
    "dec = Decoder(output_dim,embedding_dim,hidden_dim,num_layer,dropout,attention_obj,window).to(device)\n",
    "batch = next(iter(train_iter))\n",
    "\n",
    "model = Seq2Seq(enc,dec,device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fitting_process = fit(model,train_iter,test_iter,epoch=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fitting_process.fit_by_iterate(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
