{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "deeply refered on  \n",
    "https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/1%20-%20Simple%20Sentiment%20Analysis.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from konlpy.tag import Okt\n",
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import Field\n",
    "from torchtext.data import TabularDataset\n",
    "from torchtext.data import Iterator, BucketIterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declaring the Fields\n",
    "- Torchtext 는 데이터를 가져오는 과정에서 선언하는 방식을 사용합니다.\n",
    "    - 데이터가 어떤 형식을 지닐 것인지에 대한 것에 대해 선언을 해주고 이에 따라 torchtext 는 데이터를 로딩합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "tokenize = lambda x: okt.nouns(x)\n",
    "\n",
    "# TEXT = Field(tokenize=tokenize,use_vocab=True, include_lengths=False,batch_first=True,fix_length=20) \n",
    "# TEXT = Field(tokenize=tokenize) \n",
    "TEXT = Field(tokenize=tokenize,use_vocab=True) \n",
    "LABEL = Field(sequential=False,unk_token=None, use_vocab=True,dtype=torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing the Dataset\n",
    "- fields 객체는 raw data 를 어떻게 가져올 지에 대한 선언이 담겨있습니다.\n",
    "- TabularDataset 객체를 통해서, 어디서, 어떤 데이터를 가져올 지에 대해 선언을 해줍니다.\n",
    "- 아래의 소스코드를 통해 형성된 객체는 generator 의 형태를 띕니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9min 19s, sys: 5.02 s, total: 9min 24s\n",
      "Wall time: 6min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "datafields = [(\"X\", TEXT), (\"y\", LABEL)]\n",
    "\n",
    "train_data,test_data = TabularDataset.splits(\n",
    "                            path=\".\", \n",
    "                            train='train_df.csv',\n",
    "                            test='test_df.csv',\n",
    "                            format='csv',\n",
    "                            skip_header=True,\n",
    "                            fields=datafields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train_data);LABEL.build_vocab(train_data)\n",
    "TEXT.build_vocab(test_data);LABEL.build_vocab(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TabularDatset 을 통해서, tokenizing 까지는 되었지만, word_to_integar process는 아직 이뤄지지 않았습니다. \n",
    "- 우리의 경우, train , text 데이터 셋에 대해서 TEXT 부분에 대해서, word_to_integar converting이 필요합니다.\n",
    "- `TEXT.build_vocab(trn)` 이라는 코드를 통해, converting이 가능합니다.\n",
    "- 위의 연산은 모든 training set에 있는 모든 엘리먼트들을 torchtext로 만들어줍니다. Torchtext는 vocabulary를 핸들링하는 Vocab이라는 클래스를 가지고 있습니다. Vocab클래스는 word와 id를 stoi attribute에서 mapping 시켜주고, itos attribute에서는 reverse mapping시켜줍니다.\n",
    "- stoi : word_to_idx default dictionary \n",
    "- itos : word list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = 'cpu'\n",
    "train_iter, test_iter = BucketIterator.splits(datasets=(train_data,test_data),\n",
    "                                            batch_sizes=(BATCH_SIZE,BATCH_SIZE),  \n",
    "                                            sort_key=lambda x: len(x.X), \n",
    "                                            device=device,\n",
    "                                            sort_within_batch=True, \n",
    "                                            repeat=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "1. RNN\n",
    "2. LSTM\n",
    "3. GRU\n",
    "\n",
    "- 모델 각각에 대한 깊은 논의는 관련 논문을 따로 읽고 정리한 파일을 같은 경로에 올리면서 새롭게 다루도록 할 예정입니다.\n",
    "- 모델 아키텍처를 쌓아가는 과정, 그리고 분류 모델을(many-to-one) 형성하는 과정 내에서의 차이점은, RNN은 `outputs, hidden`을 \n",
    "- 반환한다는 것이고, 이와 달리 LSTM, GRU 모델은 반환값의 형태가 `outputs, (hidden,cell)`이라는 점입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bi-birectional 에 대한 논의\n",
    "- Bi는 말 그대로 양쪽이라는 의미입니다. LSTM 과 GRU 모두 RNN 이 baseline인 뉴럴 네트워크인데, RNN의 특징은 sequential 데이터에 적합한 뉴럴 네트워크라는 것입니다. ex. 주식 시장의 시계열적 움직임, 문장을 형성 구조 (영어의 경우 주어 + 동사 + 목적어 와 같은 형태)\n",
    "- Sequential 데이터의 특성 상, 방향성이 존재하겠지만, (시간은 거꾸로 흐르지 않습니다.) 자연어 처리의 경우, 분류 모델이나 언어 모델(language modeling)을 학습할 경우, 그 시퀀스의 방향성을 양측 모두 고려해서 학습한다는 것에 특징이 있습니다.\n",
    "- \"나는 밥을 먹었다\" 라는 데이터가 있을 경우, 토큰화 이후, ['나는', '밥을', '먹었다'] 만을 고려하는 것이 아니라, ['먹었다', '밥을', '나는'] 라는 역순(reverse) 까지 고려해준다는 것이죠.\n",
    "- 아키텍쳐의 형성부분에서 고려해야 할 점은, 모델이 반환하는 값이 두 개가 나온다는 것입니다. 여기서 두 개란 차원이 두 배로 증가한다는 것이 아닌, RNN의 경우 두 개의 outputs, 두 개의 hidden 이 나온다는 것이지요.\n",
    "- 따라서, 이런 경우, 반환값들을 옆으로 붙여서(concatenate라고 합니다) fully-connect layer로 보내는 방법을 사용하게 됩니다.\n",
    "- 정리를 하자면, bi-directional 은, 독립적인 두 개의 시퀀스, 순서로 데이터를 읽어드려서 양방향을 모두 고려하게 하는 방법이 되겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../bi-directional.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim, \\\n",
    "                 dropout, num_layers,bidirectional,batch_size , model_name='rnn'):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.model_name = model_name # 해당 클래스는 3 개의 모델을 선택적으로 적용시키기 때문에, model_name 을 인풋으로 받습니다.\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        if bidirectional : \n",
    "            self.bidirectional = 2\n",
    "        else : \n",
    "            self.bidirectional = 1\n",
    "        # True 면 양방향(2), False 면 단방향(1)입니다.\n",
    "        self.hidden_dim = hidden_dim # hidden layer를 의미하는 것이 아닌, hidden_layer의 차원을 의미합니다.(하이퍼 파라미터)\n",
    "        self.batch_size = batch_size # 초기의 hidden_layer 값의 dimension을 정해주기 위해 인풋으로 받습니다.\n",
    "        self.num_layers = num_layers \n",
    "        # multi-layer perceptron 을 생각해보면, W*x + b 가 하나의 layer이고 이들을 계속 쌓으면 multi-layer perceptron 이라고\n",
    "        # 하는데요. 여기서 num_layers도 그와 같은 맥락으로, rnn layer 가 여러개 쌓여 간다는 것입니다.\n",
    "        # layer의 갯수가 커질 수록 모델의 complexity는 높아지게 됩니다.\n",
    "        \n",
    "        if self.model_name == 'rnn' : \n",
    "            self.rnn = nn.RNN(embedding_dim, hidden_dim, num_layers = num_layers,\\\n",
    "                          dropout = dropout, bidirectional = bidirectional)            \n",
    "        elif self.model_name == 'lstm' : \n",
    "            self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, \n",
    "                           bidirectional=bidirectional, dropout=dropout)\n",
    "        elif self.model_name == 'gru' : \n",
    "            self.rnn = nn.GRU(embedding_dim, hidden_dim, num_layers = num_layers,\\\n",
    "                          dropout = dropout, bidirectional = bidirectional)\n",
    "        else : raise ValueError('model should be among RNN and LSTM and GRU')\n",
    "        \n",
    "        if bidirectional  : \n",
    "            self.fc = nn.Linear(hidden_dim*2, output_dim) \n",
    "            #양방향이고, 위에서 말씀드렸다시피, 후에 hidden_layer를 concatenate 시키고 fully-connected layer \n",
    "            # 단계로 넘어가기 때문에, hidden_dimension에 곱하기 2를 해줍니다.\n",
    "        else : \n",
    "            self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def init_hidden(self,batch) : \n",
    "        \n",
    "        if self.model_name == 'rnn' : \n",
    "            hidden = Variable(torch.zeros(self.num_layers*self.bidirectional,self.batch_size,self.hidden_dim))\n",
    "            return hidden\n",
    "        \n",
    "        else : \n",
    "            hidden = Variable(torch.zeros(self.num_layers*self.bidirectional,self.batch_size,self.hidden_dim))\n",
    "            cell = Variable(torch.zeros(self.num_layers*self.bidirectional,self.batch_size,self.hidden_dim))\n",
    "            # LSTM 과 GRU 는 RNN 아키텍처의 약점인 오래전의 기억을 잃어버린다는 장거리 의존성에 대한 문제를 해결하기 위해,\n",
    "            # 내부적으로 cell을 추가하였는데, 이로 인해서 인풋값과 반환값에 hidden 에 더 나아가 cell 이라는 변수가 추가됩니다.\n",
    "            return hidden,cell\n",
    "        \n",
    "    def forward(self, text):\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        # embedding vector 를 만들어주고, 바로 dropout 을 해주었습니다(가장 위에서 refer 한 블로그에 따른 것입니다.)\n",
    "        #dropout 을 해당 레이어에서 하고 싶지 않으시다면, embedded =self.embedding(text)로 만들어주시면 됩니다.\n",
    "        \n",
    "        if not self.bidirectional  : \n",
    "            h0 = self.init_hidden(embedded.size(0)) \n",
    "            # embedded.size(0) 은 batch_size 입니다.  h0 의 차원은, [num_layer, batch, hidden]이 됩니다.\n",
    "            output, hidden = self.rnn(embedded,h0)\n",
    "            # 참고로 self.rnn(embedded) 만 넣어주면, h0은 자동으로 dimension 에 맞게끔 영벡터로 들어가게 됩니다.\n",
    "            # 여기서 저는 init_hidden이라는 함수를 따로 정의하였지만, 따로 선언하지 않아도 영벡터로 들어간다는 것이죠.\n",
    "            assert torch.equal(output[-1,:,:], hidden.squeeze(0))\n",
    "            # 개념 상 중요한 것인데, output 값의 마지막 값은 시퀀스의 마지막 output을 의미하는 것이고,\n",
    "            # 그것이 hidden 과 같아야 하는 이유는, hidden 은 이전 스텝의 hidden 들이 누적적으로 계산되어 나오는 하나의 값이 되고,\n",
    "            # output 또한 이전 스텝의 output들이 누적적으로 계산되어 나오지만, 이전 스텝의 output은 0_t라는 개념으로 sequential \n",
    "            # 하게 남기 때문에, 차원은 다르게 되는 것입니다. 따라서, 단뱡향의 경우,마지막 outputs의 값과 hidden값은 같아야 합니다.\n",
    "            return self.fc(hidden)\n",
    "        # hidden을 fully-connected layer에 넣어서 보내는 이유는 간단합니다. 분류 모델의 경우, many-to-one 모델이고 \n",
    "        # 이에 따라서 sequential input 이 들어가도 결국 단 하나의 값만 출력하면 되기 때문에, 마지막 hidden 을 넣어주는 것입니다.\n",
    "        # self.fc(hidden) 의 차원은, [batch,1] 이 됩니다.\n",
    "        \n",
    "        else : \n",
    "            if self.model_name == 'rnn' :\n",
    "                h0 = self.init_hidden(embedded.size(0))\n",
    "                output, hidden = self.rnn(embedded)\n",
    "            else : \n",
    "                h0 = self.init_hidden(embedded.size(0))\n",
    "                output, (hidden, cell) = self.rnn(embedded)\n",
    "            hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n",
    "            print('hidden',hidden.size())\n",
    "            # 양방향의 경우, 마지막에 concatenate 를 해준다고 말씀드렸는데요. pytorch 에서는 bidirectional 파라미터를 선언해주면,\n",
    "            # user가 따로 concatenate 할 필요없이, 각각의 directional 에 따른 반환값들을 합쳐주는데, 이 때 데이터의 형태가\n",
    "            # [정방향][역방향] 순이 아닌, [정][역][정][역] 과 같은 형태이기 때문에, 양방향의 데이터를 모두 가지는 hidden 값을\n",
    "            # 알기 위해서는 마지막 [정][역]만을 가지고 와서 concat 시켜주게 되는 것입니다. 이제 위의 __init__ 에서 Linear의 차원에\n",
    "            # bidirectional 일 경우, hidden * 2 가 적용되었는지 이해가 가실 것 같습니다.\n",
    "            return self.fc(hidden)\n",
    "        # self.fc(hidden) 의 차원은, [batch,1] 이 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model\n",
    "- optimizer\n",
    "    - stochastic gradient descent 사용\n",
    "- loss function\n",
    "    - 라벨은 우리가 명시적으로 0,1 만 사용하게 하도록 하였다. 즉, binary classification이다. 이에 따라, sigmoid를 사용한다.\n",
    "- batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "class fit() : \n",
    "    \n",
    "    def __init__(self, model, train_iter, test_iter, epoch = 5) : \n",
    "        self.optimizer = optim.SGD(model.parameters(), lr=1e-3)\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model = model.to(device)\n",
    "        self.criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "        self.train_iter = train_iter\n",
    "        self.test_iter = test_iter\n",
    "        self.epoch = epoch\n",
    "        \n",
    "    def binary_accuracy(self, preds, y):\n",
    "        \"\"\"\n",
    "        Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "        \"\"\"\n",
    "\n",
    "        rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "        # model 에서는 fully connecting 만 시켰기 때문에, sigmoid 를 씌워서, rounding을 시켜줍니다.\n",
    "        correct = (rounded_preds == y).float() # 같으면 1, 다르면 0이 된다.\n",
    "        acc = correct.sum()/len(correct) # 한 번의 배치에서 맞은 갯수의 비율 즉, accuracy가 된다!\n",
    "        return acc\n",
    "    \n",
    "    def train(self, model, iterator):\n",
    "    \n",
    "        epoch_loss = 0 # loss per epoch\n",
    "        epoch_acc = 0 # accuracy per epoch\n",
    "\n",
    "        model.train()\n",
    "        for batch in iterator:\n",
    "            if batch.X.size(0) == 0 : continue #데이터가 없으면, continue 합니다. 위에서 padding을 manually 적용해주어서 상관 x\n",
    "            self.optimizer.zero_grad() # optimizer 의 gradient 를 manually 하게 zero 로 초기화시켜주어야 합니다.\n",
    "\n",
    "            predictions = model(batch.X).squeeze(1) # model 에서 나오는 fc 의 size 는 [batch_size,num_layer] 이기 때문            \n",
    "            loss = self.criterion(predictions, batch.y) # calculating the loss \n",
    "\n",
    "            acc = self.binary_accuracy(predictions, batch.y) # return the accracuy in form of ratio\n",
    "\n",
    "            loss.backward() # back propagation\n",
    "\n",
    "            self.optimizer.step() # update the SGD\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "\n",
    "        return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "    \n",
    "    def evaluate(self, model, iterator):\n",
    "    \n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "\n",
    "        model.eval() # stop the every change in gradient of model\n",
    "\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            for batch in iterator:\n",
    "                if batch.X.size(0) == 0 : continue\n",
    "                predictions = model(batch.X).squeeze(1)\n",
    "                loss = self.criterion(predictions, batch.y)\n",
    "\n",
    "                acc = self.binary_accuracy(predictions, batch.y)\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "                epoch_acc += acc.item()\n",
    "\n",
    "        return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "    \n",
    "    def fit_by_iterate(self) : \n",
    "        \n",
    "        for epoch in range(self.epoch+1):\n",
    "            print('epoch : ',epoch+1,end='\\r')\n",
    "            train_loss, train_acc = self.train(self.model, self.train_iter)\n",
    "\n",
    "        valid_loss, valid_acc = self.evaluate(self.model, self.test_iter)\n",
    "        print(f'| Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}% | Val. Loss: {valid_loss:.3f} | Val. Acc: {valid_acc*100:.2f}% |')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (embedding): Embedding(23043, 256)\n",
       "  (rnn): LSTM(256, 100, num_layers=2, dropout=0.5, bidirectional=True)\n",
       "  (fc): Linear(in_features=200, out_features=1, bias=True)\n",
       "  (dropout): Dropout(p=0.5)\n",
       ")"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 256\n",
    "HIDDEN_DIM = 100\n",
    "OUTPUT_DIM = 1\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.5\n",
    "MODEL_NAME = 'lstm'\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,\\\n",
    "            DROPOUT, N_LAYERS, BIDIRECTIONAL, BATCH_SIZE, MODEL_NAME)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitting_process = fit(model,train_iter,test_iter,epoch=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 05 | Train Loss: 0.670 | Train Acc: 48.78% | Val. Loss: 0.666 | Val. Acc: 50.83% |\n"
     ]
    }
   ],
   "source": [
    "fitting_process.fit_by_iterate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
