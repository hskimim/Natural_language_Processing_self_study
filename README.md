# Natural_language_Processing_self_study

독학이지만, 주변 분들과 인터넷을 힘을 빌려 하나씩 해내가고 있습니다. 개인적 목표는 한글로 제가 이해한 것을 모두 풀어내어 제가 도움을 받았던 만큼 누구에게 도움이 되고픈 소망이 있습니다.

자연어 처리에서 널리 사용되는 네트워크들을 하나하나 곱씹으면서 살펴보려 합니다. 네트워크마다 관련 논문 또한 나름대로 번역하고 요약해서 동일한 파일에 올리려고 하니, 많은 피드백 부탁드립니다!

Although it is self-study, I borrow strength from the people around me and the Internet. My personal goal is to solve everything I understand in Hangul and I have a desire to help anyone as I have been helped.

Let's look at each of the widely used networks in natural language processing one by one. I would like to translate and summarize related papers in the same file per network, so please give me lots of feedback!

[CNN_for_classification](https://github.com/hskimim/Natural_language_Processing_self_study/tree/master/Attention_for_classification)
- Yoon kim (2014), Convolutional Neural Networks for Sentence

[RNN_LSTM_GRU](https://github.com/hskimim/Natural_language_Processing_self_study/tree/master/RNN_LSTM_GRU_for_classification)
- it is not based on paper(literature)

[Char_CNN_for_Language_Modeling](https://github.com/hskimim/Natural_language_Processing_self_study/tree/master/Char_CNN_for_Language_Modeling)
- Yoon kim(2015),"Character-Aware Neural Language Models"

[RNN_for_Seq2Seq](https://github.com/hskimim/Natural_language_Processing_self_study/tree/master/RNN_for_Seq2Seq)
- Ilya Sutskever, Oriol Vinyals, Quoc V. Le(2014), "Sequence to Sequence Learning with Neural Networks
- Cho,  Merrienboer, Gulcehre (2014), "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation"

[Attention of Seq2Seq](https://github.com/hskimim/Natural_language_Processing_self_study/tree/master/Attention_for_Seq2Seq)
- Dzmitry Bahdanau(2014), Neural Machine Translation by Jointly Learning to Align and Translate
- Minh-Thang Luong(2015), "Effective Approaches to Attention-based Neural Machine Translation"

[Attention for Classfication](https://github.com/hskimim/Natural_language_Processing_self_study/tree/master/Attention_for_classification)
- Zichao Yang(2016), "Hierarchical Attention Networks for Document Classification"

[Self-Attention for Machine Translation](https://github.com/hskimim/Natural_language_Processing_self_study/tree/master/Self-Attention_for_Machine_Translation)
- Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin(2017),"Attention Is All You Need"
